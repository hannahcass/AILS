{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import resnet50, convnext_base, convnext_tiny, convnext_small, convnext_large\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision.transforms.functional import to_tensor\n",
    "\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "np.random.seed(1234)\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_id</th>\n",
       "      <th>cell_line</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>MCF7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>RT4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>U-2 OS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>RT4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>A549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9627</th>\n",
       "      <td>9628</td>\n",
       "      <td>PC-3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9628</th>\n",
       "      <td>9629</td>\n",
       "      <td>HEK 293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9629</th>\n",
       "      <td>9630</td>\n",
       "      <td>RT4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9630</th>\n",
       "      <td>9631</td>\n",
       "      <td>PC-3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9631</th>\n",
       "      <td>9632</td>\n",
       "      <td>PC-3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9632 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      file_id cell_line\n",
       "0           1      MCF7\n",
       "1           2       RT4\n",
       "2           3    U-2 OS\n",
       "3           4       RT4\n",
       "4           5      A549\n",
       "...       ...       ...\n",
       "9627     9628      PC-3\n",
       "9628     9629   HEK 293\n",
       "9629     9630       RT4\n",
       "9630     9631      PC-3\n",
       "9631     9632      PC-3\n",
       "\n",
       "[9632 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('y_train.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RT4         2100\n",
       "CACO-2      1626\n",
       "HEK 293     1378\n",
       "MCF7        1082\n",
       "U-2 OS       775\n",
       "U-251 MG     768\n",
       "PC-3         663\n",
       "HeLa         632\n",
       "A549         608\n",
       "Name: cell_line, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_counts = data.cell_line.value_counts()\n",
    "label_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_images(data, img_dir, output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    for idx, row in data.iterrows():\n",
    "        sample_id = row['file_id']\n",
    "        blue_img_path = os.path.join(img_dir, f\"{str(sample_id).zfill(5)}_blue.png\")\n",
    "        red_img_path = os.path.join(img_dir, f\"{str(sample_id).zfill(5)}_red.png\")\n",
    "        yellow_img_path = os.path.join(img_dir, f\"{str(sample_id).zfill(5)}_yellow.png\")\n",
    "\n",
    "        blue_img = Image.open(blue_img_path).convert('L')\n",
    "        red_img = Image.open(red_img_path).convert('L')\n",
    "        yellow_img = Image.open(yellow_img_path).convert('L')\n",
    "\n",
    "        combined_img = Image.merge(\"RGB\", (red_img, blue_img, yellow_img))\n",
    "\n",
    "        combined_img_path = os.path.join(output_dir, f\"{str(sample_id).zfill(5)}_combined.png\")\n",
    "        combined_img.save(combined_img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine_images(data, img_dir=\"images_train/images_train/\", output_dir=\"images_combined/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_data, val_data = train_test_split(\n",
    "#    data, test_size=1/3, random_state=42, stratify=data['cell_line'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_data.to_csv(\"train_data.csv\", index=False)\n",
    "#val_data.to_csv(\"val_data.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CellLineDataset(Dataset):\n",
    "    def __init__(self, img_dir, labels_file=None, transform=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        if labels_file:\n",
    "            self.labels_df = pd.read_csv(labels_file)\n",
    "            self.has_labels = True\n",
    "            self.class_to_idx = {class_name: i for i, class_name in enumerate(\n",
    "                self.labels_df[\"cell_line\"].unique())}\n",
    "        else:\n",
    "            self.has_labels = False\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        if self.has_labels:\n",
    "            row = self.labels_df.iloc[idx]\n",
    "            sample_id = row['file_id']\n",
    "            img_path = os.path.join(self.img_dir, f\"{str(sample_id).zfill(5)}_combined.png\")\n",
    "            img = Image.open(img_path)\n",
    "\n",
    "            if self.transform:\n",
    "                img = self.transform(img)\n",
    "\n",
    "            label = self.class_to_idx[row['cell_line']]\n",
    "            return img, label\n",
    "        else:\n",
    "            raise IndexError(f\"No matching row found for index {idx}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mean_std(loader):\n",
    "    mean = 0.\n",
    "    std = 0.\n",
    "    nb_samples = 0.\n",
    "    for data, _ in tqdm(loader): \n",
    "        batch_samples = data.size(0)\n",
    "        data = data.view(batch_samples, data.size(1), -1)\n",
    "        mean += data.mean(2).sum(0)\n",
    "        std += data.std(2).sum(0)\n",
    "        nb_samples += batch_samples\n",
    "\n",
    "    mean /= nb_samples\n",
    "    std /= nb_samples\n",
    "    return mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "batch_size =64\n",
    "lr = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/151 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 151/151 [00:08<00:00, 17.41it/s]\n"
     ]
    }
   ],
   "source": [
    "transform_to_tensor = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "raw_train_data = CellLineDataset(\n",
    "    img_dir=\"images_combined/\", labels_file=\"y_train.csv\", transform=transform_to_tensor)\n",
    "\n",
    "raw_train_loader = DataLoader(raw_train_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "mean, std = calculate_mean_std(raw_train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform = transforms.Compose([\n",
    "#     transforms.RandomRotation(30),\n",
    "#     transforms.RandomResizedCrop(64),\n",
    "#     transforms.RandomHorizontalFlip(),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean=mean, std=std)\n",
    "# ])\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomRotation(30),\n",
    "    transforms.RandomResizedCrop(112),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomAffine(degrees=20, translate=(0.1,0.1), scale=(0.8, 1.2)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean, std=std),\n",
    "    transforms.RandomErasing(p=0.1, scale=(0.02, 0.33), ratio=(0.3, 3.3), value=0, inplace=False)\n",
    "])\n",
    "\n",
    "raw_train_data = CellLineDataset(\n",
    "    img_dir=\"images_combined/\", labels_file=\"y_train.csv\", transform=transform)\n",
    "\n",
    "raw_train_loader = DataLoader(raw_train_data, batch_size=batch_size, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform = transforms.Compose([\n",
    "#     transforms.RandomRotation(30),\n",
    "#     transforms.RandomResizedCrop(224),\n",
    "#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "#     transforms.RandomHorizontalFlip(),\n",
    "#     transforms.RandomAffine(degrees=20, translate=(0.1,0.1), scale=(0.8, 1.2)),\n",
    "#     transforms.Normalize(mean=mean, std=std),\n",
    "#     transforms.RandomErasing(p=0.1, scale=(0.02, 0.33), ratio=(0.3, 3.3), value=0, inplace=False)\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = CellLineDataset(\n",
    "#     img_dir=\"images_combined/\", labels_file=\"train_data.csv\", transform=transform)\n",
    "# val_dataset = CellLineDataset(\n",
    "#     img_dir=\"images_combined/\", labels_file='val_data.csv', transform=transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CellLineDataset(\n",
    "    img_dir=\"images_combined/\",\n",
    "    labels_file=\"y_train.csv\",\n",
    "    transform=transform\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "# test_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = convnext_tiny(weights=torchvision.models.ConvNeXt_Tiny_Weights.IMAGENET1K_V1, progress=True)\n",
    "#model.classifier[2] = torch.nn.Linear(in_features=1024, out_features=9)\n",
    "#model = model.to(device)  \n",
    "model = convnext_base(weights=torchvision.models.ConvNeXt_Base_Weights.IMAGENET1K_V1, progress=True)\n",
    "model.classifier[2].out_features = 9\n",
    "#model.classifier[2] = torch.nn.Linear(in_features=1024, out_features=9)\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-3)\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "exp_lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 - Training Loss: 2.2731\n",
      "Epoch 2/100 - Training Loss: 2.0002\n",
      "Epoch 3/100 - Training Loss: 1.8523\n",
      "Epoch 4/100 - Training Loss: 1.7296\n",
      "Epoch 5/100 - Training Loss: 1.5989\n",
      "Epoch 6/100 - Training Loss: 1.5285\n",
      "Epoch 7/100 - Training Loss: 1.4223\n",
      "Epoch 8/100 - Training Loss: 1.1874\n",
      "Epoch 9/100 - Training Loss: 1.1181\n",
      "Epoch 10/100 - Training Loss: 1.0924\n",
      "Epoch 11/100 - Training Loss: 1.0779\n",
      "Epoch 12/100 - Training Loss: 1.0493\n",
      "Epoch 13/100 - Training Loss: 1.0284\n",
      "Epoch 14/100 - Training Loss: 1.0116\n",
      "Epoch 15/100 - Training Loss: 0.9695\n",
      "Epoch 16/100 - Training Loss: 0.9604\n",
      "Epoch 17/100 - Training Loss: 0.9592\n",
      "Epoch 18/100 - Training Loss: 0.9699\n",
      "Epoch 19/100 - Training Loss: 0.9490\n",
      "Epoch 20/100 - Training Loss: 0.9563\n",
      "Epoch 21/100 - Training Loss: 0.9698\n",
      "Epoch 22/100 - Training Loss: 0.9574\n",
      "Epoch 23/100 - Training Loss: 0.9509\n",
      "Epoch 24/100 - Training Loss: 0.9495\n",
      "Epoch 25/100 - Training Loss: 0.9372\n",
      "Epoch 26/100 - Training Loss: 0.9473\n",
      "Epoch 27/100 - Training Loss: 0.9484\n",
      "Epoch 28/100 - Training Loss: 0.9390\n",
      "Epoch 29/100 - Training Loss: 0.9472\n",
      "Epoch 30/100 - Training Loss: 0.9461\n",
      "Epoch 31/100 - Training Loss: 0.9362\n",
      "Epoch 32/100 - Training Loss: 0.9600\n",
      "Epoch 33/100 - Training Loss: 0.9567\n",
      "Epoch 34/100 - Training Loss: 0.9422\n",
      "Epoch 35/100 - Training Loss: 0.9443\n",
      "Epoch 36/100 - Training Loss: 0.9441\n",
      "Epoch 37/100 - Training Loss: 0.9450\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "for epoch in range(1, epochs + 1):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    #progress_bar = tqdm(train_loader, desc='Epoch {}/{}'.format(epoch, epochs), leave=False)\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    print('Epoch {}/{} - Training Loss: {:.4f}'.format(epoch, epochs, epoch_loss))\n",
    "\n",
    "    exp_lr_scheduler.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'ConvNext_2.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = convnext_small(weights=torchvision.models.ConvNeXt_Small_Weights.IMAGENET1K_V1, progress=True)\n",
    "model.classifier[2].out_features = 9\n",
    "model.load_state_dict(torch.load('ConvNext_1.pth'))\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_images(img_dir, output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    img_filenames = os.listdir(img_dir)\n",
    "    file_ids = set(re.match(r'(\\d+)_', filename).group(1) for filename in img_filenames)\n",
    "\n",
    "    for file_id in file_ids:\n",
    "        blue_img_path = os.path.join(img_dir, f\"{str(file_id).zfill(5)}_blue.png\")\n",
    "        red_img_path = os.path.join(img_dir, f\"{str(file_id).zfill(5)}_red.png\")\n",
    "        yellow_img_path = os.path.join(img_dir, f\"{str(file_id).zfill(5)}_yellow.png\")\n",
    "\n",
    "        blue_img = Image.open(blue_img_path).convert('L')\n",
    "        red_img = Image.open(red_img_path).convert('L')\n",
    "        yellow_img = Image.open(yellow_img_path).convert('L')\n",
    "\n",
    "        combined_img = Image.merge(\"RGB\", (red_img, blue_img, yellow_img))\n",
    "\n",
    "        combined_img_path = os.path.join(output_dir, f\"{str(file_id).zfill(5)}_combined.png\")\n",
    "        combined_img.save(combined_img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine_images(img_dir=\"images_test/images_test/\", output_dir=\"images_test/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CellLineDataset(Dataset):\n",
    "    def __init__(self, img_dir, labels_file=None, transform=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.labels_df = pd.read_csv(labels_file) if labels_file else None\n",
    "        if self.labels_df is not None:\n",
    "            self.has_labels = True\n",
    "            self.class_to_idx = {class_name: i for i, class_name in enumerate(\n",
    "                self.labels_df[\"cell_line\"].unique())}\n",
    "        else:\n",
    "            self.has_labels = False\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.has_labels:\n",
    "            return len(self.labels_df)\n",
    "        else:\n",
    "            return len([file for file in os.listdir(self.img_dir) if file.endswith(\"_combined.png\")])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        if self.has_labels:\n",
    "            row = self.labels_df.iloc[idx]\n",
    "        else:\n",
    "            file_list = sorted([f for f in os.listdir(self.img_dir) if f.endswith(\"_combined.png\")])\n",
    "            file_id_with_zeros = os.path.splitext(file_list[idx])[0].replace('_combined', '')\n",
    "            row = {'file_id': file_id_with_zeros}\n",
    "\n",
    "        sample_id = row['file_id']\n",
    "        img_path = os.path.join(self.img_dir, f\"{sample_id}_combined.png\")\n",
    "        img = Image.open(img_path)\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        if self.has_labels:\n",
    "            # Convert label to integer\n",
    "            label = self.class_to_idx[row['cell_line']]\n",
    "            return img, label\n",
    "        else:\n",
    "            file_id_without_zeros = str(int(sample_id))\n",
    "            return img, file_id_without_zeros\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test dataset length: 6869\n"
     ]
    }
   ],
   "source": [
    "class_to_idx = train_dataset.class_to_idx\n",
    "idx_to_class = {idx: class_name for class_name, idx in class_to_idx.items()}\n",
    "\n",
    "test_dataset = CellLineDataset(\n",
    "    img_dir=\"images_test/\", transform=transform)\n",
    "print(f\"Test dataset length: {len(test_dataset)}\")\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:   0%|          | 0/108 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|██████████| 108/108 [00:56<00:00,  1.90it/s]\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "predictions = []\n",
    "file_ids = []\n",
    "with torch.no_grad():\n",
    "    for inputs, file_id in tqdm(test_loader, desc='Predicting'):\n",
    "        inputs = inputs.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        predictions.extend([idx_to_class[pred.item()] for pred in preds])\n",
    "        file_ids.extend(file_id)\n",
    "\n",
    "# Save predictions to a CSV file\n",
    "df_predictions = pd.DataFrame({'file_id': file_ids, 'cell_line': predictions})\n",
    "df_predictions.to_csv('predictions_convnext_1.csv', index=False)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
