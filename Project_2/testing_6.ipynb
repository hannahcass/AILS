{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RDKit:  2022.09.5\n"
     ]
    }
   ],
   "source": [
    "#from fcd import get_fcd, load_ref_model, canonical_smiles, get_predictions, calculate_frechet_distance\n",
    "import warnings\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy\n",
    "import pickle\n",
    "import rdkit\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from rdkit.Chem import Draw\n",
    "#from rdkit.Chem.Draw import IPythonConsole\n",
    "\n",
    "\n",
    "# Ignore some warnings from RDKIT and keras\n",
    "from rdkit import RDLogger, Chem\n",
    "from torch.nn.functional import one_hot\n",
    "import itertools\n",
    "from functools import reduce\n",
    "from rdkit.Chem import rdMolDescriptors\n",
    "import torch.utils.data as torch_data\n",
    "\n",
    "RDLogger.DisableLog('rdApp.*')\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Load methods from the FCD library\n",
    "\n",
    "np.random.seed(1234)\n",
    "\n",
    "print(\"RDKit: \", rdkit.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"smiles_train.txt\", header=None)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"smiles_train.txt\", \"r\") as f:\n",
    "    smiles = []\n",
    "    for line in f:\n",
    "        # Convert each line to a SMILES string and append to the list\n",
    "        mol = Chem.MolFromSmiles(line.strip())\n",
    "        if mol is not None:\n",
    "            smiles.append(Chem.MolToSmiles(mol))\n",
    "\n",
    "# Write the list of SMILES to a new .smi file\n",
    "with open(\"smiles_train.smi\", \"w\") as f:\n",
    "    for s in smiles:\n",
    "        f.write(s + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_canonical(smiles):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is not None:\n",
    "        return Chem.MolToSmiles(mol, canonical=True)\n",
    "    else:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_canonical = data.apply(to_canonical).dropna().reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_canonical.to_csv(\"canonical_smiles_train.txt\", index=False, header=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\n",
    "    \"canonical_smiles_train.txt\", header=None, squeeze=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"smiles_train.smi\", \"r\") as f:\n",
    "    smiles = [line.strip() for line in f]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_char2index(smiles):\n",
    "    char_set = set()\n",
    "    for smile in smiles:\n",
    "        char_set.update(list(smile))\n",
    "    char_set = sorted(list(char_set))\n",
    "    char_set.extend([\"B\", \"E\", \" \"])  # Add B, E, and space characters\n",
    "    char2index = {char: i for i, char in enumerate(char_set)}\n",
    "    return char2index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "char2index = get_char2index(smiles)\n",
    "max_length = max([len(smile) for smile in smiles]) + 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_data = smiles[:int(\n",
    "    0.9 * len(smiles))], smiles[int(0.9 * len(smiles)):]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_smiles(smiles, char2index, max_length):\n",
    "    target_sequences = []\n",
    "    for smi in smiles:\n",
    "        target_sequence = [char2index[c] for c in smi[1:]]\n",
    "        target_sequence.append(char2index['E'])\n",
    "        if len(target_sequence) < max_length:\n",
    "            target_sequence.extend(\n",
    "                [char2index['P']] * (max_length - len(target_sequence)))\n",
    "        else:\n",
    "            target_sequence = target_sequence[:max_length]\n",
    "\n",
    "        # Check for invalid characters\n",
    "        if any(x >= len(char2index) for x in target_sequence):\n",
    "            print(f\"Invalid characters found in SMILES: {smi}\")\n",
    "            continue\n",
    "\n",
    "        target_sequences.append(target_sequence)\n",
    "\n",
    "    # Use np.int64 instead of np.long\n",
    "    return np.array(target_sequences, dtype=np.int64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SMILESData(torch_data.Dataset):\n",
    "    def __init__(self, smiles, char2index, max_length):\n",
    "        self.smiles = smiles\n",
    "        self.char2index = char2index\n",
    "        self.max_length = max_length\n",
    "        self.target_sequences = vectorize_smiles(\n",
    "            smiles, char2index, max_length)\n",
    "        bad_indices = np.where((self.target_sequences < 0) | (\n",
    "            self.target_sequences >= len(char2index)))\n",
    "\n",
    "\n",
    "        print(\"Bad indices:\", bad_indices)\n",
    "        print(\"Bad values:\", self.target_sequences[bad_indices])\n",
    "\n",
    "        assert np.all(self.target_sequences >= 0) and np.all(\n",
    "            self.target_sequences < len(char2index)), f\"Invalid target values. Min: {self.target_sequences.min()}, Max: {self.target_sequences.max()}, Unique values: {np.unique(self.target_sequences)}\"\n",
    "\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.smiles)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        smile = self.smiles[index]\n",
    "        one_hot = np.zeros((self.max_length - 1, len(  # Change the padding range to self.max_length - 1\n",
    "            self.char2index)), dtype=np.float32)\n",
    "        smile = \"B\" + smile + \"E\"\n",
    "\n",
    "        for j, char in enumerate(smile[:-1]):  # Remove the last character \"E\"\n",
    "            one_hot[j, self.char2index[char]] = 1.0\n",
    "\n",
    "        return one_hot, self.target_sequences[index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bad indices: (array([], dtype=int64), array([], dtype=int64))\n",
      "Bad values: []\n",
      "Bad indices: (array([], dtype=int64), array([], dtype=int64))\n",
      "Bad values: []\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "train_dataset = SMILESData(train_data, char2index, max_length)\n",
    "val_dataset = SMILESData(val_data, char2index, max_length)\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = len(char2index)\n",
    "hidden_size = 512\n",
    "output_size = len(char2index)\n",
    "num_layers = 3\n",
    "num_epochs = 5\n",
    "learning_rate = 0.001\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimplifiedSMILESGRU(nn.Module):\n",
    "    def __init__(self, input_size=input_size, hidden_size=512, output_size=output_size, num_layers=3, dropout=0.2):\n",
    "        super(SimplifiedSMILESGRU, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers,\n",
    "                          batch_first=True, dropout=dropout)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0),\n",
    "                         self.hidden_size).to(device)\n",
    "        out, _ = self.gru(x, h0)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model initialization\n",
    "model = SimplifiedSMILESGRU(input_size, hidden_size,\n",
    "                            output_size, num_layers).to(device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.1, patience=2, verbose=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/5:   0%|          | 0/7289 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-hot data shape: torch.Size([128, 102, 39])\n",
      "Target data shape: torch.Size([128, 103])\n",
      "One-hot data min, max values: tensor(0.) tensor(1.)\n",
      "Target data min, max values: tensor(0) tensor(38)\n",
      "Output shape: torch.Size([128, 102, 39])\n",
      "Target shape: torch.Size([128, 103])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected target size [128, 102], got [128, 103]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 22\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mOutput shape:\u001b[39m\u001b[39m\"\u001b[39m, output\u001b[39m.\u001b[39mshape)\n\u001b[0;32m     20\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mTarget shape:\u001b[39m\u001b[39m\"\u001b[39m, target\u001b[39m.\u001b[39mshape)\n\u001b[1;32m---> 22\u001b[0m loss \u001b[39m=\u001b[39m criterion(output\u001b[39m.\u001b[39;49mtranspose(\u001b[39m1\u001b[39;49m, \u001b[39m2\u001b[39;49m), target)\n\u001b[0;32m     23\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m     24\u001b[0m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mclip_grad_norm_(model\u001b[39m.\u001b[39mparameters(), max_norm\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32md:\\Miniconda\\envs\\ails\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\Miniconda\\envs\\ails\\lib\\site-packages\\torch\\nn\\modules\\loss.py:1164\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1163\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m-> 1164\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mcross_entropy(\u001b[39minput\u001b[39;49m, target, weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[0;32m   1165\u001b[0m                            ignore_index\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mignore_index, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction,\n\u001b[0;32m   1166\u001b[0m                            label_smoothing\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlabel_smoothing)\n",
      "File \u001b[1;32md:\\Miniconda\\envs\\ails\\lib\\site-packages\\torch\\nn\\functional.py:3014\u001b[0m, in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   3012\u001b[0m \u001b[39mif\u001b[39;00m size_average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   3013\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3014\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mcross_entropy_loss(\u001b[39minput\u001b[39;49m, target, weight, _Reduction\u001b[39m.\u001b[39;49mget_enum(reduction), ignore_index, label_smoothing)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected target size [128, 102], got [128, 103]"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_count = 0\n",
    "    for one_hot_data, target in tqdm(train_loader, desc=f\"Training Epoch {epoch + 1}/{num_epochs}\", leave=False):\n",
    "        \n",
    "        one_hot_data, target = one_hot_data.to(device), target.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(one_hot_data.float())\n",
    "\n",
    "        print(\"Output shape:\", output.shape)\n",
    "        print(\"Target shape:\", target.shape)\n",
    "\n",
    "        loss = criterion(output.transpose(1, 2), target)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        train_count += 1\n",
    "\n",
    "    avg_train_loss = train_loss / train_count\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_count = 0\n",
    "    with torch.no_grad():\n",
    "        for one_hot_data, target in tqdm(val_loader, desc=f\"Validating Epoch {epoch + 1}/{num_epochs}\", leave=False):\n",
    "            one_hot_data, target = one_hot_data.to(device), target.to(device)\n",
    "            output = model(one_hot_data.float())\n",
    "            loss = criterion(output.transpose(1, 2), target)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            val_count += 1\n",
    "\n",
    "    avg_val_loss = val_loss / val_count\n",
    "\n",
    "    print(\n",
    "        f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    scheduler.step(avg_val_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_smiles(smiles, char2index, max_length):\n",
    "    target_sequences = []\n",
    "\n",
    "    for smile in smiles:\n",
    "        smile = \"<<BOS>>\" + smile + \"<<EOS>>\"\n",
    "        target_sequence = [char2index[char] for char in smile[1:]] + \\\n",
    "            [char2index[\"<<PAD>>\"]] * (max_length - len(smile))\n",
    "        target_sequences.append(target_sequence)\n",
    "\n",
    "    return np.array(target_sequences, dtype=np.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SMILESData(torch_data.Dataset):\n",
    "    def __init__(self, smiles, char2index, max_length):\n",
    "        self.smiles = smiles\n",
    "        self.char2index = char2index\n",
    "        self.max_length = max_length\n",
    "        self.target_sequences = vectorize_smiles(\n",
    "            smiles, char2index, max_length)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.smiles)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        smile = self.smiles[index]\n",
    "        one_hot = np.zeros((self.max_length, len(\n",
    "            self.char2index)), dtype=np.float32)\n",
    "        smile = \"<<BOS>>\" + smile + \"<<EOS>>\"\n",
    "\n",
    "        for j, char in enumerate(smile):\n",
    "            one_hot[j, self.char2index[char]] = 1.0\n",
    "\n",
    "        return one_hot, self.target_sequences[index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'<'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[158], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_dataset \u001b[39m=\u001b[39m SMILESData(train_data, char2index, max_length)\n\u001b[0;32m      2\u001b[0m val_dataset \u001b[39m=\u001b[39m SMILESData(val_data, char2index, max_length)\n\u001b[0;32m      3\u001b[0m train_loader \u001b[39m=\u001b[39m DataLoader(train_dataset, batch_size\u001b[39m=\u001b[39m\u001b[39m128\u001b[39m, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[157], line 6\u001b[0m, in \u001b[0;36mSMILESData.__init__\u001b[1;34m(self, smiles, char2index, max_length)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchar2index \u001b[39m=\u001b[39m char2index\n\u001b[0;32m      5\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_length \u001b[39m=\u001b[39m max_length\n\u001b[1;32m----> 6\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_sequences \u001b[39m=\u001b[39m vectorize_smiles(\n\u001b[0;32m      7\u001b[0m     smiles, char2index, max_length)\n",
      "Cell \u001b[1;32mIn[156], line 6\u001b[0m, in \u001b[0;36mvectorize_smiles\u001b[1;34m(smiles, char2index, max_length)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[39mfor\u001b[39;00m smile \u001b[39min\u001b[39;00m smiles:\n\u001b[0;32m      5\u001b[0m     smile \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m<<BOS>>\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m smile \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m<<EOS>>\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m----> 6\u001b[0m     target_sequence \u001b[39m=\u001b[39m [char2index[char] \u001b[39mfor\u001b[39;00m char \u001b[39min\u001b[39;00m smile[\u001b[39m1\u001b[39m:]] \u001b[39m+\u001b[39m \\\n\u001b[0;32m      7\u001b[0m         [char2index[\u001b[39m\"\u001b[39m\u001b[39m<<PAD>>\u001b[39m\u001b[39m\"\u001b[39m]] \u001b[39m*\u001b[39m (max_length \u001b[39m-\u001b[39m \u001b[39mlen\u001b[39m(smile))\n\u001b[0;32m      8\u001b[0m     target_sequences\u001b[39m.\u001b[39mappend(target_sequence)\n\u001b[0;32m     10\u001b[0m \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39marray(target_sequences, dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mlong)\n",
      "Cell \u001b[1;32mIn[156], line 6\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[39mfor\u001b[39;00m smile \u001b[39min\u001b[39;00m smiles:\n\u001b[0;32m      5\u001b[0m     smile \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m<<BOS>>\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m smile \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m<<EOS>>\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m----> 6\u001b[0m     target_sequence \u001b[39m=\u001b[39m [char2index[char] \u001b[39mfor\u001b[39;00m char \u001b[39min\u001b[39;00m smile[\u001b[39m1\u001b[39m:]] \u001b[39m+\u001b[39m \\\n\u001b[0;32m      7\u001b[0m         [char2index[\u001b[39m\"\u001b[39m\u001b[39m<<PAD>>\u001b[39m\u001b[39m\"\u001b[39m]] \u001b[39m*\u001b[39m (max_length \u001b[39m-\u001b[39m \u001b[39mlen\u001b[39m(smile))\n\u001b[0;32m      8\u001b[0m     target_sequences\u001b[39m.\u001b[39mappend(target_sequence)\n\u001b[0;32m     10\u001b[0m \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39marray(target_sequences, dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mlong)\n",
      "\u001b[1;31mKeyError\u001b[0m: '<'"
     ]
    }
   ],
   "source": [
    "train_dataset = SMILESData(train_data, char2index, max_length)\n",
    "val_dataset = SMILESData(val_data, char2index, max_length)\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "input_size = len(char2index)\n",
    "hidden_size = 512\n",
    "output_size = len(char2index)\n",
    "num_layers = 3\n",
    "num_epochs = 5\n",
    "learning_rate = 0.001\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimplifiedSMILESGRU(nn.Module):\n",
    "    def __init__(self, input_size=input_size, hidden_size=512, output_size=output_size, num_layers=3, dropout=0.2):\n",
    "        super(SimplifiedSMILESGRU, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers,\n",
    "                          batch_first=True, dropout=dropout)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0),\n",
    "                         self.hidden_size).to(device)\n",
    "        out, _ = self.gru(x, h0)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = SimplifiedSMILESGRU(input_size, hidden_size,\n",
    "#                             output_size, num_layers).to(device)\n",
    "model = SimplifiedSMILESLSTM(input_size, hidden_size,\n",
    "                             output_size, num_layers).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "\n",
    "scheduler = ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.1, patience=2, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'>'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[125], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m train_loss \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m      5\u001b[0m train_count \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m----> 6\u001b[0m \u001b[39mfor\u001b[39;00m one_hot_data, target \u001b[39min\u001b[39;00m tqdm(train_loader, desc\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTraining Epoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m \u001b[39m\u001b[39m+\u001b[39m\u001b[39m \u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mnum_epochs\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m, leave\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m      7\u001b[0m     one_hot_data, target \u001b[39m=\u001b[39m one_hot_data\u001b[39m.\u001b[39mto(device), target\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m      9\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32md:\\Miniconda\\envs\\ails\\lib\\site-packages\\tqdm\\std.py:1178\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1175\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[0;32m   1177\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1178\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[0;32m   1179\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[0;32m   1180\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1181\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32md:\\Miniconda\\envs\\ails\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:681\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    678\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    679\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    680\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 681\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    682\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    683\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    684\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    685\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32md:\\Miniconda\\envs\\ails\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:721\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    719\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    720\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 721\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    722\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    723\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32md:\\Miniconda\\envs\\ails\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32md:\\Miniconda\\envs\\ails\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[121], line 15\u001b[0m, in \u001b[0;36mSMILESData.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, index):\n\u001b[0;32m     14\u001b[0m     smile \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msmiles[index]\n\u001b[1;32m---> 15\u001b[0m     one_hot, target_sequence \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvectorize_smile(smile)\n\u001b[0;32m     16\u001b[0m     \u001b[39mreturn\u001b[39;00m one_hot, target_sequence\n",
      "Cell \u001b[1;32mIn[121], line 22\u001b[0m, in \u001b[0;36mSMILESData.vectorize_smile\u001b[1;34m(self, smile)\u001b[0m\n\u001b[0;32m     19\u001b[0m one_hot \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros((\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_length, \u001b[39mlen\u001b[39m(\n\u001b[0;32m     20\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchar2index)), dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mfloat32)\n\u001b[0;32m     21\u001b[0m smile \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m<BOS>\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m smile \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m<EOS>\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m---> 22\u001b[0m target_sequence \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchar2index[char] \u001b[39mfor\u001b[39;00m char \u001b[39min\u001b[39;00m smile[\u001b[39m1\u001b[39m:]] \u001b[39m+\u001b[39m \\\n\u001b[0;32m     23\u001b[0m     [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchar2index[\u001b[39m\"\u001b[39m\u001b[39m<PAD>\u001b[39m\u001b[39m\"\u001b[39m]] \u001b[39m*\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_length \u001b[39m-\u001b[39m \u001b[39mlen\u001b[39m(smile))\n\u001b[0;32m     25\u001b[0m \u001b[39mfor\u001b[39;00m j, char \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(smile):\n\u001b[0;32m     26\u001b[0m     one_hot[j, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchar2index[char]] \u001b[39m=\u001b[39m \u001b[39m1.0\u001b[39m\n",
      "Cell \u001b[1;32mIn[121], line 22\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     19\u001b[0m one_hot \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros((\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_length, \u001b[39mlen\u001b[39m(\n\u001b[0;32m     20\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchar2index)), dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mfloat32)\n\u001b[0;32m     21\u001b[0m smile \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m<BOS>\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m smile \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m<EOS>\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m---> 22\u001b[0m target_sequence \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mchar2index[char] \u001b[39mfor\u001b[39;00m char \u001b[39min\u001b[39;00m smile[\u001b[39m1\u001b[39m:]] \u001b[39m+\u001b[39m \\\n\u001b[0;32m     23\u001b[0m     [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchar2index[\u001b[39m\"\u001b[39m\u001b[39m<PAD>\u001b[39m\u001b[39m\"\u001b[39m]] \u001b[39m*\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_length \u001b[39m-\u001b[39m \u001b[39mlen\u001b[39m(smile))\n\u001b[0;32m     25\u001b[0m \u001b[39mfor\u001b[39;00m j, char \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(smile):\n\u001b[0;32m     26\u001b[0m     one_hot[j, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchar2index[char]] \u001b[39m=\u001b[39m \u001b[39m1.0\u001b[39m\n",
      "\u001b[1;31mKeyError\u001b[0m: '>'"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_count = 0\n",
    "    for one_hot_data, target in tqdm(train_loader, desc=f\"Training Epoch {epoch + 1}/{num_epochs}\", leave=False):\n",
    "        one_hot_data, target = one_hot_data.to(device), target.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(one_hot_data.float())\n",
    "\n",
    "        loss = criterion(output.transpose(1, 2), target)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        train_count += 1\n",
    "\n",
    "    avg_train_loss = train_loss / train_count\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_count = 0\n",
    "    with torch.no_grad():\n",
    "        for one_hot_data, target in tqdm(val_loader, desc=f\"Validating Epoch {epoch + 1}/{num_epochs}\", leave=False):\n",
    "            one_hot_data, target = one_hot_data.to(device), target.to(device)\n",
    "            output = model(one_hot_data.float())\n",
    "            loss = criterion(output.transpose(1, 2), target)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            val_count += 1\n",
    "\n",
    "    avg_val_loss = val_loss / val_count\n",
    "\n",
    "    print(\n",
    "        f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}\")\n",
    "    \n",
    "    scheduler.step(avg_val_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"trained_model_lstm_10.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimplifiedSMILESGRU(\n",
       "  (gru): GRU(40, 512, num_layers=3, batch_first=True, dropout=0.2)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       "  (fc): Linear(in_features=512, out_features=40, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"trained_model_lstm_8.pth\"))\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_smiles(smiles, token2index, max_length):\n",
    "    vector = [1]  # <BOS>\n",
    "    for char in smiles:\n",
    "        vector.append(token2index[char])\n",
    "    vector.append(2)  # <EOS>\n",
    "    while len(vector) < max_length:\n",
    "        vector.append(0)  # <PAD>\n",
    "    return vector[:max_length]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unvectorize_smiles(vector, index2token):\n",
    "    smiles = \"\"\n",
    "    for index in vector:\n",
    "        if index in __special__:\n",
    "            continue\n",
    "        smiles += index2token[index]\n",
    "    return smiles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_smiles(smiles):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    return mol is not None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, input_smiles, charset, max_length, temperature=1.6):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        data = vectorize_smiles(input_smiles, charset, max_length)\n",
    "        one_hot_data = torch.zeros(1, data.size(0), len(charset)).to(device)\n",
    "        one_hot_data.scatter_(2, data.to(device).unsqueeze(0).unsqueeze(2), 1)\n",
    "        output = model(one_hot_data.float())\n",
    "\n",
    "        # Apply temperature\n",
    "        output.div_(temperature)\n",
    "        probabilities = F.softmax(output, dim=2)\n",
    "\n",
    "        # Remove batch dimension\n",
    "        probabilities = probabilities.squeeze(0)\n",
    "\n",
    "        # Sample from the distribution\n",
    "        sampled_indices = torch.multinomial(probabilities, 1).squeeze(1)\n",
    "        output_smiles = unvectorize_smiles(sampled_indices.tolist(), charset)\n",
    "        if not output_smiles:\n",
    "            return None\n",
    "        return output_smiles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_count = 0\n",
    "smiles_gen = set()\n",
    "\n",
    "with open(\"predictions_lstm_10.txt\", \"w\") as f:\n",
    "    while valid_count < 10001:\n",
    "        input_smiles = sample_random_seed(list(data))\n",
    "        predicted_smiles = predict(model, input_smiles, charset, max_length)\n",
    "        if predicted_smiles is not None and is_valid_smiles(predicted_smiles):\n",
    "            smiles_gen.add(predicted_smiles)\n",
    "            valid_count += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit import DataStructs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 22\u001b[0m\n\u001b[0;32m     20\u001b[0m is_similar \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[39mfor\u001b[39;00m smiles \u001b[39min\u001b[39;00m smiles_gen:\n\u001b[1;32m---> 22\u001b[0m     fp \u001b[39m=\u001b[39m get_fingerprint(smiles)\n\u001b[0;32m     23\u001b[0m     similarity \u001b[39m=\u001b[39m DataStructs\u001b[39m.\u001b[39mTanimotoSimilarity(fp, fp_pred)\n\u001b[0;32m     24\u001b[0m     \u001b[39mif\u001b[39;00m similarity \u001b[39m>\u001b[39m threshold:\n",
      "Cell \u001b[1;32mIn[24], line 3\u001b[0m, in \u001b[0;36mget_fingerprint\u001b[1;34m(smiles)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_fingerprint\u001b[39m(smiles):\n\u001b[0;32m      2\u001b[0m     mol \u001b[39m=\u001b[39m Chem\u001b[39m.\u001b[39mMolFromSmiles(smiles)\n\u001b[1;32m----> 3\u001b[0m     fp \u001b[39m=\u001b[39m AllChem\u001b[39m.\u001b[39mGetMorganFingerprint(mol, \u001b[39m2\u001b[39m)\n\u001b[0;32m      4\u001b[0m     \u001b[39mreturn\u001b[39;00m fp\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def get_fingerprint(smiles):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    fp = AllChem.GetMorganFingerprint(mol, 2)\n",
    "    return fp\n",
    "\n",
    "\n",
    "threshold = 0.8\n",
    "\n",
    "valid_count = 0\n",
    "smiles_gen = set()\n",
    "with open(\"predictions_lstm_test.txt\", \"w\") as f:\n",
    "    for input_smiles in val_data:\n",
    "        predicted_smiles = predict(model, input_smiles, charset, max_length)\n",
    "        # print(f\"{predicted_smiles}\")\n",
    "        if predicted_smiles is not None and is_valid_smiles(predicted_smiles):\n",
    "            if predicted_smiles not in data_canonical:\n",
    "                # Compute the fingerprint of the predicted SMILES\n",
    "                fp_pred = get_fingerprint(predicted_smiles)\n",
    "                # Check if the predicted SMILES is similar to any of the previously generated SMILES\n",
    "                is_similar = False\n",
    "                for smiles in smiles_gen:\n",
    "                    fp = get_fingerprint(smiles)\n",
    "                    similarity = DataStructs.TanimotoSimilarity(fp, fp_pred)\n",
    "                    if similarity > threshold:\n",
    "                        is_similar = True\n",
    "                        break\n",
    "                if not is_similar:\n",
    "                    smiles_gen.add(predicted_smiles)\n",
    "                    f.write(predicted_smiles + \"\\n\")\n",
    "                    valid_count += 1\n",
    "                if valid_count >= 10001:\n",
    "                    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ails-fcd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
