{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:rgb(0,120,170)\">Artificial Intelligence in Life Sciences</h1>\n",
    "<h2 style=\"color:rgb(0,120,170)\">QSAR and model evaluation</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Authors:</b> Rumetshofer, Renz, Schimunek <br>\n",
    "<b>Date:</b> 24-03-2022\n",
    "\n",
    "This file is part of the \"Artificial Intelligence in Life Sciences\" lecture material.\n",
    "The following copyright statement applies to all code within this file.\n",
    "\n",
    "<b>Copyright statement:</b><br>\n",
    "This material, no matter whether in printed or electronic form, may be used for personal and non-commercial educational\n",
    "use only. Any reproduction of this manuscript, no matter whether as a whole or in parts, no matter whether in printed\n",
    "or in electronic form, requires explicit prior acceptance of the authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022.09.5\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMClassifier\n",
    "import optuna\n",
    "from sklearn.calibration import CalibratedClassifierCV, calibration_curve\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "import rdkit\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem, DataStructs\n",
    "\n",
    "from rdkit import RDLogger  \n",
    "RDLogger.DisableLog('rdApp.*') \n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print(rdkit.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>smiles</th>\n",
       "      <th>task1</th>\n",
       "      <th>task2</th>\n",
       "      <th>task3</th>\n",
       "      <th>task4</th>\n",
       "      <th>task5</th>\n",
       "      <th>task6</th>\n",
       "      <th>task7</th>\n",
       "      <th>task8</th>\n",
       "      <th>task9</th>\n",
       "      <th>task10</th>\n",
       "      <th>task11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CC(=O)N(C)c1cccc(-c2ccnc3c(C(=O)c4cccs4)cnn23)c1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>COc1cc(N)c(Cl)cc1C(=O)OCCCN1CCCCC1.Cl</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CCCCNc1c(C(=O)OCC)cnc2c1cnn2CC</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C#Cc1cccc(Nc2ncnc3cc(OCCOC)c(OCCOC)cc23)c1.Cl</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CC1OC2(CCCCC2Oc2cccc(Cl)c2)N=C1O</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11995</th>\n",
       "      <td>CC(C)(C)NC[C@@H](O)COc1nsnc1N1CCOCC1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11996</th>\n",
       "      <td>CCC[C@@]1(CCc2ccccc2)CC(O)=C([C@H](CC)c2cccc(N...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11997</th>\n",
       "      <td>N=C(O)c1cnc(C2CC2)[nH]1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11998</th>\n",
       "      <td>CN=C=O</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11999</th>\n",
       "      <td>Cc1ccc2c(c1N)C(=O)c1ccccc1C2=O</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12000 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  smiles  task1  task2  task3  \\\n",
       "0       CC(=O)N(C)c1cccc(-c2ccnc3c(C(=O)c4cccs4)cnn23)c1      0      0      0   \n",
       "1                  COc1cc(N)c(Cl)cc1C(=O)OCCCN1CCCCC1.Cl      0      0      0   \n",
       "2                         CCCCNc1c(C(=O)OCC)cnc2c1cnn2CC      0      0      0   \n",
       "3          C#Cc1cccc(Nc2ncnc3cc(OCCOC)c(OCCOC)cc23)c1.Cl      0      0      0   \n",
       "4                       CC1OC2(CCCCC2Oc2cccc(Cl)c2)N=C1O      0      0      0   \n",
       "...                                                  ...    ...    ...    ...   \n",
       "11995               CC(C)(C)NC[C@@H](O)COc1nsnc1N1CCOCC1      0      0      0   \n",
       "11996  CCC[C@@]1(CCc2ccccc2)CC(O)=C([C@H](CC)c2cccc(N...      0      0      0   \n",
       "11997                            N=C(O)c1cnc(C2CC2)[nH]1      0      0      0   \n",
       "11998                                             CN=C=O      0      0      0   \n",
       "11999                     Cc1ccc2c(c1N)C(=O)c1ccccc1C2=O      0      0      0   \n",
       "\n",
       "       task4  task5  task6  task7  task8  task9  task10  task11  \n",
       "0          0      0      0      0     -1      0       0       0  \n",
       "1          0      0      0      0     -1      0       0       0  \n",
       "2          0      0      0      0      0      0       1       0  \n",
       "3          0      0      0      0     -1      0       0       1  \n",
       "4          0      0      0      0      0      0       1       0  \n",
       "...      ...    ...    ...    ...    ...    ...     ...     ...  \n",
       "11995      0      0      0      0     -1      0       0       0  \n",
       "11996      0      0      0      0     -1      0       0       0  \n",
       "11997     -1      0      0      0      0      0       0       0  \n",
       "11998      0      0      0     -1      0      0       0       0  \n",
       "11999      0      0      0     -1      0      1       0       0  \n",
       "\n",
       "[12000 rows x 12 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocessed Tox21 dataset with pre-assigned clusters\n",
    "data = pd.read_csv(\"data_train.csv\",index_col=0).reset_index(drop=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:rgb(0,120,170)\">Data preprocessing</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use the dataset for training a model we replace the missing values with `-1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 1, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 1, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select labels, convert to numpy array\n",
    "y = data[data.columns[1:]].to_numpy()\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12000, 11)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we calculate Morgan fingerprints from the Smiles string for each sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12000/12000 [00:07<00:00, 1574.34it/s]\n"
     ]
    }
   ],
   "source": [
    "# Initialize variables\n",
    "fp_length = 1024\n",
    "fps = np.zeros((len(data), fp_length))\n",
    "\n",
    "# Calculate Morgan fingerprints and convert to numpy array\n",
    "for i, smiles in enumerate(tqdm(data['smiles'])):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    fp_vec = AllChem.GetMorganFingerprintAsBitVect(mol, radius=3, nBits=fp_length)\n",
    "    arr = np.zeros((1,))\n",
    "    DataStructs.ConvertToNumpyArray(fp_vec, arr)\n",
    "    fps[i] = arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 1., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:rgb(0,120,170)\">Train model on random split</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fps, val_fps, y_train, y_val = train_test_split(fps, y,\n",
    "                                                      test_size=0.2, random_state=1234)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-04-10 16:03:38,839]\u001b[0m A new study created in memory with name: no-name-52325624-0587-4a0d-b5ff-541906d70321\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 363, number of negative: 9237\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.028930 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2048\n",
      "[LightGBM] [Info] Number of data points in the train set: 9600, number of used features: 1024\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037812 -> initscore=-3.236570\n",
      "[LightGBM] [Info] Start training from score -3.236570\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-04-10 16:03:39,352]\u001b[0m Trial 0 finished with value: 0.15828870814679985 and parameters: {'num_leaves': 14, 'learning_rate': 0.00041691523050076107, 'feature_fraction': 0.7697668301022773, 'bagging_fraction': 0.7328152528063749, 'bagging_freq': 5, 'min_child_samples': 89}. Best is trial 0 with value: 0.15828870814679985.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 363, number of negative: 9237\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.030415 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2048\n",
      "[LightGBM] [Info] Number of data points in the train set: 9600, number of used features: 1024\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037812 -> initscore=-3.236570\n",
      "[LightGBM] [Info] Start training from score -3.236570\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-04-10 16:03:39,914]\u001b[0m Trial 1 finished with value: 0.15353160616368425 and parameters: {'num_leaves': 24, 'learning_rate': 0.00042792163785502834, 'feature_fraction': 0.8550502363116506, 'bagging_fraction': 0.47650529772600936, 'bagging_freq': 6, 'min_child_samples': 12}. Best is trial 1 with value: 0.15353160616368425.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 363, number of negative: 9237\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.024244 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2048\n",
      "[LightGBM] [Info] Number of data points in the train set: 9600, number of used features: 1024\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037812 -> initscore=-3.236570\n",
      "[LightGBM] [Info] Start training from score -3.236570\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-04-10 16:03:40,781]\u001b[0m Trial 2 finished with value: 0.1554987951463129 and parameters: {'num_leaves': 62, 'learning_rate': 0.0005456315611098472, 'feature_fraction': 0.4172094633527854, 'bagging_fraction': 0.7366541429839353, 'bagging_freq': 7, 'min_child_samples': 83}. Best is trial 1 with value: 0.15353160616368425.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 363, number of negative: 9237\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.024565 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2048\n",
      "[LightGBM] [Info] Number of data points in the train set: 9600, number of used features: 1024\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037812 -> initscore=-3.236570\n",
      "[LightGBM] [Info] Start training from score -3.236570\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-04-10 16:03:41,557]\u001b[0m Trial 3 finished with value: 0.13972348527770923 and parameters: {'num_leaves': 67, 'learning_rate': 0.0010224113938887183, 'feature_fraction': 0.7552169527412081, 'bagging_fraction': 0.7490607160405172, 'bagging_freq': 6, 'min_child_samples': 42}. Best is trial 3 with value: 0.13972348527770923.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 363, number of negative: 9237\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019664 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2048\n",
      "[LightGBM] [Info] Number of data points in the train set: 9600, number of used features: 1024\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037812 -> initscore=-3.236570\n",
      "[LightGBM] [Info] Start training from score -3.236570\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-04-10 16:03:42,217]\u001b[0m Trial 4 finished with value: 0.11458489466799475 and parameters: {'num_leaves': 34, 'learning_rate': 0.0030167655409689942, 'feature_fraction': 0.48723498361498635, 'bagging_fraction': 0.7228428441574727, 'bagging_freq': 1, 'min_child_samples': 62}. Best is trial 4 with value: 0.11458489466799475.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 363, number of negative: 9237\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018782 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2048\n",
      "[LightGBM] [Info] Number of data points in the train set: 9600, number of used features: 1024\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037812 -> initscore=-3.236570\n",
      "[LightGBM] [Info] Start training from score -3.236570\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-04-10 16:03:42,706]\u001b[0m Trial 5 finished with value: 0.11265610883266913 and parameters: {'num_leaves': 37, 'learning_rate': 0.002579738760624978, 'feature_fraction': 0.682911301254642, 'bagging_fraction': 0.4134989399558773, 'bagging_freq': 2, 'min_child_samples': 14}. Best is trial 5 with value: 0.11265610883266913.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 363, number of negative: 9237\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017133 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2048\n",
      "[LightGBM] [Info] Number of data points in the train set: 9600, number of used features: 1024\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037812 -> initscore=-3.236570\n",
      "[LightGBM] [Info] Start training from score -3.236570\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-04-10 16:03:43,526]\u001b[0m Trial 6 finished with value: 0.05667429623602807 and parameters: {'num_leaves': 76, 'learning_rate': 0.01575263709318802, 'feature_fraction': 0.5505094100818214, 'bagging_fraction': 0.9619942129954822, 'bagging_freq': 1, 'min_child_samples': 60}. Best is trial 6 with value: 0.05667429623602807.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 363, number of negative: 9237\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016055 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2048\n",
      "[LightGBM] [Info] Number of data points in the train set: 9600, number of used features: 1024\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037812 -> initscore=-3.236570\n",
      "[LightGBM] [Info] Start training from score -3.236570\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-04-10 16:03:44,142]\u001b[0m Trial 7 finished with value: 0.042267052643796474 and parameters: {'num_leaves': 76, 'learning_rate': 0.030919612603084005, 'feature_fraction': 0.7838984203280872, 'bagging_fraction': 0.7379818943901397, 'bagging_freq': 3, 'min_child_samples': 20}. Best is trial 7 with value: 0.042267052643796474.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 363, number of negative: 9237\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015348 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2048\n",
      "[LightGBM] [Info] Number of data points in the train set: 9600, number of used features: 1024\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037812 -> initscore=-3.236570\n",
      "[LightGBM] [Info] Start training from score -3.236570\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-04-10 16:03:44,732]\u001b[0m Trial 8 finished with value: 0.05712517229689763 and parameters: {'num_leaves': 75, 'learning_rate': 0.016492243483559356, 'feature_fraction': 0.9964328239929403, 'bagging_fraction': 0.8496160182632138, 'bagging_freq': 6, 'min_child_samples': 62}. Best is trial 7 with value: 0.042267052643796474.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 363, number of negative: 9237\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015998 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2048\n",
      "[LightGBM] [Info] Number of data points in the train set: 9600, number of used features: 1024\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037812 -> initscore=-3.236570\n",
      "[LightGBM] [Info] Start training from score -3.236570\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-04-10 16:03:45,290]\u001b[0m Trial 9 finished with value: 0.15272325227201555 and parameters: {'num_leaves': 81, 'learning_rate': 0.0005967761058727991, 'feature_fraction': 0.6770631884567345, 'bagging_fraction': 0.5461107603244645, 'bagging_freq': 3, 'min_child_samples': 48}. Best is trial 7 with value: 0.042267052643796474.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 363, number of negative: 9237\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015997 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2048\n",
      "[LightGBM] [Info] Number of data points in the train set: 9600, number of used features: 1024\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037812 -> initscore=-3.236570\n",
      "[LightGBM] [Info] Start training from score -3.236570\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-04-10 16:03:45,684]\u001b[0m Trial 10 finished with value: 0.047647700656292674 and parameters: {'num_leaves': 99, 'learning_rate': 0.19912047475384628, 'feature_fraction': 0.883668353980444, 'bagging_fraction': 0.5869105440757193, 'bagging_freq': 4, 'min_child_samples': 32}. Best is trial 7 with value: 0.042267052643796474.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 363, number of negative: 9237\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012543 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2048\n",
      "[LightGBM] [Info] Number of data points in the train set: 9600, number of used features: 1024\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037812 -> initscore=-3.236570\n",
      "[LightGBM] [Info] Start training from score -3.236570\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-04-10 16:03:46,038]\u001b[0m Trial 11 finished with value: 0.04711186555966518 and parameters: {'num_leaves': 100, 'learning_rate': 0.17222230320375245, 'feature_fraction': 0.8942664545113046, 'bagging_fraction': 0.5831365757516519, 'bagging_freq': 4, 'min_child_samples': 30}. Best is trial 7 with value: 0.042267052643796474.\u001b[0m\n",
      "\u001b[32m[I 2023-04-10 16:03:46,359]\u001b[0m Trial 12 finished with value: 0.04573796851241448 and parameters: {'num_leaves': 99, 'learning_rate': 0.15983672327752205, 'feature_fraction': 0.9047714190143895, 'bagging_fraction': 0.6220066853531836, 'bagging_freq': 3, 'min_child_samples': 27}. Best is trial 7 with value: 0.042267052643796474.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 363, number of negative: 9237\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013602 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2048\n",
      "[LightGBM] [Info] Number of data points in the train set: 9600, number of used features: 1024\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037812 -> initscore=-3.236570\n",
      "[LightGBM] [Info] Start training from score -3.236570\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 363, number of negative: 9237\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012456 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2048\n",
      "[LightGBM] [Info] Number of data points in the train set: 9600, number of used features: 1024\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037812 -> initscore=-3.236570\n",
      "[LightGBM] [Info] Start training from score -3.236570\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-04-10 16:03:46,922]\u001b[0m Trial 13 finished with value: 0.042414353958880546 and parameters: {'num_leaves': 89, 'learning_rate': 0.0378012873864016, 'feature_fraction': 0.950073814145337, 'bagging_fraction': 0.6393300256173694, 'bagging_freq': 3, 'min_child_samples': 25}. Best is trial 7 with value: 0.042267052643796474.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 363, number of negative: 9237\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012333 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2048\n",
      "[LightGBM] [Info] Number of data points in the train set: 9600, number of used features: 1024\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037812 -> initscore=-3.236570\n",
      "[LightGBM] [Info] Start training from score -3.236570\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-04-10 16:03:47,327]\u001b[0m Trial 14 finished with value: 0.16526103956397448 and parameters: {'num_leaves': 49, 'learning_rate': 0.00011739003015105583, 'feature_fraction': 0.9585409797505818, 'bagging_fraction': 0.6581890180731815, 'bagging_freq': 3, 'min_child_samples': 5}. Best is trial 7 with value: 0.042267052643796474.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 363, number of negative: 9237\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011962 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2048\n",
      "[LightGBM] [Info] Number of data points in the train set: 9600, number of used features: 1024\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037812 -> initscore=-3.236570\n",
      "[LightGBM] [Info] Start training from score -3.236570\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-04-10 16:03:47,909]\u001b[0m Trial 15 finished with value: 0.042627445851485875 and parameters: {'num_leaves': 87, 'learning_rate': 0.03745800014199516, 'feature_fraction': 0.808592091982965, 'bagging_fraction': 0.8256932745158593, 'bagging_freq': 2, 'min_child_samples': 23}. Best is trial 7 with value: 0.042267052643796474.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 363, number of negative: 9237\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012450 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2048\n",
      "[LightGBM] [Info] Number of data points in the train set: 9600, number of used features: 1024\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037812 -> initscore=-3.236570\n",
      "[LightGBM] [Info] Start training from score -3.236570\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-04-10 16:03:48,374]\u001b[0m Trial 16 finished with value: 0.041213579114061 and parameters: {'num_leaves': 54, 'learning_rate': 0.04399133587526455, 'feature_fraction': 0.9874834831481893, 'bagging_fraction': 0.6665414225921092, 'bagging_freq': 2, 'min_child_samples': 39}. Best is trial 16 with value: 0.041213579114061.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 363, number of negative: 9237\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011623 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2048\n",
      "[LightGBM] [Info] Number of data points in the train set: 9600, number of used features: 1024\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037812 -> initscore=-3.236570\n",
      "[LightGBM] [Info] Start training from score -3.236570\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-04-10 16:03:48,769]\u001b[0m Trial 17 finished with value: 0.08106387699093927 and parameters: {'num_leaves': 44, 'learning_rate': 0.007132802415947579, 'feature_fraction': 0.824760559191716, 'bagging_fraction': 0.6700960157258209, 'bagging_freq': 2, 'min_child_samples': 42}. Best is trial 16 with value: 0.041213579114061.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 363, number of negative: 9237\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011817 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2048\n",
      "[LightGBM] [Info] Number of data points in the train set: 9600, number of used features: 1024\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037812 -> initscore=-3.236570\n",
      "[LightGBM] [Info] Start training from score -3.236570\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-04-10 16:03:49,182]\u001b[0m Trial 18 finished with value: 0.04396862478474068 and parameters: {'num_leaves': 57, 'learning_rate': 0.0613760723073035, 'feature_fraction': 0.980947785346699, 'bagging_fraction': 0.5294250282947607, 'bagging_freq': 4, 'min_child_samples': 78}. Best is trial 16 with value: 0.041213579114061.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 363, number of negative: 9237\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010603 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2048\n",
      "[LightGBM] [Info] Number of data points in the train set: 9600, number of used features: 1024\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037812 -> initscore=-3.236570\n",
      "[LightGBM] [Info] Start training from score -3.236570\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-04-10 16:03:49,511]\u001b[0m Trial 19 finished with value: 0.04248684930256256 and parameters: {'num_leaves': 67, 'learning_rate': 0.0701717669335135, 'feature_fraction': 0.6123783635070883, 'bagging_fraction': 0.8129679036428293, 'bagging_freq': 1, 'min_child_samples': 37}. Best is trial 16 with value: 0.041213579114061.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 363, number of negative: 9237\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012171 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2048\n",
      "[LightGBM] [Info] Number of data points in the train set: 9600, number of used features: 1024\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037812 -> initscore=-3.236570\n",
      "[LightGBM] [Info] Start training from score -3.236570\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-04-10 16:03:49,963]\u001b[0m Trial 20 finished with value: 0.07316532147529235 and parameters: {'num_leaves': 53, 'learning_rate': 0.011675957698670789, 'feature_fraction': 0.7308055457514926, 'bagging_fraction': 0.6815949192243704, 'bagging_freq': 2, 'min_child_samples': 99}. Best is trial 16 with value: 0.041213579114061.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 363, number of negative: 9237\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011513 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2048\n",
      "[LightGBM] [Info] Number of data points in the train set: 9600, number of used features: 1024\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037812 -> initscore=-3.236570\n",
      "[LightGBM] [Info] Start training from score -3.236570\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-04-10 16:03:50,511]\u001b[0m Trial 21 finished with value: 0.043640978492507886 and parameters: {'num_leaves': 89, 'learning_rate': 0.03139927992286344, 'feature_fraction': 0.9255269269705545, 'bagging_fraction': 0.6392867202235271, 'bagging_freq': 3, 'min_child_samples': 19}. Best is trial 16 with value: 0.041213579114061.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 363, number of negative: 9237\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011483 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2048\n",
      "[LightGBM] [Info] Number of data points in the train set: 9600, number of used features: 1024\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037812 -> initscore=-3.236570\n",
      "[LightGBM] [Info] Start training from score -3.236570\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-04-10 16:03:51,098]\u001b[0m Trial 22 finished with value: 0.044746610939897756 and parameters: {'num_leaves': 88, 'learning_rate': 0.037726044921004835, 'feature_fraction': 0.9483393810160006, 'bagging_fraction': 0.6288129755508269, 'bagging_freq': 4, 'min_child_samples': 54}. Best is trial 16 with value: 0.041213579114061.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 363, number of negative: 9237\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012907 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2048\n",
      "[LightGBM] [Info] Number of data points in the train set: 9600, number of used features: 1024\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037812 -> initscore=-3.236570\n",
      "[LightGBM] [Info] Start training from score -3.236570\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-04-10 16:03:51,442]\u001b[0m Trial 23 finished with value: 0.04850768544003736 and parameters: {'num_leaves': 70, 'learning_rate': 0.07801703783771641, 'feature_fraction': 0.9990528074408153, 'bagging_fraction': 0.705886208710587, 'bagging_freq': 3, 'min_child_samples': 9}. Best is trial 16 with value: 0.041213579114061.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 363, number of negative: 9237\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012028 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2048\n",
      "[LightGBM] [Info] Number of data points in the train set: 9600, number of used features: 1024\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037812 -> initscore=-3.236570\n",
      "[LightGBM] [Info] Start training from score -3.236570\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-04-10 16:03:51,908]\u001b[0m Trial 24 finished with value: 0.04562968956202185 and parameters: {'num_leaves': 60, 'learning_rate': 0.02263393770868149, 'feature_fraction': 0.8551105530530294, 'bagging_fraction': 0.7739865095133951, 'bagging_freq': 2, 'min_child_samples': 17}. Best is trial 16 with value: 0.041213579114061.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 363, number of negative: 9237\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012017 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2048\n",
      "[LightGBM] [Info] Number of data points in the train set: 9600, number of used features: 1024\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037812 -> initscore=-3.236570\n",
      "[LightGBM] [Info] Start training from score -3.236570\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-04-10 16:03:52,444]\u001b[0m Trial 25 finished with value: 0.07191136535195404 and parameters: {'num_leaves': 82, 'learning_rate': 0.008956352397669322, 'feature_fraction': 0.931653987259896, 'bagging_fraction': 0.6810126680576883, 'bagging_freq': 3, 'min_child_samples': 34}. Best is trial 16 with value: 0.041213579114061.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 363, number of negative: 9237\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011563 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2048\n",
      "[LightGBM] [Info] Number of data points in the train set: 9600, number of used features: 1024\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037812 -> initscore=-3.236570\n",
      "[LightGBM] [Info] Start training from score -3.236570\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-04-10 16:03:53,000]\u001b[0m Trial 26 finished with value: 0.04481485661216057 and parameters: {'num_leaves': 93, 'learning_rate': 0.024205530882975838, 'feature_fraction': 0.9433331045510012, 'bagging_fraction': 0.7725695835559533, 'bagging_freq': 5, 'min_child_samples': 20}. Best is trial 16 with value: 0.041213579114061.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 363, number of negative: 9237\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012470 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2048\n",
      "[LightGBM] [Info] Number of data points in the train set: 9600, number of used features: 1024\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037812 -> initscore=-3.236570\n",
      "[LightGBM] [Info] Start training from score -3.236570\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-04-10 16:03:53,418]\u001b[0m Trial 27 finished with value: 0.08188243453732998 and parameters: {'num_leaves': 45, 'learning_rate': 0.007131250218173555, 'feature_fraction': 0.8071363331784382, 'bagging_fraction': 0.6024754399719239, 'bagging_freq': 2, 'min_child_samples': 40}. Best is trial 16 with value: 0.041213579114061.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 363, number of negative: 9237\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011923 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2048\n",
      "[LightGBM] [Info] Number of data points in the train set: 9600, number of used features: 1024\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037812 -> initscore=-3.236570\n",
      "[LightGBM] [Info] Start training from score -3.236570\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-04-10 16:03:53,835]\u001b[0m Trial 28 finished with value: 0.04295219059594939 and parameters: {'num_leaves': 77, 'learning_rate': 0.04991265298353865, 'feature_fraction': 0.8743664662114481, 'bagging_fraction': 0.6966416711378127, 'bagging_freq': 5, 'min_child_samples': 25}. Best is trial 16 with value: 0.041213579114061.\u001b[0m\n",
      "\u001b[32m[I 2023-04-10 16:03:54,075]\u001b[0m Trial 29 finished with value: 0.042806362956725755 and parameters: {'num_leaves': 16, 'learning_rate': 0.09291826450333436, 'feature_fraction': 0.7794589077521958, 'bagging_fraction': 0.6420463862588077, 'bagging_freq': 4, 'min_child_samples': 47}. Best is trial 16 with value: 0.041213579114061.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 363, number of negative: 9237\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011992 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2048\n",
      "[LightGBM] [Info] Number of data points in the train set: 9600, number of used features: 1024\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037812 -> initscore=-3.236570\n",
      "[LightGBM] [Info] Start training from score -3.236570\n",
      "[LightGBM] [Info] Number of positive: 363, number of negative: 9237\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011740 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2048\n",
      "[LightGBM] [Info] Number of data points in the train set: 9600, number of used features: 1024\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037812 -> initscore=-3.236570\n",
      "[LightGBM] [Info] Start training from score -3.236570\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-04-10 16:03:54,559]\u001b[0m Trial 30 finished with value: 0.051491344011527745 and parameters: {'num_leaves': 71, 'learning_rate': 0.022982013169381456, 'feature_fraction': 0.902371897825401, 'bagging_fraction': 0.7184575892694547, 'bagging_freq': 1, 'min_child_samples': 72}. Best is trial 16 with value: 0.041213579114061.\u001b[0m\n",
      "\u001b[32m[I 2023-04-10 16:03:54,874]\u001b[0m Trial 31 finished with value: 0.04288777499386921 and parameters: {'num_leaves': 65, 'learning_rate': 0.08916677111024225, 'feature_fraction': 0.6303873351588948, 'bagging_fraction': 0.7952270491217175, 'bagging_freq': 1, 'min_child_samples': 36}. Best is trial 16 with value: 0.041213579114061.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 363, number of negative: 9237\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012557 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2048\n",
      "[LightGBM] [Info] Number of data points in the train set: 9600, number of used features: 1024\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037812 -> initscore=-3.236570\n",
      "[LightGBM] [Info] Start training from score -3.236570\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 363, number of negative: 9237\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011934 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2048\n",
      "[LightGBM] [Info] Number of data points in the train set: 9600, number of used features: 1024\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037812 -> initscore=-3.236570\n",
      "[LightGBM] [Info] Start training from score -3.236570\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-04-10 16:03:55,373]\u001b[0m Trial 32 finished with value: 0.042977045661251524 and parameters: {'num_leaves': 82, 'learning_rate': 0.04694461453435874, 'feature_fraction': 0.8452511299013505, 'bagging_fraction': 0.8562942849463608, 'bagging_freq': 1, 'min_child_samples': 38}. Best is trial 16 with value: 0.041213579114061.\u001b[0m\n",
      "\u001b[32m[I 2023-04-10 16:03:55,683]\u001b[0m Trial 33 finished with value: 0.043132059868418386 and parameters: {'num_leaves': 54, 'learning_rate': 0.0964235022272825, 'feature_fraction': 0.9647251353388534, 'bagging_fraction': 0.7456284410613755, 'bagging_freq': 2, 'min_child_samples': 28}. Best is trial 16 with value: 0.041213579114061.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 363, number of negative: 9237\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012157 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2048\n",
      "[LightGBM] [Info] Number of data points in the train set: 9600, number of used features: 1024\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037812 -> initscore=-3.236570\n",
      "[LightGBM] [Info] Start training from score -3.236570\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 363, number of negative: 9237\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012024 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2048\n",
      "[LightGBM] [Info] Number of data points in the train set: 9600, number of used features: 1024\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037812 -> initscore=-3.236570\n",
      "[LightGBM] [Info] Start training from score -3.236570\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-04-10 16:03:56,163]\u001b[0m Trial 34 finished with value: 0.04366499091851932 and parameters: {'num_leaves': 63, 'learning_rate': 0.03517848984096878, 'feature_fraction': 0.6227690695932991, 'bagging_fraction': 0.6981777109832703, 'bagging_freq': 1, 'min_child_samples': 48}. Best is trial 16 with value: 0.041213579114061.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 363, number of negative: 9237\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012208 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2048\n",
      "[LightGBM] [Info] Number of data points in the train set: 9600, number of used features: 1024\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037812 -> initscore=-3.236570\n",
      "[LightGBM] [Info] Start training from score -3.236570\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-04-10 16:03:56,534]\u001b[0m Trial 35 finished with value: 0.04367218882280537 and parameters: {'num_leaves': 71, 'learning_rate': 0.06499181058691289, 'feature_fraction': 0.7714418486903029, 'bagging_fraction': 0.7377211447471573, 'bagging_freq': 3, 'min_child_samples': 11}. Best is trial 16 with value: 0.041213579114061.\u001b[0m\n",
      "\u001b[32m[I 2023-04-10 16:03:56,813]\u001b[0m Trial 36 finished with value: 0.04481318496021476 and parameters: {'num_leaves': 34, 'learning_rate': 0.11647665584269473, 'feature_fraction': 0.7152785779943672, 'bagging_fraction': 0.8019615173065532, 'bagging_freq': 2, 'min_child_samples': 52}. Best is trial 16 with value: 0.041213579114061.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 363, number of negative: 9237\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012149 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2048\n",
      "[LightGBM] [Info] Number of data points in the train set: 9600, number of used features: 1024\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037812 -> initscore=-3.236570\n",
      "[LightGBM] [Info] Start training from score -3.236570\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 363, number of negative: 9237\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012123 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2048\n",
      "[LightGBM] [Info] Number of data points in the train set: 9600, number of used features: 1024\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037812 -> initscore=-3.236570\n",
      "[LightGBM] [Info] Start training from score -3.236570\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-04-10 16:03:57,285]\u001b[0m Trial 37 finished with value: 0.04411241208786553 and parameters: {'num_leaves': 93, 'learning_rate': 0.05277718646245115, 'feature_fraction': 0.8593812691329593, 'bagging_fraction': 0.8781374780020267, 'bagging_freq': 7, 'min_child_samples': 23}. Best is trial 16 with value: 0.041213579114061.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 363, number of negative: 9237\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012869 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2048\n",
      "[LightGBM] [Info] Number of data points in the train set: 9600, number of used features: 1024\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037812 -> initscore=-3.236570\n",
      "[LightGBM] [Info] Start training from score -3.236570\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-04-10 16:03:57,773]\u001b[0m Trial 38 finished with value: 0.05600258036850766 and parameters: {'num_leaves': 78, 'learning_rate': 0.013284631248551731, 'feature_fraction': 0.9227715885664639, 'bagging_fraction': 0.7267513815719528, 'bagging_freq': 1, 'min_child_samples': 16}. Best is trial 16 with value: 0.041213579114061.\u001b[0m\n",
      "\u001b[32m[I 2023-04-10 16:03:58,081]\u001b[0m Trial 39 finished with value: 0.043953038986850546 and parameters: {'num_leaves': 67, 'learning_rate': 0.12916819476137464, 'feature_fraction': 0.9754638817526016, 'bagging_fraction': 0.6630422320695671, 'bagging_freq': 2, 'min_child_samples': 35}. Best is trial 16 with value: 0.041213579114061.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 363, number of negative: 9237\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012135 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2048\n",
      "[LightGBM] [Info] Number of data points in the train set: 9600, number of used features: 1024\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037812 -> initscore=-3.236570\n",
      "[LightGBM] [Info] Start training from score -3.236570\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 363, number of negative: 9237\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012000 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2048\n",
      "[LightGBM] [Info] Number of data points in the train set: 9600, number of used features: 1024\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037812 -> initscore=-3.236570\n",
      "[LightGBM] [Info] Start training from score -3.236570\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-04-10 16:03:58,458]\u001b[0m Trial 40 finished with value: 0.04190628665777745 and parameters: {'num_leaves': 58, 'learning_rate': 0.07102720453864919, 'feature_fraction': 0.7549448974365621, 'bagging_fraction': 0.7512034341021296, 'bagging_freq': 3, 'min_child_samples': 45}. Best is trial 16 with value: 0.041213579114061.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 363, number of negative: 9237\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011962 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2048\n",
      "[LightGBM] [Info] Number of data points in the train set: 9600, number of used features: 1024\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037812 -> initscore=-3.236570\n",
      "[LightGBM] [Info] Start training from score -3.236570\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-04-10 16:03:58,898]\u001b[0m Trial 41 finished with value: 0.041852950840467175 and parameters: {'num_leaves': 59, 'learning_rate': 0.055380632145357864, 'feature_fraction': 0.7625897681469107, 'bagging_fraction': 0.7490533948036616, 'bagging_freq': 3, 'min_child_samples': 45}. Best is trial 16 with value: 0.041213579114061.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 363, number of negative: 9237\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011928 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2048\n",
      "[LightGBM] [Info] Number of data points in the train set: 9600, number of used features: 1024\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037812 -> initscore=-3.236570\n",
      "[LightGBM] [Info] Start training from score -3.236570\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-04-10 16:03:59,305]\u001b[0m Trial 42 finished with value: 0.04445800230116795 and parameters: {'num_leaves': 40, 'learning_rate': 0.03217636727658807, 'feature_fraction': 0.7547714218930719, 'bagging_fraction': 0.7617750757577768, 'bagging_freq': 3, 'min_child_samples': 58}. Best is trial 16 with value: 0.041213579114061.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 363, number of negative: 9237\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011709 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2048\n",
      "[LightGBM] [Info] Number of data points in the train set: 9600, number of used features: 1024\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037812 -> initscore=-3.236570\n",
      "[LightGBM] [Info] Start training from score -3.236570\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-04-10 16:03:59,762]\u001b[0m Trial 43 finished with value: 0.05136871776525247 and parameters: {'num_leaves': 58, 'learning_rate': 0.020072868616861892, 'feature_fraction': 0.7991416958699786, 'bagging_fraction': 0.7170731897663603, 'bagging_freq': 4, 'min_child_samples': 45}. Best is trial 16 with value: 0.041213579114061.\u001b[0m\n",
      "\u001b[32m[I 2023-04-10 16:04:00,070]\u001b[0m Trial 44 finished with value: 0.04758875624563104 and parameters: {'num_leaves': 51, 'learning_rate': 0.14281343647075273, 'feature_fraction': 0.7540971907537349, 'bagging_fraction': 0.7600512477908132, 'bagging_freq': 3, 'min_child_samples': 54}. Best is trial 16 with value: 0.041213579114061.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 363, number of negative: 9237\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011947 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2048\n",
      "[LightGBM] [Info] Number of data points in the train set: 9600, number of used features: 1024\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037812 -> initscore=-3.236570\n",
      "[LightGBM] [Info] Start training from score -3.236570\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 363, number of negative: 9237\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012021 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2048\n",
      "[LightGBM] [Info] Number of data points in the train set: 9600, number of used features: 1024\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037812 -> initscore=-3.236570\n",
      "[LightGBM] [Info] Start training from score -3.236570\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-04-10 16:04:00,457]\u001b[0m Trial 45 finished with value: 0.04106002329049448 and parameters: {'num_leaves': 47, 'learning_rate': 0.04920031160221542, 'feature_fraction': 0.8277746159959851, 'bagging_fraction': 0.6846852351069285, 'bagging_freq': 3, 'min_child_samples': 31}. Best is trial 45 with value: 0.04106002329049448.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 363, number of negative: 9237\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012989 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2048\n",
      "[LightGBM] [Info] Number of data points in the train set: 9600, number of used features: 1024\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037812 -> initscore=-3.236570\n",
      "[LightGBM] [Info] Start training from score -3.236570\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-04-10 16:04:00,801]\u001b[0m Trial 46 finished with value: 0.042280385258540505 and parameters: {'num_leaves': 47, 'learning_rate': 0.05744446437430618, 'feature_fraction': 0.8305107601883294, 'bagging_fraction': 0.6995089933573692, 'bagging_freq': 4, 'min_child_samples': 30}. Best is trial 45 with value: 0.04106002329049448.\u001b[0m\n",
      "\u001b[32m[I 2023-04-10 16:04:01,031]\u001b[0m Trial 47 finished with value: 0.04463498231299974 and parameters: {'num_leaves': 26, 'learning_rate': 0.1991629982394159, 'feature_fraction': 0.7334774420943122, 'bagging_fraction': 0.7431859934616132, 'bagging_freq': 3, 'min_child_samples': 68}. Best is trial 45 with value: 0.04106002329049448.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 363, number of negative: 9237\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011767 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2048\n",
      "[LightGBM] [Info] Number of data points in the train set: 9600, number of used features: 1024\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037812 -> initscore=-3.236570\n",
      "[LightGBM] [Info] Start training from score -3.236570\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-04-10 16:04:01,313]\u001b[0m Trial 48 finished with value: 0.044197956887882674 and parameters: {'num_leaves': 39, 'learning_rate': 0.11293228367441789, 'feature_fraction': 0.7818961213652978, 'bagging_fraction': 0.7833934259498239, 'bagging_freq': 4, 'min_child_samples': 43}. Best is trial 45 with value: 0.04106002329049448.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 363, number of negative: 9237\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012953 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2048\n",
      "[LightGBM] [Info] Number of data points in the train set: 9600, number of used features: 1024\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037812 -> initscore=-3.236570\n",
      "[LightGBM] [Info] Start training from score -3.236570\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 363, number of negative: 9237\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011071 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2048\n",
      "[LightGBM] [Info] Number of data points in the train set: 9600, number of used features: 1024\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037812 -> initscore=-3.236570\n",
      "[LightGBM] [Info] Start training from score -3.236570\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-04-10 16:04:01,706]\u001b[0m Trial 49 finished with value: 0.053576460390093804 and parameters: {'num_leaves': 43, 'learning_rate': 0.016990225967076274, 'feature_fraction': 0.7011707668540861, 'bagging_fraction': 0.6788417455320028, 'bagging_freq': 3, 'min_child_samples': 31}. Best is trial 45 with value: 0.04106002329049448.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial:\n",
      "  Value: 0.04106002329049448\n",
      "  Params: \n",
      "    num_leaves: 47\n",
      "    learning_rate: 0.04920031160221542\n",
      "    feature_fraction: 0.8277746159959851\n",
      "    bagging_fraction: 0.6846852351069285\n",
      "    bagging_freq: 3\n",
      "    min_child_samples: 31\n",
      "[LightGBM] [Info] Number of positive: 363, number of negative: 9237\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011470 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2048\n",
      "[LightGBM] [Info] Number of data points in the train set: 9600, number of used features: 1024\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037812 -> initscore=-3.236570\n",
      "[LightGBM] [Info] Start training from score -3.236570\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1]\ttraining's binary_logloss: 0.136293\tvalid_1's binary_logloss: 0.145274\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[2]\ttraining's binary_logloss: 0.124141\tvalid_1's binary_logloss: 0.132846\n",
      "[3]\ttraining's binary_logloss: 0.11577\tvalid_1's binary_logloss: 0.124135\n",
      "[4]\ttraining's binary_logloss: 0.108781\tvalid_1's binary_logloss: 0.116917\n",
      "[5]\ttraining's binary_logloss: 0.102654\tvalid_1's binary_logloss: 0.111241\n",
      "[6]\ttraining's binary_logloss: 0.097684\tvalid_1's binary_logloss: 0.106691\n",
      "[7]\ttraining's binary_logloss: 0.0927275\tvalid_1's binary_logloss: 0.101586\n",
      "[8]\ttraining's binary_logloss: 0.0884002\tvalid_1's binary_logloss: 0.0970419\n",
      "[9]\ttraining's binary_logloss: 0.0846703\tvalid_1's binary_logloss: 0.0932444\n",
      "[10]\ttraining's binary_logloss: 0.0810568\tvalid_1's binary_logloss: 0.0898809\n",
      "[11]\ttraining's binary_logloss: 0.0778363\tvalid_1's binary_logloss: 0.0867042\n",
      "[12]\ttraining's binary_logloss: 0.0749853\tvalid_1's binary_logloss: 0.0838187\n",
      "[13]\ttraining's binary_logloss: 0.0723957\tvalid_1's binary_logloss: 0.0813204\n",
      "[14]\ttraining's binary_logloss: 0.0700115\tvalid_1's binary_logloss: 0.0789734\n",
      "[15]\ttraining's binary_logloss: 0.0677404\tvalid_1's binary_logloss: 0.0766177\n",
      "[16]\ttraining's binary_logloss: 0.0654893\tvalid_1's binary_logloss: 0.0746948\n",
      "[17]\ttraining's binary_logloss: 0.063341\tvalid_1's binary_logloss: 0.0727317\n",
      "[18]\ttraining's binary_logloss: 0.061399\tvalid_1's binary_logloss: 0.0712378\n",
      "[19]\ttraining's binary_logloss: 0.0596048\tvalid_1's binary_logloss: 0.0696079\n",
      "[20]\ttraining's binary_logloss: 0.0579316\tvalid_1's binary_logloss: 0.0681529\n",
      "[21]\ttraining's binary_logloss: 0.0562995\tvalid_1's binary_logloss: 0.0668051\n",
      "[22]\ttraining's binary_logloss: 0.0547421\tvalid_1's binary_logloss: 0.0654723\n",
      "[23]\ttraining's binary_logloss: 0.0531971\tvalid_1's binary_logloss: 0.0642468\n",
      "[24]\ttraining's binary_logloss: 0.0517525\tvalid_1's binary_logloss: 0.0630598\n",
      "[25]\ttraining's binary_logloss: 0.0502957\tvalid_1's binary_logloss: 0.0617743\n",
      "[26]\ttraining's binary_logloss: 0.0489852\tvalid_1's binary_logloss: 0.0606461\n",
      "[27]\ttraining's binary_logloss: 0.0478517\tvalid_1's binary_logloss: 0.0596672\n",
      "[28]\ttraining's binary_logloss: 0.0467317\tvalid_1's binary_logloss: 0.0586766\n",
      "[29]\ttraining's binary_logloss: 0.0456294\tvalid_1's binary_logloss: 0.0576299\n",
      "[30]\ttraining's binary_logloss: 0.0446286\tvalid_1's binary_logloss: 0.0567697\n",
      "[31]\ttraining's binary_logloss: 0.043419\tvalid_1's binary_logloss: 0.0558943\n",
      "[32]\ttraining's binary_logloss: 0.0424886\tvalid_1's binary_logloss: 0.0550824\n",
      "[33]\ttraining's binary_logloss: 0.041575\tvalid_1's binary_logloss: 0.0544555\n",
      "[34]\ttraining's binary_logloss: 0.0406031\tvalid_1's binary_logloss: 0.0535137\n",
      "[35]\ttraining's binary_logloss: 0.039617\tvalid_1's binary_logloss: 0.0527993\n",
      "[36]\ttraining's binary_logloss: 0.0387658\tvalid_1's binary_logloss: 0.0521098\n",
      "[37]\ttraining's binary_logloss: 0.0377966\tvalid_1's binary_logloss: 0.0514296\n",
      "[38]\ttraining's binary_logloss: 0.0369235\tvalid_1's binary_logloss: 0.0506847\n",
      "[39]\ttraining's binary_logloss: 0.0361468\tvalid_1's binary_logloss: 0.0501706\n",
      "[40]\ttraining's binary_logloss: 0.0352585\tvalid_1's binary_logloss: 0.0498894\n",
      "[41]\ttraining's binary_logloss: 0.0344396\tvalid_1's binary_logloss: 0.0495206\n",
      "[42]\ttraining's binary_logloss: 0.0337001\tvalid_1's binary_logloss: 0.0492601\n",
      "[43]\ttraining's binary_logloss: 0.032978\tvalid_1's binary_logloss: 0.0487351\n",
      "[44]\ttraining's binary_logloss: 0.0323344\tvalid_1's binary_logloss: 0.0481661\n",
      "[45]\ttraining's binary_logloss: 0.0316409\tvalid_1's binary_logloss: 0.047789\n",
      "[46]\ttraining's binary_logloss: 0.0310646\tvalid_1's binary_logloss: 0.0473696\n",
      "[47]\ttraining's binary_logloss: 0.0305129\tvalid_1's binary_logloss: 0.0468759\n",
      "[48]\ttraining's binary_logloss: 0.0299087\tvalid_1's binary_logloss: 0.0466579\n",
      "[49]\ttraining's binary_logloss: 0.0292659\tvalid_1's binary_logloss: 0.0462897\n",
      "[50]\ttraining's binary_logloss: 0.0286813\tvalid_1's binary_logloss: 0.0461634\n",
      "[51]\ttraining's binary_logloss: 0.0280494\tvalid_1's binary_logloss: 0.0458914\n",
      "[52]\ttraining's binary_logloss: 0.0274464\tvalid_1's binary_logloss: 0.0455436\n",
      "[53]\ttraining's binary_logloss: 0.0268671\tvalid_1's binary_logloss: 0.0452369\n",
      "[54]\ttraining's binary_logloss: 0.0263844\tvalid_1's binary_logloss: 0.045041\n",
      "[55]\ttraining's binary_logloss: 0.0257642\tvalid_1's binary_logloss: 0.0447464\n",
      "[56]\ttraining's binary_logloss: 0.0252065\tvalid_1's binary_logloss: 0.0443886\n",
      "[57]\ttraining's binary_logloss: 0.0246611\tvalid_1's binary_logloss: 0.0440349\n",
      "[58]\ttraining's binary_logloss: 0.0241146\tvalid_1's binary_logloss: 0.0437584\n",
      "[59]\ttraining's binary_logloss: 0.0237038\tvalid_1's binary_logloss: 0.0437491\n",
      "[60]\ttraining's binary_logloss: 0.0233022\tvalid_1's binary_logloss: 0.0435444\n",
      "[61]\ttraining's binary_logloss: 0.0228453\tvalid_1's binary_logloss: 0.0434512\n",
      "[62]\ttraining's binary_logloss: 0.022372\tvalid_1's binary_logloss: 0.0433474\n",
      "[63]\ttraining's binary_logloss: 0.0219511\tvalid_1's binary_logloss: 0.0434006\n",
      "[64]\ttraining's binary_logloss: 0.0214681\tvalid_1's binary_logloss: 0.0431527\n",
      "[65]\ttraining's binary_logloss: 0.0210344\tvalid_1's binary_logloss: 0.0429066\n",
      "[66]\ttraining's binary_logloss: 0.0206203\tvalid_1's binary_logloss: 0.0427469\n",
      "[67]\ttraining's binary_logloss: 0.0201482\tvalid_1's binary_logloss: 0.0426114\n",
      "[68]\ttraining's binary_logloss: 0.0197157\tvalid_1's binary_logloss: 0.0423371\n",
      "[69]\ttraining's binary_logloss: 0.01929\tvalid_1's binary_logloss: 0.0420205\n",
      "[70]\ttraining's binary_logloss: 0.0188649\tvalid_1's binary_logloss: 0.041887\n",
      "[71]\ttraining's binary_logloss: 0.0184945\tvalid_1's binary_logloss: 0.0418457\n",
      "[72]\ttraining's binary_logloss: 0.0181288\tvalid_1's binary_logloss: 0.0415781\n",
      "[73]\ttraining's binary_logloss: 0.0177745\tvalid_1's binary_logloss: 0.0415052\n",
      "[74]\ttraining's binary_logloss: 0.0174831\tvalid_1's binary_logloss: 0.0415629\n",
      "[75]\ttraining's binary_logloss: 0.0171645\tvalid_1's binary_logloss: 0.0415803\n",
      "[76]\ttraining's binary_logloss: 0.0168511\tvalid_1's binary_logloss: 0.0414601\n",
      "[77]\ttraining's binary_logloss: 0.0165818\tvalid_1's binary_logloss: 0.0414114\n",
      "[78]\ttraining's binary_logloss: 0.0163024\tvalid_1's binary_logloss: 0.0413591\n",
      "[79]\ttraining's binary_logloss: 0.0159752\tvalid_1's binary_logloss: 0.0410822\n",
      "[80]\ttraining's binary_logloss: 0.0156756\tvalid_1's binary_logloss: 0.04106\n",
      "[81]\ttraining's binary_logloss: 0.0153919\tvalid_1's binary_logloss: 0.0412403\n",
      "[82]\ttraining's binary_logloss: 0.0150909\tvalid_1's binary_logloss: 0.0412495\n",
      "[83]\ttraining's binary_logloss: 0.0147868\tvalid_1's binary_logloss: 0.0411774\n",
      "[84]\ttraining's binary_logloss: 0.0145035\tvalid_1's binary_logloss: 0.0411966\n",
      "[85]\ttraining's binary_logloss: 0.0142466\tvalid_1's binary_logloss: 0.0412412\n",
      "[86]\ttraining's binary_logloss: 0.0139801\tvalid_1's binary_logloss: 0.0411252\n",
      "[87]\ttraining's binary_logloss: 0.0137271\tvalid_1's binary_logloss: 0.0412696\n",
      "[88]\ttraining's binary_logloss: 0.0134407\tvalid_1's binary_logloss: 0.0412186\n",
      "[89]\ttraining's binary_logloss: 0.0131629\tvalid_1's binary_logloss: 0.0411891\n",
      "[90]\ttraining's binary_logloss: 0.0129386\tvalid_1's binary_logloss: 0.0412394\n",
      "Early stopping, best iteration is:\n",
      "[80]\ttraining's binary_logloss: 0.0156756\tvalid_1's binary_logloss: 0.04106\n"
     ]
    }
   ],
   "source": [
    "def objective(trial):\n",
    "    # Optimize the hyperparameters using Optuna\n",
    "    params = {\n",
    "        'objective': 'binary',\n",
    "        'metric': 'binary_logloss',\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 10, 100),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 1e-4, 0.2, log=True),\n",
    "        'feature_fraction': trial.suggest_float('feature_fraction', 0.4, 1.0),\n",
    "        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.4, 1.0),\n",
    "        'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n",
    "    }\n",
    "\n",
    "    lgb_train = lgb.Dataset(train_fps, label=y_train[:, 0])\n",
    "    lgb_val = lgb.Dataset(val_fps, label=y_val[:, 0])\n",
    "\n",
    "    num_round = 100\n",
    "    lgb_model = lgb.train(params, lgb_train, num_round, valid_sets=[lgb_val],\n",
    "                          early_stopping_rounds=10, verbose_eval=False)\n",
    "\n",
    "    # Return the binary_logloss from validation set as the objective to minimize\n",
    "    return lgb_model.best_score[\"valid_0\"][\"binary_logloss\"]\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(objective, n_trials=50)\n",
    "\n",
    "    print(\"Best trial:\")\n",
    "    trial = study.best_trial\n",
    "    print(\"  Value: {}\".format(trial.value))\n",
    "    print(\"  Params: \")\n",
    "    for key, value in trial.params.items():\n",
    "        print(\"    {}: {}\".format(key, value))\n",
    "\n",
    "    # Train the model with the best hyperparameters\n",
    "    best_params = trial.params\n",
    "    best_params.update({\"objective\": \"binary\", \"metric\": \"binary_logloss\"})\n",
    "    lgb_train = lgb.Dataset(train_fps, label=y_train[:, 0])\n",
    "    lgb_val = lgb.Dataset(val_fps, label=y_val[:, 0])\n",
    "    num_round = 100\n",
    "\n",
    "    lgb_model = lgb.train(best_params, lgb_train, num_round, valid_sets=[lgb_train, lgb_val],\n",
    "                          early_stopping_rounds=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # Optimize the hyperparameters using Optuna\n",
    "    params = {\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 10, 100),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 1e-4, 0.2, log=True),\n",
    "        'feature_fraction': trial.suggest_float('feature_fraction', 0.4, 1.0),\n",
    "        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.4, 1.0),\n",
    "        'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n",
    "    }\n",
    "    num_round = 100\n",
    "\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    y_train_binarized = mlb.fit_transform(y_train)\n",
    "    y_val_binarized = mlb.transform(y_val)\n",
    "\n",
    "    logloss_list = []\n",
    "\n",
    "    for i in range(y_train_binarized.shape[1]):\n",
    "        lgb_model = LGBMClassifier(n_estimators=num_round, **params)\n",
    "        calibrated_lgb_model = CalibratedClassifierCV(\n",
    "            lgb_model, method=\"sigmoid\", cv=3)\n",
    "        calibrated_lgb_model.fit(train_fps, y_train_binarized[:, i])\n",
    "\n",
    "        # Return the binary_logloss from validation set as the objective to minimize\n",
    "        val_preds = calibrated_lgb_model.predict_proba(val_fps)\n",
    "        logloss = log_loss(y_val_binarized[:, i], val_preds)\n",
    "        logloss_list.append(logloss)\n",
    "\n",
    "    return np.mean(logloss_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>smiles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>OC(COc1ccc(Cl)cc1)=N[C@H]1CC[C@H](N=C(O)COc2cc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CCCO/N=C(/C)c1cc(C(O)=NC(Cc2cc(F)cc(F)c2)[C@@H...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>COc1cc(Cl)ccc1Cl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>COc1cc(C(O)=NCc2ccc(OCCN(C)C)cc2)cc(OC)c1OC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CCC(=O)O[C@@]1(C(=O)CCl)[C@@H](C)C[C@H]2[C@@H]...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5891</th>\n",
       "      <td>N#Cc1cc(NC(=O)C(=O)O)c(Cl)c(NC(=O)C(=O)O)c1.NC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5892</th>\n",
       "      <td>O=c1cccc2n1C[C@@H]1CNC[C@H]2C1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5893</th>\n",
       "      <td>CSCC[C@H](N=C(O)[C@H](Cc1ccccc1)N=C(O)CN=C(O)C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5894</th>\n",
       "      <td>CCn1cc2c3c(cc(C(O)=NC(Cc4ccccc4)[C@H](O)C[NH2+...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5895</th>\n",
       "      <td>C[C@H]1CCCN(S(C)(=O)=O)[C@@H]1CO</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5896 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 smiles\n",
       "0     OC(COc1ccc(Cl)cc1)=N[C@H]1CC[C@H](N=C(O)COc2cc...\n",
       "1     CCCO/N=C(/C)c1cc(C(O)=NC(Cc2cc(F)cc(F)c2)[C@@H...\n",
       "2                                      COc1cc(Cl)ccc1Cl\n",
       "3           COc1cc(C(O)=NCc2ccc(OCCN(C)C)cc2)cc(OC)c1OC\n",
       "4     CCC(=O)O[C@@]1(C(=O)CCl)[C@@H](C)C[C@H]2[C@@H]...\n",
       "...                                                 ...\n",
       "5891  N#Cc1cc(NC(=O)C(=O)O)c(Cl)c(NC(=O)C(=O)O)c1.NC...\n",
       "5892                     O=c1cccc2n1C[C@@H]1CNC[C@H]2C1\n",
       "5893  CSCC[C@H](N=C(O)[C@H](Cc1ccccc1)N=C(O)CN=C(O)C...\n",
       "5894  CCn1cc2c3c(cc(C(O)=NC(Cc4ccccc4)[C@H](O)C[NH2+...\n",
       "5895                   C[C@H]1CCCN(S(C)(=O)=O)[C@@H]1CO\n",
       "\n",
       "[5896 rows x 1 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = pd.read_csv(\"smiles_test.csv\", index_col=0).reset_index(drop=True)\n",
    "test_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp_length = 1024\n",
    "\n",
    "test_fps = np.zeros((len(test_data), fp_length))\n",
    "for i, smiles in enumerate(test_data['smiles']):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    fp_vec = AllChem.GetMorganFingerprintAsBitVect(\n",
    "        mol, radius=3, nBits=fp_length)\n",
    "    arr = np.zeros((1,))\n",
    "    Chem.DataStructs.ConvertToNumpyArray(fp_vec, arr)\n",
    "    test_fps[i] = arr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_probs = lgb_model.predict(test_fps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_tasks = y_train.shape[1]\n",
    "test_probs = np.empty((test_fps.shape[0], n_tasks))\n",
    "\n",
    "for j in range(n_tasks):\n",
    "    lgb_test = lgb.Dataset(test_fps)\n",
    "    test_probs[:, j] = lgb_model.predict(test_fps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = pd.DataFrame(test_probs, columns=[\n",
    "                     \"task1\", \"task2\", \"task3\", \"task4\", \"task5\", \"task6\", \"task7\", \"task8\", \"task9\", \"task10\", \"task11\"])\n",
    "probs.index = test_data.index\n",
    "\n",
    "probs.to_csv(\"test_predictions_4.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look at the predicted number of samples for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0.0: 26028, 1.0: 372}"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique, counts = np.unique(y_hats_class, return_counts=True)\n",
    "dict(zip(unique, counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:rgb(0,120,170)\">Metrics</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To determine the quality of the model we look at several metrics. When calculating metrics we need to remove predictions for missing values as there's no way to measure the quality of these predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:rgb(0,120,170)\">Confusion Matrix, Precision, Recall, F1-score</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at these metrics (or methods) for the first task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = 0\n",
    "# Mask out unknown samples\n",
    "idx = (y_test[:, task] != (-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2188,    1],\n",
       "       [  13,   86]], dtype=int64)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm = confusion_matrix(y_test[idx,task], y_hats_class[idx,task])\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2188,    1,   13,   86], dtype=int64)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# True Negatives, False Positives, False Negatives, True Positives\n",
    "cm.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision, Recall and F1-Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The **precision** is the ratio $\\frac{TP}{TP + FP}$ where TP is the number of true positives and FP the number of false positives. The precision is intuitively the ability of the classifier to not label negative samples as positive.\n",
    "\n",
    "- The **recall** is the ratio $\\frac{TP}{TP + FN}$ where TP is the number of true positives and FN the number of false negatives. The recall is intuitively the ability of the classifier to find all the positive samples.\n",
    "\n",
    "- The **F1-score** can be interpreted as a weighted harmonic mean of the precision and recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.99      1.00      1.00      2189\n",
      "     class 1       0.99      0.87      0.92        99\n",
      "\n",
      "    accuracy                           0.99      2288\n",
      "   macro avg       0.99      0.93      0.96      2288\n",
      "weighted avg       0.99      0.99      0.99      2288\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test[idx,task],y_hats_class[idx,task], target_names=[\"class 0\", \"class 1\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:rgb(0,120,170)\">Area under the ROC curve (AUC)</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we calculate the AUC for each task and the mean over all tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_masked_AUC_per_task(prediction, target):\n",
    "    auc_per_task = []\n",
    "    for j in range(target.shape[1]):\n",
    "        y_score = prediction[:, j]\n",
    "        y_true = target[:, j]\n",
    "        # Mask out unknown samples\n",
    "        idx = (y_true != (-1))\n",
    "        # Calculate AUC per task\n",
    "        auc_per_task.append(roc_auc_score(y_true[idx], y_score[idx]))\n",
    "    return auc_per_task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9998223440434495,\n",
       " 0.5305144112321529,\n",
       " 0.6112104995296186,\n",
       " 0.955122591943958,\n",
       " 0.5633698958429347,\n",
       " 0.5469617140850018,\n",
       " 0.6722359040829553,\n",
       " 0.6896354484441732,\n",
       " 0.7863222707526398,\n",
       " 0.7882040752210403,\n",
       " 0.650561797752809]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate AUC per task\n",
    "auc_per_task = calc_masked_AUC_per_task(y_hats_proba, y_test)\n",
    "auc_per_task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7085419048118848"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(auc_per_task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing predictions locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>task1</th>\n",
       "      <th>task2</th>\n",
       "      <th>task3</th>\n",
       "      <th>task4</th>\n",
       "      <th>task5</th>\n",
       "      <th>task6</th>\n",
       "      <th>task7</th>\n",
       "      <th>task8</th>\n",
       "      <th>task9</th>\n",
       "      <th>task10</th>\n",
       "      <th>task11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     task1  task2  task3  task4  task5  task6  task7  task8  task9  task10  \\\n",
       "0     -1.0   -1.0   -1.0   -1.0   -1.0   -1.0   -1.0    0.0   -1.0    -1.0   \n",
       "1     -1.0   -1.0   -1.0   -1.0   -1.0   -1.0   -1.0    0.0   -1.0    -1.0   \n",
       "2     -1.0   -1.0   -1.0   -1.0   -1.0   -1.0   -1.0   -1.0   -1.0     1.0   \n",
       "3     -1.0   -1.0   -1.0   -1.0   -1.0   -1.0   -1.0    0.0   -1.0    -1.0   \n",
       "4     -1.0   -1.0   -1.0   -1.0   -1.0   -1.0   -1.0   -1.0   -1.0     1.0   \n",
       "..     ...    ...    ...    ...    ...    ...    ...    ...    ...     ...   \n",
       "995   -1.0   -1.0   -1.0   -1.0   -1.0   -1.0   -1.0    0.0   -1.0    -1.0   \n",
       "996   -1.0   -1.0   -1.0   -1.0   -1.0   -1.0    0.0   -1.0    0.0    -1.0   \n",
       "997   -1.0   -1.0    0.0   -1.0   -1.0   -1.0   -1.0   -1.0   -1.0    -1.0   \n",
       "998   -1.0   -1.0   -1.0   -1.0   -1.0    0.0    0.0   -1.0    0.0    -1.0   \n",
       "999   -1.0   -1.0    1.0   -1.0   -1.0   -1.0   -1.0   -1.0   -1.0    -1.0   \n",
       "\n",
       "     task11  \n",
       "0      -1.0  \n",
       "1      -1.0  \n",
       "2      -1.0  \n",
       "3       1.0  \n",
       "4      -1.0  \n",
       "..      ...  \n",
       "995    -1.0  \n",
       "996    -1.0  \n",
       "997    -1.0  \n",
       "998    -1.0  \n",
       "999    -1.0  \n",
       "\n",
       "[1000 rows x 11 columns]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = data.iloc[:1000,1:]\n",
    "target = (target +1)/2\n",
    "target[target==0.5] = -1\n",
    "target "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>task1</th>\n",
       "      <th>task2</th>\n",
       "      <th>task3</th>\n",
       "      <th>task4</th>\n",
       "      <th>task5</th>\n",
       "      <th>task6</th>\n",
       "      <th>task7</th>\n",
       "      <th>task8</th>\n",
       "      <th>task9</th>\n",
       "      <th>task10</th>\n",
       "      <th>task11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11995</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11996</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11997</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11998</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11999</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12000 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       task1  task2  task3  task4  task5  task6  task7  task8  task9  task10  \\\n",
       "0       -1.0   -1.0   -1.0   -1.0   -1.0   -1.0   -1.0    0.0   -1.0    -1.0   \n",
       "1       -1.0   -1.0   -1.0   -1.0   -1.0   -1.0   -1.0    0.0   -1.0    -1.0   \n",
       "2       -1.0   -1.0   -1.0   -1.0   -1.0   -1.0   -1.0   -1.0   -1.0     1.0   \n",
       "3       -1.0   -1.0   -1.0   -1.0   -1.0   -1.0   -1.0    0.0   -1.0    -1.0   \n",
       "4       -1.0   -1.0   -1.0   -1.0   -1.0   -1.0   -1.0   -1.0   -1.0     1.0   \n",
       "...      ...    ...    ...    ...    ...    ...    ...    ...    ...     ...   \n",
       "11995   -1.0   -1.0   -1.0   -1.0   -1.0   -1.0   -1.0    0.0   -1.0    -1.0   \n",
       "11996   -1.0   -1.0   -1.0   -1.0   -1.0   -1.0   -1.0    0.0   -1.0    -1.0   \n",
       "11997   -1.0   -1.0   -1.0    0.0   -1.0   -1.0   -1.0   -1.0   -1.0    -1.0   \n",
       "11998   -1.0   -1.0   -1.0   -1.0   -1.0   -1.0    0.0   -1.0   -1.0    -1.0   \n",
       "11999   -1.0   -1.0   -1.0   -1.0   -1.0   -1.0    0.0   -1.0    1.0    -1.0   \n",
       "\n",
       "       task11  \n",
       "0        -1.0  \n",
       "1        -1.0  \n",
       "2        -1.0  \n",
       "3         1.0  \n",
       "4        -1.0  \n",
       "...       ...  \n",
       "11995    -1.0  \n",
       "11996    -1.0  \n",
       "11997    -1.0  \n",
       "11998    -1.0  \n",
       "11999    -1.0  \n",
       "\n",
       "[12000 rows x 11 columns]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = data.iloc[:, 1:]\n",
    "target = (target + 1)/2\n",
    "target[target == 0.5] = -1\n",
    "target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 12000 entries, 0 to 11999\n",
      "Data columns (total 11 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   task1   12000 non-null  float64\n",
      " 1   task2   12000 non-null  float64\n",
      " 2   task3   12000 non-null  float64\n",
      " 3   task4   12000 non-null  float64\n",
      " 4   task5   12000 non-null  float64\n",
      " 5   task6   12000 non-null  float64\n",
      " 6   task7   12000 non-null  float64\n",
      " 7   task8   12000 non-null  float64\n",
      " 8   task9   12000 non-null  float64\n",
      " 9   task10  12000 non-null  float64\n",
      " 10  task11  12000 non-null  float64\n",
      "dtypes: float64(11)\n",
      "memory usage: 1.0 MB\n"
     ]
    }
   ],
   "source": [
    "target.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>task1</th>\n",
       "      <th>task2</th>\n",
       "      <th>task3</th>\n",
       "      <th>task4</th>\n",
       "      <th>task5</th>\n",
       "      <th>task6</th>\n",
       "      <th>task7</th>\n",
       "      <th>task8</th>\n",
       "      <th>task9</th>\n",
       "      <th>task10</th>\n",
       "      <th>task11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.012500</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.011429</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.130000</td>\n",
       "      <td>0.060000</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02</td>\n",
       "      <td>0.036667</td>\n",
       "      <td>0.062619</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.275000</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.081333</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.59</td>\n",
       "      <td>0.021667</td>\n",
       "      <td>0.015000</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.015000</td>\n",
       "      <td>0.090000</td>\n",
       "      <td>0.014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.071667</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006667</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.685000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2395</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.220000</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2396</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.090000</td>\n",
       "      <td>0.130000</td>\n",
       "      <td>0.020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2397</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.030000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2398</th>\n",
       "      <td>0.88</td>\n",
       "      <td>0.017500</td>\n",
       "      <td>0.042667</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2399</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.030000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2400 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      task1     task2     task3  task4  task5     task6     task7  task8  \\\n",
       "0      0.00  0.010000  0.012500   0.00   0.00  0.011429  0.010000  0.015   \n",
       "1      0.02  0.036667  0.062619   0.00   0.00  0.275000  0.025000  0.010   \n",
       "2      0.59  0.021667  0.015000   0.02   0.01  0.000000  0.000000  0.000   \n",
       "3      0.00  0.000000  0.010000   0.00   0.00  0.000000  0.000000  0.000   \n",
       "4      0.00  0.000000  0.006667   0.00   0.00  0.000000  0.010000  0.000   \n",
       "...     ...       ...       ...    ...    ...       ...       ...    ...   \n",
       "2395   0.00  0.000000  0.020000   0.00   0.00  0.000000  0.200000  0.000   \n",
       "2396   0.01  0.041667  0.050000   0.02   0.00  0.000000  0.033333  0.000   \n",
       "2397   0.00  0.010000  0.020000   0.00   0.00  0.000000  0.030000  0.000   \n",
       "2398   0.88  0.017500  0.042667   0.00   0.01  0.000000  0.020000  0.010   \n",
       "2399   0.00  0.000000  0.000000   0.00   0.00  0.000000  0.000000  0.000   \n",
       "\n",
       "         task9    task10  task11  \n",
       "0     0.130000  0.060000   0.010  \n",
       "1     0.020000  0.081333   0.005  \n",
       "2     0.015000  0.090000   0.014  \n",
       "3     0.071667  0.010000   0.000  \n",
       "4     0.010000  0.685000   0.000  \n",
       "...        ...       ...     ...  \n",
       "2395  0.220000  0.040000   0.000  \n",
       "2396  0.090000  0.130000   0.020  \n",
       "2397  0.010000  0.010000   0.000  \n",
       "2398  0.010000  0.040000   0.010  \n",
       "2399  0.030000  0.000000   0.000  \n",
       "\n",
       "[2400 rows x 11 columns]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = probs.iloc[:, :]\n",
    "submission\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2400 entries, 0 to 2399\n",
      "Data columns (total 11 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   task1   2400 non-null   float64\n",
      " 1   task2   2400 non-null   float64\n",
      " 2   task3   2400 non-null   float64\n",
      " 3   task4   2400 non-null   float64\n",
      " 4   task5   2400 non-null   float64\n",
      " 5   task6   2400 non-null   float64\n",
      " 6   task7   2400 non-null   float64\n",
      " 7   task8   2400 non-null   float64\n",
      " 8   task9   2400 non-null   float64\n",
      " 9   task10  2400 non-null   float64\n",
      " 10  task11  2400 non-null   float64\n",
      "dtypes: float64(11)\n",
      "memory usage: 206.4 KB\n"
     ]
    }
   ],
   "source": [
    "submission.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2400,)\n",
      "(12000,)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [1023, 212]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[133], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m     idx \u001b[39m=\u001b[39m (y_true \u001b[39m!=\u001b[39m (\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))\n\u001b[0;32m      9\u001b[0m         \u001b[39m# calculate AUC per task\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m     auc_per_task\u001b[39m.\u001b[39mappend(roc_auc_score(y_true[idx], y_score[idx]))\n\u001b[0;32m     11\u001b[0m avg_auc \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmean(auc_per_task)\n\u001b[0;32m     12\u001b[0m \u001b[39mprint\u001b[39m(avg_auc)\n",
      "File \u001b[1;32md:\\Miniconda\\envs\\ails\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:572\u001b[0m, in \u001b[0;36mroc_auc_score\u001b[1;34m(y_true, y_score, average, sample_weight, max_fpr, multi_class, labels)\u001b[0m\n\u001b[0;32m    570\u001b[0m     labels \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39munique(y_true)\n\u001b[0;32m    571\u001b[0m     y_true \u001b[39m=\u001b[39m label_binarize(y_true, classes\u001b[39m=\u001b[39mlabels)[:, \u001b[39m0\u001b[39m]\n\u001b[1;32m--> 572\u001b[0m     \u001b[39mreturn\u001b[39;00m _average_binary_score(\n\u001b[0;32m    573\u001b[0m         partial(_binary_roc_auc_score, max_fpr\u001b[39m=\u001b[39;49mmax_fpr),\n\u001b[0;32m    574\u001b[0m         y_true,\n\u001b[0;32m    575\u001b[0m         y_score,\n\u001b[0;32m    576\u001b[0m         average,\n\u001b[0;32m    577\u001b[0m         sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[0;32m    578\u001b[0m     )\n\u001b[0;32m    579\u001b[0m \u001b[39melse\u001b[39;00m:  \u001b[39m# multilabel-indicator\u001b[39;00m\n\u001b[0;32m    580\u001b[0m     \u001b[39mreturn\u001b[39;00m _average_binary_score(\n\u001b[0;32m    581\u001b[0m         partial(_binary_roc_auc_score, max_fpr\u001b[39m=\u001b[39mmax_fpr),\n\u001b[0;32m    582\u001b[0m         y_true,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    585\u001b[0m         sample_weight\u001b[39m=\u001b[39msample_weight,\n\u001b[0;32m    586\u001b[0m     )\n",
      "File \u001b[1;32md:\\Miniconda\\envs\\ails\\lib\\site-packages\\sklearn\\metrics\\_base.py:75\u001b[0m, in \u001b[0;36m_average_binary_score\u001b[1;34m(binary_metric, y_true, y_score, average, sample_weight)\u001b[0m\n\u001b[0;32m     72\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m format is not supported\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(y_type))\n\u001b[0;32m     74\u001b[0m \u001b[39mif\u001b[39;00m y_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbinary\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m---> 75\u001b[0m     \u001b[39mreturn\u001b[39;00m binary_metric(y_true, y_score, sample_weight\u001b[39m=\u001b[39;49msample_weight)\n\u001b[0;32m     77\u001b[0m check_consistent_length(y_true, y_score, sample_weight)\n\u001b[0;32m     78\u001b[0m y_true \u001b[39m=\u001b[39m check_array(y_true)\n",
      "File \u001b[1;32md:\\Miniconda\\envs\\ails\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:344\u001b[0m, in \u001b[0;36m_binary_roc_auc_score\u001b[1;34m(y_true, y_score, sample_weight, max_fpr)\u001b[0m\n\u001b[0;32m    338\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(np\u001b[39m.\u001b[39munique(y_true)) \u001b[39m!=\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[0;32m    339\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    340\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mOnly one class present in y_true. ROC AUC score \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    341\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mis not defined in that case.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    342\u001b[0m     )\n\u001b[1;32m--> 344\u001b[0m fpr, tpr, _ \u001b[39m=\u001b[39m roc_curve(y_true, y_score, sample_weight\u001b[39m=\u001b[39;49msample_weight)\n\u001b[0;32m    345\u001b[0m \u001b[39mif\u001b[39;00m max_fpr \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m max_fpr \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    346\u001b[0m     \u001b[39mreturn\u001b[39;00m auc(fpr, tpr)\n",
      "File \u001b[1;32md:\\Miniconda\\envs\\ails\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:992\u001b[0m, in \u001b[0;36mroc_curve\u001b[1;34m(y_true, y_score, pos_label, sample_weight, drop_intermediate)\u001b[0m\n\u001b[0;32m    904\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mroc_curve\u001b[39m(\n\u001b[0;32m    905\u001b[0m     y_true, y_score, \u001b[39m*\u001b[39m, pos_label\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, sample_weight\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, drop_intermediate\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    906\u001b[0m ):\n\u001b[0;32m    907\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Compute Receiver operating characteristic (ROC).\u001b[39;00m\n\u001b[0;32m    908\u001b[0m \n\u001b[0;32m    909\u001b[0m \u001b[39m    Note: this implementation is restricted to the binary classification task.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    990\u001b[0m \u001b[39m    array([1.8 , 0.8 , 0.4 , 0.35, 0.1 ])\u001b[39;00m\n\u001b[0;32m    991\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 992\u001b[0m     fps, tps, thresholds \u001b[39m=\u001b[39m _binary_clf_curve(\n\u001b[0;32m    993\u001b[0m         y_true, y_score, pos_label\u001b[39m=\u001b[39;49mpos_label, sample_weight\u001b[39m=\u001b[39;49msample_weight\n\u001b[0;32m    994\u001b[0m     )\n\u001b[0;32m    996\u001b[0m     \u001b[39m# Attempt to drop thresholds corresponding to points in between and\u001b[39;00m\n\u001b[0;32m    997\u001b[0m     \u001b[39m# collinear with other points. These are always suboptimal and do not\u001b[39;00m\n\u001b[0;32m    998\u001b[0m     \u001b[39m# appear on a plotted ROC curve (and thus do not affect the AUC).\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1003\u001b[0m     \u001b[39m# but does not drop more complicated cases like fps = [1, 3, 7],\u001b[39;00m\n\u001b[0;32m   1004\u001b[0m     \u001b[39m# tps = [1, 2, 4]; there is no harm in keeping too many thresholds.\u001b[39;00m\n\u001b[0;32m   1005\u001b[0m     \u001b[39mif\u001b[39;00m drop_intermediate \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(fps) \u001b[39m>\u001b[39m \u001b[39m2\u001b[39m:\n",
      "File \u001b[1;32md:\\Miniconda\\envs\\ails\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:751\u001b[0m, in \u001b[0;36m_binary_clf_curve\u001b[1;34m(y_true, y_score, pos_label, sample_weight)\u001b[0m\n\u001b[0;32m    748\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (y_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbinary\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m (y_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mmulticlass\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m pos_label \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m)):\n\u001b[0;32m    749\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m format is not supported\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(y_type))\n\u001b[1;32m--> 751\u001b[0m check_consistent_length(y_true, y_score, sample_weight)\n\u001b[0;32m    752\u001b[0m y_true \u001b[39m=\u001b[39m column_or_1d(y_true)\n\u001b[0;32m    753\u001b[0m y_score \u001b[39m=\u001b[39m column_or_1d(y_score)\n",
      "File \u001b[1;32md:\\Miniconda\\envs\\ails\\lib\\site-packages\\sklearn\\utils\\validation.py:397\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    395\u001b[0m uniques \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39munique(lengths)\n\u001b[0;32m    396\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(uniques) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m--> 397\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    398\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    399\u001b[0m         \u001b[39m%\u001b[39m [\u001b[39mint\u001b[39m(l) \u001b[39mfor\u001b[39;00m l \u001b[39min\u001b[39;00m lengths]\n\u001b[0;32m    400\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [1023, 212]"
     ]
    }
   ],
   "source": [
    "auc_per_task = []\n",
    "for j in range(target.shape[1]):\n",
    "    y_score = submission.iloc[:, j]\n",
    "    print(y_score.shape)\n",
    "    y_true = target.iloc[:, j]\n",
    "    print(y_true.shape)\n",
    "        # mask out unknown samples\n",
    "    idx = (y_true != (-1))\n",
    "        # calculate AUC per task\n",
    "    auc_per_task.append(roc_auc_score(y_true[idx], y_score[idx]))\n",
    "avg_auc = np.mean(auc_per_task)\n",
    "print(avg_auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:rgb(0,120,170)\">Cluster Cross-Validation</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous model was trained with samples randomly assigned to the training and test sets. However, if we want to know how well our model generalizes to future data it might be a better idea to assign the training and test samples based on structural similarity. If we cluster the samples and assign all samples of some clusters to the training set and all samples of the other clusters to the test set we avoid that very similar samples are in the training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 1, 2, 3, 0])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We have 5 different cluster folds\n",
    "data['cluster_folds'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:24<00:00,  2.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7419875728081738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# We can select a list of test-folds here, to save time we only select one\n",
    "test_folds = [0]\n",
    "\n",
    "# For each test_fold we train a model on the remaining folds and calculate the AUC on the selected test fold\n",
    "for test_fold in test_folds:\n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = split_data(test_fold, fps, y, data['cluster_folds'])\n",
    "    y_hats_proba, y_hats_class = train_rf(X_train, y_train, X_test)\n",
    "\n",
    "    # Calculate mean AUC\n",
    "    auc_per_task = calc_masked_AUC_per_task(y_hats_proba, y_test)\n",
    "    print(np.mean(auc_per_task))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
