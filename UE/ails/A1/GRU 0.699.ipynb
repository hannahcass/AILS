{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import optuna\n",
    "\n",
    "\n",
    "import rdkit\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem, DataStructs\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>smiles</th>\n",
       "      <th>task1</th>\n",
       "      <th>task2</th>\n",
       "      <th>task3</th>\n",
       "      <th>task4</th>\n",
       "      <th>task5</th>\n",
       "      <th>task6</th>\n",
       "      <th>task7</th>\n",
       "      <th>task8</th>\n",
       "      <th>task9</th>\n",
       "      <th>task10</th>\n",
       "      <th>task11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CC(=O)N(C)c1cccc(-c2ccnc3c(C(=O)c4cccs4)cnn23)c1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>COc1cc(N)c(Cl)cc1C(=O)OCCCN1CCCCC1.Cl</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CCCCNc1c(C(=O)OCC)cnc2c1cnn2CC</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C#Cc1cccc(Nc2ncnc3cc(OCCOC)c(OCCOC)cc23)c1.Cl</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CC1OC2(CCCCC2Oc2cccc(Cl)c2)N=C1O</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11995</th>\n",
       "      <td>CC(C)(C)NC[C@@H](O)COc1nsnc1N1CCOCC1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11996</th>\n",
       "      <td>CCC[C@@]1(CCc2ccccc2)CC(O)=C([C@H](CC)c2cccc(N...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11997</th>\n",
       "      <td>N=C(O)c1cnc(C2CC2)[nH]1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11998</th>\n",
       "      <td>CN=C=O</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11999</th>\n",
       "      <td>Cc1ccc2c(c1N)C(=O)c1ccccc1C2=O</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12000 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  smiles  task1  task2  task3  \\\n",
       "0       CC(=O)N(C)c1cccc(-c2ccnc3c(C(=O)c4cccs4)cnn23)c1      0      0      0   \n",
       "1                  COc1cc(N)c(Cl)cc1C(=O)OCCCN1CCCCC1.Cl      0      0      0   \n",
       "2                         CCCCNc1c(C(=O)OCC)cnc2c1cnn2CC      0      0      0   \n",
       "3          C#Cc1cccc(Nc2ncnc3cc(OCCOC)c(OCCOC)cc23)c1.Cl      0      0      0   \n",
       "4                       CC1OC2(CCCCC2Oc2cccc(Cl)c2)N=C1O      0      0      0   \n",
       "...                                                  ...    ...    ...    ...   \n",
       "11995               CC(C)(C)NC[C@@H](O)COc1nsnc1N1CCOCC1      0      0      0   \n",
       "11996  CCC[C@@]1(CCc2ccccc2)CC(O)=C([C@H](CC)c2cccc(N...      0      0      0   \n",
       "11997                            N=C(O)c1cnc(C2CC2)[nH]1      0      0      0   \n",
       "11998                                             CN=C=O      0      0      0   \n",
       "11999                     Cc1ccc2c(c1N)C(=O)c1ccccc1C2=O      0      0      0   \n",
       "\n",
       "       task4  task5  task6  task7  task8  task9  task10  task11  \n",
       "0          0      0      0      0     -1      0       0       0  \n",
       "1          0      0      0      0     -1      0       0       0  \n",
       "2          0      0      0      0      0      0       1       0  \n",
       "3          0      0      0      0     -1      0       0       1  \n",
       "4          0      0      0      0      0      0       1       0  \n",
       "...      ...    ...    ...    ...    ...    ...     ...     ...  \n",
       "11995      0      0      0      0     -1      0       0       0  \n",
       "11996      0      0      0      0     -1      0       0       0  \n",
       "11997     -1      0      0      0      0      0       0       0  \n",
       "11998      0      0      0     -1      0      0       0       0  \n",
       "11999      0      0      0     -1      0      1       0       0  \n",
       "\n",
       "[12000 rows x 12 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"data_train.csv\", index_col=0).reset_index(drop=True)\n",
    "data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 1, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 1, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = data[data.columns[1:]].fillna(0)\n",
    "y=y.to_numpy()\n",
    "y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 2193/12000 [00:01<00:07, 1381.55it/s][22:55:28] WARNING: not removing hydrogen atom without neighbors\n",
      "[22:55:28] WARNING: not removing hydrogen atom without neighbors\n",
      " 20%|█▉        | 2355/12000 [00:01<00:06, 1443.11it/s][22:55:29] WARNING: not removing hydrogen atom without neighbors\n",
      "[22:55:29] WARNING: not removing hydrogen atom without neighbors\n",
      "100%|██████████| 12000/12000 [00:06<00:00, 1769.51it/s]\n"
     ]
    }
   ],
   "source": [
    "fp_length = 1024\n",
    "fps = np.zeros((len(data), fp_length))\n",
    "\n",
    "# Calculate Morgan fingerprints and convert to numpy array\n",
    "for i, smiles in enumerate(tqdm(data['smiles'])):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    fp_vec = AllChem.GetMorganFingerprintAsBitVect(\n",
    "        mol, radius=3, nBits=fp_length)\n",
    "    arr = np.zeros((1,))\n",
    "    DataStructs.ConvertToNumpyArray(fp_vec, arr)\n",
    "    fps[i] = arr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoleculeDataset(Dataset):\n",
    "    def __init__(self, fps, labels):\n",
    "        self.fps = fps\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.fps)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return torch.FloatTensor(self.fps[index]), torch.FloatTensor(self.labels[index])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 57\n",
    "input_size = 1024\n",
    "sequence_length = 1\n",
    "output_size = 11\n",
    "#num_epochs = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "fps = fps.reshape(-1, sequence_length, input_size)\n",
    "\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(\n",
    "    fps, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "train_dataset = MoleculeDataset(X_train, Y_train)\n",
    "val_dataset = MoleculeDataset(X_val, Y_val)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputs shape: torch.Size([48, 11])\n",
      "Labels shape: torch.Size([48, 11])\n"
     ]
    }
   ],
   "source": [
    "print(\"Outputs shape:\", outputs.shape)\n",
    "print(\"Labels shape:\", labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SimpleGRU(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, dropout_rate, device):\n",
    "        super(SimpleGRU, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.device = device\n",
    "\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers,\n",
    "                          batch_first=True, dropout=dropout_rate)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(\n",
    "            0), self.hidden_size).to(self.device)\n",
    "        out, _ = self.gru(x, h0)\n",
    "        out = self.dropout(out[:, -1, :])\n",
    "        out = self.fc(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout_rate=0.5):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size,\n",
    "                            num_layers, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(\n",
    "            0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(\n",
    "            0), self.hidden_size).to(x.device)\n",
    "\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # Suggest hyperparameters using the trial object\n",
    "    hidden_size = trial.suggest_int(\"hidden_size\", 32, 512)\n",
    "    num_layers = trial.suggest_int(\"num_layers\", 1, 5)\n",
    "    dropout_rate = trial.suggest_float(\"dropout_rate\", 0.1, 0.8)\n",
    "    learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-6, 1e-2)\n",
    "    #weight_decay = trial.suggest_loguniform(\"weight_decay\", 1e-5, 1e-1)\n",
    "    #num_epochs = trial.suggest_int(\"num_epochs\", 10, 100)\n",
    "    batch_size = trial.suggest_int(\"batch_size\", 32, 128)\n",
    "\n",
    "\n",
    "    # Initialize your model using the suggested hyperparameters\n",
    "    #model = LSTMModel(input_size, hidden_size, num_layers,\n",
    "    #                  output_size, dropout_rate).to(device)\n",
    "\n",
    "    model = SimpleGRU(input_size, hidden_size,\n",
    "                      num_layers, dropout_rate, device).to(device)\n",
    "\n",
    "    # Define your loss function and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)#, weight_decay=weight_decay)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    num_epochs=50\n",
    "\n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        for i, (fps, labels) in enumerate(train_loader):\n",
    "            inputs = fps.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            mask = labels != 0\n",
    "            masked_outputs = outputs[mask]\n",
    "            masked_labels = labels[mask]\n",
    "            #loss = criterion(outputs, labels)\n",
    "            loss = criterion(masked_outputs, masked_labels)\n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        train_loss = np.mean(train_losses)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_losses = []\n",
    "        with torch.no_grad():\n",
    "            for fps, labels in val_loader:\n",
    "                inputs = fps.to(device)\n",
    "                labels = labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                mask = labels != 0\n",
    "                masked_outputs = outputs[mask]\n",
    "                masked_labels = labels[mask]\n",
    "                loss = criterion(masked_outputs, masked_labels)\n",
    "                val_losses.append(loss.item())\n",
    "\n",
    "        val_loss = np.mean(val_losses)\n",
    "\n",
    "        print(\n",
    "            f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "        # Return the validation loss (or another metric you want to minimize)\n",
    "    return val_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-04-17 23:56:29,528]\u001b[0m A new study created in memory with name: no-name-b4932160-1058-4228-a348-72d13676f095\u001b[0m\n",
      "d:\\Miniconda\\envs\\ails\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:62: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.10746702576237721 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/50], Train Loss: 0.9876, Validation Loss: 0.9667\n",
      "Epoch [3/50], Train Loss: 0.9276, Validation Loss: 0.9118\n",
      "Epoch [4/50], Train Loss: 0.8758, Validation Loss: 0.8623\n",
      "Epoch [5/50], Train Loss: 0.8280, Validation Loss: 0.8175\n",
      "Epoch [6/50], Train Loss: 0.7854, Validation Loss: 0.7767\n",
      "Epoch [7/50], Train Loss: 0.7459, Validation Loss: 0.7396\n",
      "Epoch [8/50], Train Loss: 0.7117, Validation Loss: 0.7062\n",
      "Epoch [9/50], Train Loss: 0.6762, Validation Loss: 0.6768\n",
      "Epoch [10/50], Train Loss: 0.6495, Validation Loss: 0.6503\n",
      "Epoch [11/50], Train Loss: 0.6260, Validation Loss: 0.6275\n",
      "Epoch [12/50], Train Loss: 0.6065, Validation Loss: 0.6079\n",
      "Epoch [13/50], Train Loss: 0.5833, Validation Loss: 0.5911\n",
      "Epoch [14/50], Train Loss: 0.5705, Validation Loss: 0.5767\n",
      "Epoch [15/50], Train Loss: 0.5620, Validation Loss: 0.5644\n",
      "Epoch [16/50], Train Loss: 0.5475, Validation Loss: 0.5540\n",
      "Epoch [17/50], Train Loss: 0.5400, Validation Loss: 0.5453\n",
      "Epoch [18/50], Train Loss: 0.5300, Validation Loss: 0.5372\n",
      "Epoch [19/50], Train Loss: 0.5183, Validation Loss: 0.5300\n",
      "Epoch [20/50], Train Loss: 0.5115, Validation Loss: 0.5236\n",
      "Epoch [21/50], Train Loss: 0.5081, Validation Loss: 0.5179\n",
      "Epoch [22/50], Train Loss: 0.5021, Validation Loss: 0.5132\n",
      "Epoch [23/50], Train Loss: 0.4929, Validation Loss: 0.5087\n",
      "Epoch [24/50], Train Loss: 0.4913, Validation Loss: 0.5044\n",
      "Epoch [25/50], Train Loss: 0.4843, Validation Loss: 0.5005\n",
      "Epoch [26/50], Train Loss: 0.4826, Validation Loss: 0.4968\n",
      "Epoch [27/50], Train Loss: 0.4805, Validation Loss: 0.4933\n",
      "Epoch [28/50], Train Loss: 0.4699, Validation Loss: 0.4902\n",
      "Epoch [29/50], Train Loss: 0.4673, Validation Loss: 0.4871\n",
      "Epoch [30/50], Train Loss: 0.4605, Validation Loss: 0.4841\n",
      "Epoch [31/50], Train Loss: 0.4600, Validation Loss: 0.4813\n",
      "Epoch [32/50], Train Loss: 0.4564, Validation Loss: 0.4783\n",
      "Epoch [33/50], Train Loss: 0.4547, Validation Loss: 0.4758\n",
      "Epoch [34/50], Train Loss: 0.4560, Validation Loss: 0.4732\n",
      "Epoch [35/50], Train Loss: 0.4458, Validation Loss: 0.4710\n",
      "Epoch [36/50], Train Loss: 0.4489, Validation Loss: 0.4685\n",
      "Epoch [37/50], Train Loss: 0.4417, Validation Loss: 0.4663\n",
      "Epoch [38/50], Train Loss: 0.4440, Validation Loss: 0.4641\n",
      "Epoch [39/50], Train Loss: 0.4379, Validation Loss: 0.4620\n",
      "Epoch [40/50], Train Loss: 0.4360, Validation Loss: 0.4598\n",
      "Epoch [41/50], Train Loss: 0.4283, Validation Loss: 0.4577\n",
      "Epoch [42/50], Train Loss: 0.4306, Validation Loss: 0.4555\n",
      "Epoch [43/50], Train Loss: 0.4262, Validation Loss: 0.4537\n",
      "Epoch [44/50], Train Loss: 0.4210, Validation Loss: 0.4517\n",
      "Epoch [45/50], Train Loss: 0.4164, Validation Loss: 0.4497\n",
      "Epoch [46/50], Train Loss: 0.4161, Validation Loss: 0.4478\n",
      "Epoch [47/50], Train Loss: 0.4124, Validation Loss: 0.4459\n",
      "Epoch [48/50], Train Loss: 0.4086, Validation Loss: 0.4441\n",
      "Epoch [49/50], Train Loss: 0.4064, Validation Loss: 0.4422\n",
      "Epoch [50/50], Train Loss: 0.4105, Validation Loss: 0.4406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-04-17 23:56:51,858]\u001b[0m Trial 0 finished with value: 0.4388754225295523 and parameters: {'hidden_size': 323, 'num_layers': 1, 'dropout_rate': 0.10746702576237721, 'learning_rate': 6.872819879249902e-06, 'batch_size': 109}. Best is trial 0 with value: 0.4388754225295523.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [51/50], Train Loss: 0.4060, Validation Loss: 0.4389\n",
      "Epoch [2/50], Train Loss: 0.9842, Validation Loss: 0.9635\n",
      "Epoch [3/50], Train Loss: 0.9376, Validation Loss: 0.9027\n",
      "Epoch [4/50], Train Loss: 0.8301, Validation Loss: 0.7219\n",
      "Epoch [5/50], Train Loss: 0.5982, Validation Loss: 0.5220\n",
      "Epoch [6/50], Train Loss: 0.5069, Validation Loss: 0.4927\n",
      "Epoch [7/50], Train Loss: 0.4868, Validation Loss: 0.4800\n",
      "Epoch [8/50], Train Loss: 0.4741, Validation Loss: 0.4715\n",
      "Epoch [9/50], Train Loss: 0.4672, Validation Loss: 0.4655\n",
      "Epoch [10/50], Train Loss: 0.4616, Validation Loss: 0.4604\n",
      "Epoch [11/50], Train Loss: 0.4548, Validation Loss: 0.4559\n",
      "Epoch [12/50], Train Loss: 0.4527, Validation Loss: 0.4522\n",
      "Epoch [13/50], Train Loss: 0.4456, Validation Loss: 0.4486\n",
      "Epoch [14/50], Train Loss: 0.4431, Validation Loss: 0.4457\n",
      "Epoch [15/50], Train Loss: 0.4407, Validation Loss: 0.4428\n",
      "Epoch [16/50], Train Loss: 0.4376, Validation Loss: 0.4402\n",
      "Epoch [17/50], Train Loss: 0.4340, Validation Loss: 0.4377\n",
      "Epoch [18/50], Train Loss: 0.4326, Validation Loss: 0.4351\n",
      "Epoch [19/50], Train Loss: 0.4294, Validation Loss: 0.4326\n",
      "Epoch [20/50], Train Loss: 0.4250, Validation Loss: 0.4304\n",
      "Epoch [21/50], Train Loss: 0.4233, Validation Loss: 0.4283\n",
      "Epoch [22/50], Train Loss: 0.4188, Validation Loss: 0.4263\n",
      "Epoch [23/50], Train Loss: 0.4169, Validation Loss: 0.4244\n",
      "Epoch [24/50], Train Loss: 0.4154, Validation Loss: 0.4229\n",
      "Epoch [25/50], Train Loss: 0.4141, Validation Loss: 0.4214\n",
      "Epoch [26/50], Train Loss: 0.4102, Validation Loss: 0.4203\n",
      "Epoch [27/50], Train Loss: 0.4090, Validation Loss: 0.4191\n",
      "Epoch [28/50], Train Loss: 0.4071, Validation Loss: 0.4182\n",
      "Epoch [29/50], Train Loss: 0.4070, Validation Loss: 0.4171\n",
      "Epoch [30/50], Train Loss: 0.4044, Validation Loss: 0.4162\n",
      "Epoch [31/50], Train Loss: 0.4050, Validation Loss: 0.4155\n",
      "Epoch [32/50], Train Loss: 0.4019, Validation Loss: 0.4146\n",
      "Epoch [33/50], Train Loss: 0.4003, Validation Loss: 0.4140\n",
      "Epoch [34/50], Train Loss: 0.4007, Validation Loss: 0.4132\n",
      "Epoch [35/50], Train Loss: 0.3976, Validation Loss: 0.4126\n",
      "Epoch [36/50], Train Loss: 0.3958, Validation Loss: 0.4118\n",
      "Epoch [37/50], Train Loss: 0.3960, Validation Loss: 0.4113\n",
      "Epoch [38/50], Train Loss: 0.3947, Validation Loss: 0.4104\n",
      "Epoch [39/50], Train Loss: 0.3919, Validation Loss: 0.4098\n",
      "Epoch [40/50], Train Loss: 0.3910, Validation Loss: 0.4092\n",
      "Epoch [41/50], Train Loss: 0.3918, Validation Loss: 0.4082\n",
      "Epoch [42/50], Train Loss: 0.3903, Validation Loss: 0.4075\n",
      "Epoch [43/50], Train Loss: 0.3866, Validation Loss: 0.4065\n",
      "Epoch [44/50], Train Loss: 0.3860, Validation Loss: 0.4065\n",
      "Epoch [45/50], Train Loss: 0.3840, Validation Loss: 0.4051\n",
      "Epoch [46/50], Train Loss: 0.3827, Validation Loss: 0.4041\n",
      "Epoch [47/50], Train Loss: 0.3809, Validation Loss: 0.4032\n",
      "Epoch [48/50], Train Loss: 0.3789, Validation Loss: 0.4022\n",
      "Epoch [49/50], Train Loss: 0.3777, Validation Loss: 0.4014\n",
      "Epoch [50/50], Train Loss: 0.3752, Validation Loss: 0.4003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-04-17 23:57:27,374]\u001b[0m Trial 1 finished with value: 0.3993268189461608 and parameters: {'hidden_size': 272, 'num_layers': 4, 'dropout_rate': 0.2528314317083498, 'learning_rate': 8.9891219200738e-06, 'batch_size': 64}. Best is trial 1 with value: 0.3993268189461608.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [51/50], Train Loss: 0.3743, Validation Loss: 0.3993\n",
      "Epoch [2/50], Train Loss: 0.9817, Validation Loss: 0.9440\n",
      "Epoch [3/50], Train Loss: 0.8248, Validation Loss: 0.5877\n",
      "Epoch [4/50], Train Loss: 0.5083, Validation Loss: 0.4829\n",
      "Epoch [5/50], Train Loss: 0.4724, Validation Loss: 0.4649\n",
      "Epoch [6/50], Train Loss: 0.4606, Validation Loss: 0.4548\n",
      "Epoch [7/50], Train Loss: 0.4512, Validation Loss: 0.4481\n",
      "Epoch [8/50], Train Loss: 0.4452, Validation Loss: 0.4429\n",
      "Epoch [9/50], Train Loss: 0.4421, Validation Loss: 0.4386\n",
      "Epoch [10/50], Train Loss: 0.4349, Validation Loss: 0.4345\n",
      "Epoch [11/50], Train Loss: 0.4331, Validation Loss: 0.4312\n",
      "Epoch [12/50], Train Loss: 0.4280, Validation Loss: 0.4279\n",
      "Epoch [13/50], Train Loss: 0.4250, Validation Loss: 0.4248\n",
      "Epoch [14/50], Train Loss: 0.4201, Validation Loss: 0.4220\n",
      "Epoch [15/50], Train Loss: 0.4177, Validation Loss: 0.4197\n",
      "Epoch [16/50], Train Loss: 0.4130, Validation Loss: 0.4181\n",
      "Epoch [17/50], Train Loss: 0.4111, Validation Loss: 0.4166\n",
      "Epoch [18/50], Train Loss: 0.4072, Validation Loss: 0.4159\n",
      "Epoch [19/50], Train Loss: 0.4088, Validation Loss: 0.4148\n",
      "Epoch [20/50], Train Loss: 0.4079, Validation Loss: 0.4140\n",
      "Epoch [21/50], Train Loss: 0.4057, Validation Loss: 0.4138\n",
      "Epoch [22/50], Train Loss: 0.4034, Validation Loss: 0.4131\n",
      "Epoch [23/50], Train Loss: 0.4028, Validation Loss: 0.4127\n",
      "Epoch [24/50], Train Loss: 0.4016, Validation Loss: 0.4125\n",
      "Epoch [25/50], Train Loss: 0.4003, Validation Loss: 0.4121\n",
      "Epoch [26/50], Train Loss: 0.3992, Validation Loss: 0.4117\n",
      "Epoch [27/50], Train Loss: 0.3967, Validation Loss: 0.4115\n",
      "Epoch [28/50], Train Loss: 0.3959, Validation Loss: 0.4112\n",
      "Epoch [29/50], Train Loss: 0.3979, Validation Loss: 0.4107\n",
      "Epoch [30/50], Train Loss: 0.3925, Validation Loss: 0.4105\n",
      "Epoch [31/50], Train Loss: 0.3931, Validation Loss: 0.4097\n",
      "Epoch [32/50], Train Loss: 0.3936, Validation Loss: 0.4099\n",
      "Epoch [33/50], Train Loss: 0.3898, Validation Loss: 0.4093\n",
      "Epoch [34/50], Train Loss: 0.3885, Validation Loss: 0.4081\n",
      "Epoch [35/50], Train Loss: 0.3900, Validation Loss: 0.4075\n",
      "Epoch [36/50], Train Loss: 0.3872, Validation Loss: 0.4064\n",
      "Epoch [37/50], Train Loss: 0.3809, Validation Loss: 0.4053\n",
      "Epoch [38/50], Train Loss: 0.3848, Validation Loss: 0.4037\n",
      "Epoch [39/50], Train Loss: 0.3786, Validation Loss: 0.4017\n",
      "Epoch [40/50], Train Loss: 0.3763, Validation Loss: 0.3998\n",
      "Epoch [41/50], Train Loss: 0.3697, Validation Loss: 0.3976\n",
      "Epoch [42/50], Train Loss: 0.3662, Validation Loss: 0.3953\n",
      "Epoch [43/50], Train Loss: 0.3668, Validation Loss: 0.3925\n",
      "Epoch [44/50], Train Loss: 0.3618, Validation Loss: 0.3893\n",
      "Epoch [45/50], Train Loss: 0.3580, Validation Loss: 0.3872\n",
      "Epoch [46/50], Train Loss: 0.3534, Validation Loss: 0.3850\n",
      "Epoch [47/50], Train Loss: 0.3479, Validation Loss: 0.3831\n",
      "Epoch [48/50], Train Loss: 0.3464, Validation Loss: 0.3820\n",
      "Epoch [49/50], Train Loss: 0.3443, Validation Loss: 0.3815\n",
      "Epoch [50/50], Train Loss: 0.3435, Validation Loss: 0.3811\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-04-17 23:58:12,967]\u001b[0m Trial 2 finished with value: 0.3810829439240953 and parameters: {'hidden_size': 296, 'num_layers': 5, 'dropout_rate': 0.3457489972885964, 'learning_rate': 1.2103690759157509e-05, 'batch_size': 53}. Best is trial 2 with value: 0.3810829439240953.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [51/50], Train Loss: 0.3420, Validation Loss: 0.3811\n",
      "Epoch [2/50], Train Loss: 0.9938, Validation Loss: 0.9897\n",
      "Epoch [3/50], Train Loss: 0.9863, Validation Loss: 0.9826\n",
      "Epoch [4/50], Train Loss: 0.9790, Validation Loss: 0.9755\n",
      "Epoch [5/50], Train Loss: 0.9722, Validation Loss: 0.9682\n",
      "Epoch [6/50], Train Loss: 0.9646, Validation Loss: 0.9606\n",
      "Epoch [7/50], Train Loss: 0.9565, Validation Loss: 0.9527\n",
      "Epoch [8/50], Train Loss: 0.9479, Validation Loss: 0.9442\n",
      "Epoch [9/50], Train Loss: 0.9395, Validation Loss: 0.9349\n",
      "Epoch [10/50], Train Loss: 0.9293, Validation Loss: 0.9248\n",
      "Epoch [11/50], Train Loss: 0.9184, Validation Loss: 0.9134\n",
      "Epoch [12/50], Train Loss: 0.9064, Validation Loss: 0.9004\n",
      "Epoch [13/50], Train Loss: 0.8922, Validation Loss: 0.8851\n",
      "Epoch [14/50], Train Loss: 0.8754, Validation Loss: 0.8671\n",
      "Epoch [15/50], Train Loss: 0.8553, Validation Loss: 0.8453\n",
      "Epoch [16/50], Train Loss: 0.8311, Validation Loss: 0.8188\n",
      "Epoch [17/50], Train Loss: 0.8018, Validation Loss: 0.7869\n",
      "Epoch [18/50], Train Loss: 0.7660, Validation Loss: 0.7492\n",
      "Epoch [19/50], Train Loss: 0.7266, Validation Loss: 0.7066\n",
      "Epoch [20/50], Train Loss: 0.6828, Validation Loss: 0.6616\n",
      "Epoch [21/50], Train Loss: 0.6391, Validation Loss: 0.6184\n",
      "Epoch [22/50], Train Loss: 0.5990, Validation Loss: 0.5812\n",
      "Epoch [23/50], Train Loss: 0.5666, Validation Loss: 0.5535\n",
      "Epoch [24/50], Train Loss: 0.5439, Validation Loss: 0.5350\n",
      "Epoch [25/50], Train Loss: 0.5287, Validation Loss: 0.5230\n",
      "Epoch [26/50], Train Loss: 0.5201, Validation Loss: 0.5147\n",
      "Epoch [27/50], Train Loss: 0.5103, Validation Loss: 0.5083\n",
      "Epoch [28/50], Train Loss: 0.5053, Validation Loss: 0.5030\n",
      "Epoch [29/50], Train Loss: 0.5009, Validation Loss: 0.4985\n",
      "Epoch [30/50], Train Loss: 0.4951, Validation Loss: 0.4944\n",
      "Epoch [31/50], Train Loss: 0.4917, Validation Loss: 0.4907\n",
      "Epoch [32/50], Train Loss: 0.4890, Validation Loss: 0.4873\n",
      "Epoch [33/50], Train Loss: 0.4850, Validation Loss: 0.4843\n",
      "Epoch [34/50], Train Loss: 0.4810, Validation Loss: 0.4814\n",
      "Epoch [35/50], Train Loss: 0.4787, Validation Loss: 0.4789\n",
      "Epoch [36/50], Train Loss: 0.4761, Validation Loss: 0.4765\n",
      "Epoch [37/50], Train Loss: 0.4717, Validation Loss: 0.4743\n",
      "Epoch [38/50], Train Loss: 0.4729, Validation Loss: 0.4722\n",
      "Epoch [39/50], Train Loss: 0.4703, Validation Loss: 0.4704\n",
      "Epoch [40/50], Train Loss: 0.4672, Validation Loss: 0.4686\n",
      "Epoch [41/50], Train Loss: 0.4641, Validation Loss: 0.4670\n",
      "Epoch [42/50], Train Loss: 0.4628, Validation Loss: 0.4655\n",
      "Epoch [43/50], Train Loss: 0.4621, Validation Loss: 0.4641\n",
      "Epoch [44/50], Train Loss: 0.4633, Validation Loss: 0.4628\n",
      "Epoch [45/50], Train Loss: 0.4609, Validation Loss: 0.4616\n",
      "Epoch [46/50], Train Loss: 0.4580, Validation Loss: 0.4603\n",
      "Epoch [47/50], Train Loss: 0.4603, Validation Loss: 0.4592\n",
      "Epoch [48/50], Train Loss: 0.4604, Validation Loss: 0.4582\n",
      "Epoch [49/50], Train Loss: 0.4552, Validation Loss: 0.4573\n",
      "Epoch [50/50], Train Loss: 0.4561, Validation Loss: 0.4564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-04-17 23:59:23,663]\u001b[0m Trial 3 finished with value: 0.4555260548988978 and parameters: {'hidden_size': 250, 'num_layers': 5, 'dropout_rate': 0.3311338482325642, 'learning_rate': 1.041724203907712e-06, 'batch_size': 32}. Best is trial 2 with value: 0.3810829439240953.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [51/50], Train Loss: 0.4533, Validation Loss: 0.4555\n",
      "Epoch [2/50], Train Loss: 0.5034, Validation Loss: 0.4154\n",
      "Epoch [3/50], Train Loss: 0.4162, Validation Loss: 0.3978\n",
      "Epoch [4/50], Train Loss: 0.3827, Validation Loss: 0.3737\n",
      "Epoch [5/50], Train Loss: 0.3487, Validation Loss: 0.3767\n",
      "Epoch [6/50], Train Loss: 0.3290, Validation Loss: 0.3789\n",
      "Epoch [7/50], Train Loss: 0.3157, Validation Loss: 0.3908\n",
      "Epoch [8/50], Train Loss: 0.2955, Validation Loss: 0.3851\n",
      "Epoch [9/50], Train Loss: 0.2826, Validation Loss: 0.3828\n",
      "Epoch [10/50], Train Loss: 0.2686, Validation Loss: 0.3909\n",
      "Epoch [11/50], Train Loss: 0.2636, Validation Loss: 0.4006\n",
      "Epoch [12/50], Train Loss: 0.2503, Validation Loss: 0.4092\n",
      "Epoch [13/50], Train Loss: 0.2410, Validation Loss: 0.3934\n",
      "Epoch [14/50], Train Loss: 0.2308, Validation Loss: 0.4032\n",
      "Epoch [15/50], Train Loss: 0.2239, Validation Loss: 0.3990\n",
      "Epoch [16/50], Train Loss: 0.2152, Validation Loss: 0.4051\n",
      "Epoch [17/50], Train Loss: 0.2115, Validation Loss: 0.4186\n",
      "Epoch [18/50], Train Loss: 0.2053, Validation Loss: 0.3984\n",
      "Epoch [19/50], Train Loss: 0.2045, Validation Loss: 0.4108\n",
      "Epoch [20/50], Train Loss: 0.1981, Validation Loss: 0.4158\n",
      "Epoch [21/50], Train Loss: 0.1913, Validation Loss: 0.4139\n",
      "Epoch [22/50], Train Loss: 0.1902, Validation Loss: 0.4216\n",
      "Epoch [23/50], Train Loss: 0.1833, Validation Loss: 0.4136\n",
      "Epoch [24/50], Train Loss: 0.1805, Validation Loss: 0.4126\n",
      "Epoch [25/50], Train Loss: 0.1759, Validation Loss: 0.4162\n",
      "Epoch [26/50], Train Loss: 0.1781, Validation Loss: 0.4183\n",
      "Epoch [27/50], Train Loss: 0.1709, Validation Loss: 0.4243\n",
      "Epoch [28/50], Train Loss: 0.1678, Validation Loss: 0.4359\n",
      "Epoch [29/50], Train Loss: 0.1660, Validation Loss: 0.4323\n",
      "Epoch [30/50], Train Loss: 0.1700, Validation Loss: 0.4240\n",
      "Epoch [31/50], Train Loss: 0.1584, Validation Loss: 0.4318\n",
      "Epoch [32/50], Train Loss: 0.1588, Validation Loss: 0.4390\n",
      "Epoch [33/50], Train Loss: 0.1567, Validation Loss: 0.4333\n",
      "Epoch [34/50], Train Loss: 0.1502, Validation Loss: 0.4362\n",
      "Epoch [35/50], Train Loss: 0.1506, Validation Loss: 0.4197\n",
      "Epoch [36/50], Train Loss: 0.1482, Validation Loss: 0.4381\n",
      "Epoch [37/50], Train Loss: 0.1454, Validation Loss: 0.4325\n",
      "Epoch [38/50], Train Loss: 0.1459, Validation Loss: 0.4487\n",
      "Epoch [39/50], Train Loss: 0.1491, Validation Loss: 0.4413\n",
      "Epoch [40/50], Train Loss: 0.1486, Validation Loss: 0.4398\n",
      "Epoch [41/50], Train Loss: 0.1393, Validation Loss: 0.4360\n",
      "Epoch [42/50], Train Loss: 0.1438, Validation Loss: 0.4289\n",
      "Epoch [43/50], Train Loss: 0.1404, Validation Loss: 0.4315\n",
      "Epoch [44/50], Train Loss: 0.1378, Validation Loss: 0.4399\n",
      "Epoch [45/50], Train Loss: 0.1423, Validation Loss: 0.4410\n",
      "Epoch [46/50], Train Loss: 0.1380, Validation Loss: 0.4456\n",
      "Epoch [47/50], Train Loss: 0.1373, Validation Loss: 0.4457\n",
      "Epoch [48/50], Train Loss: 0.1361, Validation Loss: 0.4379\n",
      "Epoch [49/50], Train Loss: 0.1299, Validation Loss: 0.4401\n",
      "Epoch [50/50], Train Loss: 0.1390, Validation Loss: 0.4386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-04-17 23:59:55,781]\u001b[0m Trial 4 finished with value: 0.44420358041922253 and parameters: {'hidden_size': 104, 'num_layers': 5, 'dropout_rate': 0.5164924089667202, 'learning_rate': 0.001505473986603293, 'batch_size': 82}. Best is trial 2 with value: 0.3810829439240953.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [51/50], Train Loss: 0.1331, Validation Loss: 0.4442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Miniconda\\envs\\ails\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:62: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.36848092665225707 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/50], Train Loss: 0.5930, Validation Loss: 0.4714\n",
      "Epoch [3/50], Train Loss: 0.4263, Validation Loss: 0.4255\n",
      "Epoch [4/50], Train Loss: 0.3667, Validation Loss: 0.3956\n",
      "Epoch [5/50], Train Loss: 0.3293, Validation Loss: 0.3788\n",
      "Epoch [6/50], Train Loss: 0.3003, Validation Loss: 0.3711\n",
      "Epoch [7/50], Train Loss: 0.2787, Validation Loss: 0.3659\n",
      "Epoch [8/50], Train Loss: 0.2634, Validation Loss: 0.3642\n",
      "Epoch [9/50], Train Loss: 0.2458, Validation Loss: 0.3662\n",
      "Epoch [10/50], Train Loss: 0.2302, Validation Loss: 0.3684\n",
      "Epoch [11/50], Train Loss: 0.2183, Validation Loss: 0.3701\n",
      "Epoch [12/50], Train Loss: 0.2094, Validation Loss: 0.3748\n",
      "Epoch [13/50], Train Loss: 0.2020, Validation Loss: 0.3795\n",
      "Epoch [14/50], Train Loss: 0.1916, Validation Loss: 0.3830\n",
      "Epoch [15/50], Train Loss: 0.1836, Validation Loss: 0.3865\n",
      "Epoch [16/50], Train Loss: 0.1789, Validation Loss: 0.3893\n",
      "Epoch [17/50], Train Loss: 0.1717, Validation Loss: 0.3948\n",
      "Epoch [18/50], Train Loss: 0.1655, Validation Loss: 0.3992\n",
      "Epoch [19/50], Train Loss: 0.1629, Validation Loss: 0.4008\n",
      "Epoch [20/50], Train Loss: 0.1576, Validation Loss: 0.4033\n",
      "Epoch [21/50], Train Loss: 0.1505, Validation Loss: 0.4081\n",
      "Epoch [22/50], Train Loss: 0.1502, Validation Loss: 0.4129\n",
      "Epoch [23/50], Train Loss: 0.1445, Validation Loss: 0.4140\n",
      "Epoch [24/50], Train Loss: 0.1402, Validation Loss: 0.4160\n",
      "Epoch [25/50], Train Loss: 0.1383, Validation Loss: 0.4186\n",
      "Epoch [26/50], Train Loss: 0.1321, Validation Loss: 0.4226\n",
      "Epoch [27/50], Train Loss: 0.1293, Validation Loss: 0.4243\n",
      "Epoch [28/50], Train Loss: 0.1260, Validation Loss: 0.4253\n",
      "Epoch [29/50], Train Loss: 0.1228, Validation Loss: 0.4247\n",
      "Epoch [30/50], Train Loss: 0.1199, Validation Loss: 0.4284\n",
      "Epoch [31/50], Train Loss: 0.1183, Validation Loss: 0.4278\n",
      "Epoch [32/50], Train Loss: 0.1146, Validation Loss: 0.4337\n",
      "Epoch [33/50], Train Loss: 0.1134, Validation Loss: 0.4350\n",
      "Epoch [34/50], Train Loss: 0.1081, Validation Loss: 0.4377\n",
      "Epoch [35/50], Train Loss: 0.1069, Validation Loss: 0.4331\n",
      "Epoch [36/50], Train Loss: 0.1039, Validation Loss: 0.4353\n",
      "Epoch [37/50], Train Loss: 0.1005, Validation Loss: 0.4374\n",
      "Epoch [38/50], Train Loss: 0.0989, Validation Loss: 0.4382\n",
      "Epoch [39/50], Train Loss: 0.0963, Validation Loss: 0.4401\n",
      "Epoch [40/50], Train Loss: 0.0938, Validation Loss: 0.4395\n",
      "Epoch [41/50], Train Loss: 0.0924, Validation Loss: 0.4389\n",
      "Epoch [42/50], Train Loss: 0.0906, Validation Loss: 0.4404\n",
      "Epoch [43/50], Train Loss: 0.0869, Validation Loss: 0.4416\n",
      "Epoch [44/50], Train Loss: 0.0852, Validation Loss: 0.4436\n",
      "Epoch [45/50], Train Loss: 0.0854, Validation Loss: 0.4424\n",
      "Epoch [46/50], Train Loss: 0.0823, Validation Loss: 0.4433\n",
      "Epoch [47/50], Train Loss: 0.0802, Validation Loss: 0.4411\n",
      "Epoch [48/50], Train Loss: 0.0796, Validation Loss: 0.4451\n",
      "Epoch [49/50], Train Loss: 0.0764, Validation Loss: 0.4418\n",
      "Epoch [50/50], Train Loss: 0.0756, Validation Loss: 0.4438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-04-18 00:00:33,201]\u001b[0m Trial 5 finished with value: 0.44441847388560957 and parameters: {'hidden_size': 313, 'num_layers': 1, 'dropout_rate': 0.36848092665225707, 'learning_rate': 0.0002222952575912816, 'batch_size': 94}. Best is trial 2 with value: 0.3810829439240953.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [51/50], Train Loss: 0.0737, Validation Loss: 0.4444\n",
      "Epoch [2/50], Train Loss: 0.6935, Validation Loss: 0.4524\n",
      "Epoch [3/50], Train Loss: 0.4362, Validation Loss: 0.4311\n",
      "Epoch [4/50], Train Loss: 0.4238, Validation Loss: 0.4270\n",
      "Epoch [5/50], Train Loss: 0.4111, Validation Loss: 0.4163\n",
      "Epoch [6/50], Train Loss: 0.4008, Validation Loss: 0.4105\n",
      "Epoch [7/50], Train Loss: 0.3842, Validation Loss: 0.3962\n",
      "Epoch [8/50], Train Loss: 0.3565, Validation Loss: 0.3817\n",
      "Epoch [9/50], Train Loss: 0.3382, Validation Loss: 0.3790\n",
      "Epoch [10/50], Train Loss: 0.3202, Validation Loss: 0.3843\n",
      "Epoch [11/50], Train Loss: 0.3155, Validation Loss: 0.3821\n",
      "Epoch [12/50], Train Loss: 0.3040, Validation Loss: 0.3849\n",
      "Epoch [13/50], Train Loss: 0.2959, Validation Loss: 0.3908\n",
      "Epoch [14/50], Train Loss: 0.2911, Validation Loss: 0.3950\n",
      "Epoch [15/50], Train Loss: 0.2828, Validation Loss: 0.3967\n",
      "Epoch [16/50], Train Loss: 0.2756, Validation Loss: 0.3975\n",
      "Epoch [17/50], Train Loss: 0.2683, Validation Loss: 0.3988\n",
      "Epoch [18/50], Train Loss: 0.2604, Validation Loss: 0.4083\n",
      "Epoch [19/50], Train Loss: 0.2584, Validation Loss: 0.4031\n",
      "Epoch [20/50], Train Loss: 0.2508, Validation Loss: 0.4085\n",
      "Epoch [21/50], Train Loss: 0.2428, Validation Loss: 0.4191\n",
      "Epoch [22/50], Train Loss: 0.2345, Validation Loss: 0.4193\n",
      "Epoch [23/50], Train Loss: 0.2299, Validation Loss: 0.4218\n",
      "Epoch [24/50], Train Loss: 0.2286, Validation Loss: 0.4187\n",
      "Epoch [25/50], Train Loss: 0.2218, Validation Loss: 0.4255\n",
      "Epoch [26/50], Train Loss: 0.2145, Validation Loss: 0.4268\n",
      "Epoch [27/50], Train Loss: 0.2118, Validation Loss: 0.4325\n",
      "Epoch [28/50], Train Loss: 0.2073, Validation Loss: 0.4279\n",
      "Epoch [29/50], Train Loss: 0.2014, Validation Loss: 0.4339\n",
      "Epoch [30/50], Train Loss: 0.1988, Validation Loss: 0.4308\n",
      "Epoch [31/50], Train Loss: 0.1955, Validation Loss: 0.4329\n",
      "Epoch [32/50], Train Loss: 0.1920, Validation Loss: 0.4437\n",
      "Epoch [33/50], Train Loss: 0.1855, Validation Loss: 0.4432\n",
      "Epoch [34/50], Train Loss: 0.1829, Validation Loss: 0.4430\n",
      "Epoch [35/50], Train Loss: 0.1810, Validation Loss: 0.4442\n",
      "Epoch [36/50], Train Loss: 0.1773, Validation Loss: 0.4491\n",
      "Epoch [37/50], Train Loss: 0.1767, Validation Loss: 0.4565\n",
      "Epoch [38/50], Train Loss: 0.1687, Validation Loss: 0.4558\n",
      "Epoch [39/50], Train Loss: 0.1683, Validation Loss: 0.4540\n",
      "Epoch [40/50], Train Loss: 0.1673, Validation Loss: 0.4503\n",
      "Epoch [41/50], Train Loss: 0.1611, Validation Loss: 0.4465\n",
      "Epoch [42/50], Train Loss: 0.1616, Validation Loss: 0.4614\n",
      "Epoch [43/50], Train Loss: 0.1570, Validation Loss: 0.4543\n",
      "Epoch [44/50], Train Loss: 0.1566, Validation Loss: 0.4597\n",
      "Epoch [45/50], Train Loss: 0.1547, Validation Loss: 0.4626\n",
      "Epoch [46/50], Train Loss: 0.1525, Validation Loss: 0.4642\n",
      "Epoch [47/50], Train Loss: 0.1488, Validation Loss: 0.4606\n",
      "Epoch [48/50], Train Loss: 0.1495, Validation Loss: 0.4653\n",
      "Epoch [49/50], Train Loss: 0.1455, Validation Loss: 0.4680\n",
      "Epoch [50/50], Train Loss: 0.1431, Validation Loss: 0.4699\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-04-18 00:01:11,749]\u001b[0m Trial 6 finished with value: 0.47294067839781445 and parameters: {'hidden_size': 169, 'num_layers': 5, 'dropout_rate': 0.14460653351800662, 'learning_rate': 0.00012512605908290072, 'batch_size': 80}. Best is trial 2 with value: 0.3810829439240953.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [51/50], Train Loss: 0.1437, Validation Loss: 0.4729\n",
      "Epoch [2/50], Train Loss: 0.4570, Validation Loss: 0.3592\n",
      "Epoch [3/50], Train Loss: 0.3356, Validation Loss: 0.3533\n",
      "Epoch [4/50], Train Loss: 0.2888, Validation Loss: 0.3557\n",
      "Epoch [5/50], Train Loss: 0.2459, Validation Loss: 0.3721\n",
      "Epoch [6/50], Train Loss: 0.2048, Validation Loss: 0.3773\n",
      "Epoch [7/50], Train Loss: 0.1721, Validation Loss: 0.3886\n",
      "Epoch [8/50], Train Loss: 0.1431, Validation Loss: 0.3979\n",
      "Epoch [9/50], Train Loss: 0.1229, Validation Loss: 0.4033\n",
      "Epoch [10/50], Train Loss: 0.1086, Validation Loss: 0.4042\n",
      "Epoch [11/50], Train Loss: 0.0932, Validation Loss: 0.4020\n",
      "Epoch [12/50], Train Loss: 0.0829, Validation Loss: 0.4082\n",
      "Epoch [13/50], Train Loss: 0.0760, Validation Loss: 0.4104\n",
      "Epoch [14/50], Train Loss: 0.0664, Validation Loss: 0.4109\n",
      "Epoch [15/50], Train Loss: 0.0632, Validation Loss: 0.4170\n",
      "Epoch [16/50], Train Loss: 0.0599, Validation Loss: 0.4158\n",
      "Epoch [17/50], Train Loss: 0.0552, Validation Loss: 0.4153\n",
      "Epoch [18/50], Train Loss: 0.0521, Validation Loss: 0.4183\n",
      "Epoch [19/50], Train Loss: 0.0469, Validation Loss: 0.4224\n",
      "Epoch [20/50], Train Loss: 0.0462, Validation Loss: 0.4121\n",
      "Epoch [21/50], Train Loss: 0.0442, Validation Loss: 0.4169\n",
      "Epoch [22/50], Train Loss: 0.0419, Validation Loss: 0.4111\n",
      "Epoch [23/50], Train Loss: 0.0420, Validation Loss: 0.4123\n",
      "Epoch [24/50], Train Loss: 0.0397, Validation Loss: 0.4208\n",
      "Epoch [25/50], Train Loss: 0.0375, Validation Loss: 0.4193\n",
      "Epoch [26/50], Train Loss: 0.0379, Validation Loss: 0.4189\n",
      "Epoch [27/50], Train Loss: 0.0374, Validation Loss: 0.4208\n",
      "Epoch [28/50], Train Loss: 0.0364, Validation Loss: 0.4239\n",
      "Epoch [29/50], Train Loss: 0.0341, Validation Loss: 0.4257\n",
      "Epoch [30/50], Train Loss: 0.0348, Validation Loss: 0.4261\n",
      "Epoch [31/50], Train Loss: 0.0332, Validation Loss: 0.4224\n",
      "Epoch [32/50], Train Loss: 0.0345, Validation Loss: 0.4289\n",
      "Epoch [33/50], Train Loss: 0.0324, Validation Loss: 0.4247\n",
      "Epoch [34/50], Train Loss: 0.0312, Validation Loss: 0.4200\n",
      "Epoch [35/50], Train Loss: 0.0330, Validation Loss: 0.4300\n",
      "Epoch [36/50], Train Loss: 0.0339, Validation Loss: 0.4330\n",
      "Epoch [37/50], Train Loss: 0.0327, Validation Loss: 0.4341\n",
      "Epoch [38/50], Train Loss: 0.0330, Validation Loss: 0.4254\n",
      "Epoch [39/50], Train Loss: 0.0309, Validation Loss: 0.4294\n",
      "Epoch [40/50], Train Loss: 0.0300, Validation Loss: 0.4356\n",
      "Epoch [41/50], Train Loss: 0.0297, Validation Loss: 0.4272\n",
      "Epoch [42/50], Train Loss: 0.0289, Validation Loss: 0.4348\n",
      "Epoch [43/50], Train Loss: 0.0317, Validation Loss: 0.4197\n",
      "Epoch [44/50], Train Loss: 0.0310, Validation Loss: 0.4313\n",
      "Epoch [45/50], Train Loss: 0.0297, Validation Loss: 0.4331\n",
      "Epoch [46/50], Train Loss: 0.0300, Validation Loss: 0.4242\n",
      "Epoch [47/50], Train Loss: 0.0296, Validation Loss: 0.4242\n",
      "Epoch [48/50], Train Loss: 0.0277, Validation Loss: 0.4327\n",
      "Epoch [49/50], Train Loss: 0.0259, Validation Loss: 0.4311\n",
      "Epoch [50/50], Train Loss: 0.0266, Validation Loss: 0.4343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-04-18 00:01:34,365]\u001b[0m Trial 7 finished with value: 0.4205220192670822 and parameters: {'hidden_size': 193, 'num_layers': 3, 'dropout_rate': 0.38443388026129977, 'learning_rate': 0.001991542993319189, 'batch_size': 112}. Best is trial 2 with value: 0.3810829439240953.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [51/50], Train Loss: 0.0272, Validation Loss: 0.4205\n",
      "Epoch [2/50], Train Loss: 0.9384, Validation Loss: 0.8664\n",
      "Epoch [3/50], Train Loss: 0.7063, Validation Loss: 0.5409\n",
      "Epoch [4/50], Train Loss: 0.5057, Validation Loss: 0.4890\n",
      "Epoch [5/50], Train Loss: 0.4787, Validation Loss: 0.4739\n",
      "Epoch [6/50], Train Loss: 0.4615, Validation Loss: 0.4633\n",
      "Epoch [7/50], Train Loss: 0.4533, Validation Loss: 0.4553\n",
      "Epoch [8/50], Train Loss: 0.4451, Validation Loss: 0.4488\n",
      "Epoch [9/50], Train Loss: 0.4376, Validation Loss: 0.4421\n",
      "Epoch [10/50], Train Loss: 0.4283, Validation Loss: 0.4361\n",
      "Epoch [11/50], Train Loss: 0.4214, Validation Loss: 0.4303\n",
      "Epoch [12/50], Train Loss: 0.4158, Validation Loss: 0.4251\n",
      "Epoch [13/50], Train Loss: 0.4100, Validation Loss: 0.4199\n",
      "Epoch [14/50], Train Loss: 0.4038, Validation Loss: 0.4155\n",
      "Epoch [15/50], Train Loss: 0.3978, Validation Loss: 0.4115\n",
      "Epoch [16/50], Train Loss: 0.3928, Validation Loss: 0.4082\n",
      "Epoch [17/50], Train Loss: 0.3883, Validation Loss: 0.4045\n",
      "Epoch [18/50], Train Loss: 0.3814, Validation Loss: 0.4009\n",
      "Epoch [19/50], Train Loss: 0.3734, Validation Loss: 0.3979\n",
      "Epoch [20/50], Train Loss: 0.3696, Validation Loss: 0.3946\n",
      "Epoch [21/50], Train Loss: 0.3652, Validation Loss: 0.3911\n",
      "Epoch [22/50], Train Loss: 0.3589, Validation Loss: 0.3890\n",
      "Epoch [23/50], Train Loss: 0.3542, Validation Loss: 0.3851\n",
      "Epoch [24/50], Train Loss: 0.3476, Validation Loss: 0.3818\n",
      "Epoch [25/50], Train Loss: 0.3431, Validation Loss: 0.3787\n",
      "Epoch [26/50], Train Loss: 0.3374, Validation Loss: 0.3770\n",
      "Epoch [27/50], Train Loss: 0.3342, Validation Loss: 0.3743\n",
      "Epoch [28/50], Train Loss: 0.3290, Validation Loss: 0.3724\n",
      "Epoch [29/50], Train Loss: 0.3258, Validation Loss: 0.3702\n",
      "Epoch [30/50], Train Loss: 0.3214, Validation Loss: 0.3690\n",
      "Epoch [31/50], Train Loss: 0.3205, Validation Loss: 0.3683\n",
      "Epoch [32/50], Train Loss: 0.3163, Validation Loss: 0.3674\n",
      "Epoch [33/50], Train Loss: 0.3118, Validation Loss: 0.3659\n",
      "Epoch [34/50], Train Loss: 0.3104, Validation Loss: 0.3663\n",
      "Epoch [35/50], Train Loss: 0.3085, Validation Loss: 0.3651\n",
      "Epoch [36/50], Train Loss: 0.3055, Validation Loss: 0.3647\n",
      "Epoch [37/50], Train Loss: 0.3056, Validation Loss: 0.3638\n",
      "Epoch [38/50], Train Loss: 0.3031, Validation Loss: 0.3630\n",
      "Epoch [39/50], Train Loss: 0.3004, Validation Loss: 0.3626\n",
      "Epoch [40/50], Train Loss: 0.2982, Validation Loss: 0.3636\n",
      "Epoch [41/50], Train Loss: 0.2959, Validation Loss: 0.3627\n",
      "Epoch [42/50], Train Loss: 0.2948, Validation Loss: 0.3621\n",
      "Epoch [43/50], Train Loss: 0.2923, Validation Loss: 0.3625\n",
      "Epoch [44/50], Train Loss: 0.2919, Validation Loss: 0.3623\n",
      "Epoch [45/50], Train Loss: 0.2892, Validation Loss: 0.3625\n",
      "Epoch [46/50], Train Loss: 0.2881, Validation Loss: 0.3616\n",
      "Epoch [47/50], Train Loss: 0.2857, Validation Loss: 0.3622\n",
      "Epoch [48/50], Train Loss: 0.2850, Validation Loss: 0.3615\n",
      "Epoch [49/50], Train Loss: 0.2821, Validation Loss: 0.3617\n",
      "Epoch [50/50], Train Loss: 0.2803, Validation Loss: 0.3616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-04-18 00:02:09,615]\u001b[0m Trial 8 finished with value: 0.3616165483281726 and parameters: {'hidden_size': 247, 'num_layers': 3, 'dropout_rate': 0.23483425101345998, 'learning_rate': 1.9500472104762218e-05, 'batch_size': 58}. Best is trial 8 with value: 0.3616165483281726.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [51/50], Train Loss: 0.2799, Validation Loss: 0.3616\n",
      "Epoch [2/50], Train Loss: 0.5309, Validation Loss: 0.4228\n",
      "Epoch [3/50], Train Loss: 0.4362, Validation Loss: 0.4132\n",
      "Epoch [4/50], Train Loss: 0.4139, Validation Loss: 0.4002\n",
      "Epoch [5/50], Train Loss: 0.3782, Validation Loss: 0.3780\n",
      "Epoch [6/50], Train Loss: 0.3500, Validation Loss: 0.3793\n",
      "Epoch [7/50], Train Loss: 0.3360, Validation Loss: 0.3750\n",
      "Epoch [8/50], Train Loss: 0.3131, Validation Loss: 0.3798\n",
      "Epoch [9/50], Train Loss: 0.2953, Validation Loss: 0.3813\n",
      "Epoch [10/50], Train Loss: 0.2908, Validation Loss: 0.3908\n",
      "Epoch [11/50], Train Loss: 0.2794, Validation Loss: 0.3900\n",
      "Epoch [12/50], Train Loss: 0.2731, Validation Loss: 0.3913\n",
      "Epoch [13/50], Train Loss: 0.2623, Validation Loss: 0.4003\n",
      "Epoch [14/50], Train Loss: 0.2575, Validation Loss: 0.4014\n",
      "Epoch [15/50], Train Loss: 0.2509, Validation Loss: 0.4027\n",
      "Epoch [16/50], Train Loss: 0.2486, Validation Loss: 0.4009\n",
      "Epoch [17/50], Train Loss: 0.2450, Validation Loss: 0.3961\n",
      "Epoch [18/50], Train Loss: 0.2371, Validation Loss: 0.4054\n",
      "Epoch [19/50], Train Loss: 0.2365, Validation Loss: 0.3978\n",
      "Epoch [20/50], Train Loss: 0.2273, Validation Loss: 0.4006\n",
      "Epoch [21/50], Train Loss: 0.2239, Validation Loss: 0.4025\n",
      "Epoch [22/50], Train Loss: 0.2257, Validation Loss: 0.4087\n",
      "Epoch [23/50], Train Loss: 0.2158, Validation Loss: 0.4184\n",
      "Epoch [24/50], Train Loss: 0.2161, Validation Loss: 0.4011\n",
      "Epoch [25/50], Train Loss: 0.2128, Validation Loss: 0.4157\n",
      "Epoch [26/50], Train Loss: 0.2106, Validation Loss: 0.4133\n",
      "Epoch [27/50], Train Loss: 0.2054, Validation Loss: 0.4103\n",
      "Epoch [28/50], Train Loss: 0.2042, Validation Loss: 0.4133\n",
      "Epoch [29/50], Train Loss: 0.2058, Validation Loss: 0.4115\n",
      "Epoch [30/50], Train Loss: 0.2013, Validation Loss: 0.4142\n",
      "Epoch [31/50], Train Loss: 0.1985, Validation Loss: 0.4134\n",
      "Epoch [32/50], Train Loss: 0.1951, Validation Loss: 0.4075\n",
      "Epoch [33/50], Train Loss: 0.1901, Validation Loss: 0.4207\n",
      "Epoch [34/50], Train Loss: 0.1916, Validation Loss: 0.4169\n",
      "Epoch [35/50], Train Loss: 0.1854, Validation Loss: 0.4112\n",
      "Epoch [36/50], Train Loss: 0.1863, Validation Loss: 0.4133\n",
      "Epoch [37/50], Train Loss: 0.1856, Validation Loss: 0.4120\n",
      "Epoch [38/50], Train Loss: 0.1855, Validation Loss: 0.4093\n",
      "Epoch [39/50], Train Loss: 0.1827, Validation Loss: 0.4194\n",
      "Epoch [40/50], Train Loss: 0.1832, Validation Loss: 0.4224\n",
      "Epoch [41/50], Train Loss: 0.1791, Validation Loss: 0.4291\n",
      "Epoch [42/50], Train Loss: 0.1792, Validation Loss: 0.4162\n",
      "Epoch [43/50], Train Loss: 0.1757, Validation Loss: 0.4219\n",
      "Epoch [44/50], Train Loss: 0.1751, Validation Loss: 0.4197\n",
      "Epoch [45/50], Train Loss: 0.1758, Validation Loss: 0.4167\n",
      "Epoch [46/50], Train Loss: 0.1708, Validation Loss: 0.4255\n",
      "Epoch [47/50], Train Loss: 0.1729, Validation Loss: 0.4211\n",
      "Epoch [48/50], Train Loss: 0.1721, Validation Loss: 0.4225\n",
      "Epoch [49/50], Train Loss: 0.1715, Validation Loss: 0.4314\n",
      "Epoch [50/50], Train Loss: 0.1678, Validation Loss: 0.4293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-04-18 00:03:57,764]\u001b[0m Trial 9 finished with value: 0.42760222578701906 and parameters: {'hidden_size': 34, 'num_layers': 5, 'dropout_rate': 0.37160933535776874, 'learning_rate': 0.0006973503455462049, 'batch_size': 33}. Best is trial 8 with value: 0.3616165483281726.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [51/50], Train Loss: 0.1716, Validation Loss: 0.4276\n",
      "Epoch [2/50], Train Loss: 0.8291, Validation Loss: 0.5335\n",
      "Epoch [3/50], Train Loss: 0.4981, Validation Loss: 0.4650\n",
      "Epoch [4/50], Train Loss: 0.4658, Validation Loss: 0.4487\n",
      "Epoch [5/50], Train Loss: 0.4515, Validation Loss: 0.4374\n",
      "Epoch [6/50], Train Loss: 0.4392, Validation Loss: 0.4284\n",
      "Epoch [7/50], Train Loss: 0.4295, Validation Loss: 0.4222\n",
      "Epoch [8/50], Train Loss: 0.4199, Validation Loss: 0.4157\n",
      "Epoch [9/50], Train Loss: 0.4105, Validation Loss: 0.4102\n",
      "Epoch [10/50], Train Loss: 0.4034, Validation Loss: 0.4050\n",
      "Epoch [11/50], Train Loss: 0.3958, Validation Loss: 0.3996\n",
      "Epoch [12/50], Train Loss: 0.3845, Validation Loss: 0.3929\n",
      "Epoch [13/50], Train Loss: 0.3770, Validation Loss: 0.3859\n",
      "Epoch [14/50], Train Loss: 0.3649, Validation Loss: 0.3797\n",
      "Epoch [15/50], Train Loss: 0.3545, Validation Loss: 0.3734\n",
      "Epoch [16/50], Train Loss: 0.3476, Validation Loss: 0.3691\n",
      "Epoch [17/50], Train Loss: 0.3393, Validation Loss: 0.3662\n",
      "Epoch [18/50], Train Loss: 0.3349, Validation Loss: 0.3645\n",
      "Epoch [19/50], Train Loss: 0.3309, Validation Loss: 0.3629\n",
      "Epoch [20/50], Train Loss: 0.3274, Validation Loss: 0.3619\n",
      "Epoch [21/50], Train Loss: 0.3221, Validation Loss: 0.3606\n",
      "Epoch [22/50], Train Loss: 0.3170, Validation Loss: 0.3592\n",
      "Epoch [23/50], Train Loss: 0.3186, Validation Loss: 0.3590\n",
      "Epoch [24/50], Train Loss: 0.3153, Validation Loss: 0.3591\n",
      "Epoch [25/50], Train Loss: 0.3123, Validation Loss: 0.3587\n",
      "Epoch [26/50], Train Loss: 0.3080, Validation Loss: 0.3589\n",
      "Epoch [27/50], Train Loss: 0.3045, Validation Loss: 0.3581\n",
      "Epoch [28/50], Train Loss: 0.3021, Validation Loss: 0.3575\n",
      "Epoch [29/50], Train Loss: 0.2994, Validation Loss: 0.3582\n",
      "Epoch [30/50], Train Loss: 0.2967, Validation Loss: 0.3581\n",
      "Epoch [31/50], Train Loss: 0.2933, Validation Loss: 0.3590\n",
      "Epoch [32/50], Train Loss: 0.2929, Validation Loss: 0.3582\n",
      "Epoch [33/50], Train Loss: 0.2890, Validation Loss: 0.3589\n",
      "Epoch [34/50], Train Loss: 0.2885, Validation Loss: 0.3584\n",
      "Epoch [35/50], Train Loss: 0.2831, Validation Loss: 0.3584\n",
      "Epoch [36/50], Train Loss: 0.2823, Validation Loss: 0.3596\n",
      "Epoch [37/50], Train Loss: 0.2824, Validation Loss: 0.3588\n",
      "Epoch [38/50], Train Loss: 0.2777, Validation Loss: 0.3600\n",
      "Epoch [39/50], Train Loss: 0.2729, Validation Loss: 0.3596\n",
      "Epoch [40/50], Train Loss: 0.2737, Validation Loss: 0.3605\n",
      "Epoch [41/50], Train Loss: 0.2689, Validation Loss: 0.3611\n",
      "Epoch [42/50], Train Loss: 0.2672, Validation Loss: 0.3608\n",
      "Epoch [43/50], Train Loss: 0.2657, Validation Loss: 0.3631\n",
      "Epoch [44/50], Train Loss: 0.2596, Validation Loss: 0.3627\n",
      "Epoch [45/50], Train Loss: 0.2601, Validation Loss: 0.3634\n",
      "Epoch [46/50], Train Loss: 0.2568, Validation Loss: 0.3635\n",
      "Epoch [47/50], Train Loss: 0.2545, Validation Loss: 0.3645\n",
      "Epoch [48/50], Train Loss: 0.2517, Validation Loss: 0.3665\n",
      "Epoch [49/50], Train Loss: 0.2488, Validation Loss: 0.3659\n",
      "Epoch [50/50], Train Loss: 0.2471, Validation Loss: 0.3675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-04-18 00:04:36,751]\u001b[0m Trial 10 finished with value: 0.3697468807299932 and parameters: {'hidden_size': 468, 'num_layers': 3, 'dropout_rate': 0.6624623622842303, 'learning_rate': 4.770022464285832e-05, 'batch_size': 54}. Best is trial 8 with value: 0.3616165483281726.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [51/50], Train Loss: 0.2427, Validation Loss: 0.3697\n",
      "Epoch [2/50], Train Loss: 0.9297, Validation Loss: 0.7299\n",
      "Epoch [3/50], Train Loss: 0.5558, Validation Loss: 0.4842\n",
      "Epoch [4/50], Train Loss: 0.4891, Validation Loss: 0.4607\n",
      "Epoch [5/50], Train Loss: 0.4690, Validation Loss: 0.4477\n",
      "Epoch [6/50], Train Loss: 0.4603, Validation Loss: 0.4381\n",
      "Epoch [7/50], Train Loss: 0.4465, Validation Loss: 0.4311\n",
      "Epoch [8/50], Train Loss: 0.4370, Validation Loss: 0.4249\n",
      "Epoch [9/50], Train Loss: 0.4308, Validation Loss: 0.4203\n",
      "Epoch [10/50], Train Loss: 0.4231, Validation Loss: 0.4163\n",
      "Epoch [11/50], Train Loss: 0.4180, Validation Loss: 0.4129\n",
      "Epoch [12/50], Train Loss: 0.4117, Validation Loss: 0.4099\n",
      "Epoch [13/50], Train Loss: 0.4110, Validation Loss: 0.4075\n",
      "Epoch [14/50], Train Loss: 0.4001, Validation Loss: 0.4046\n",
      "Epoch [15/50], Train Loss: 0.3991, Validation Loss: 0.4016\n",
      "Epoch [16/50], Train Loss: 0.3921, Validation Loss: 0.3981\n",
      "Epoch [17/50], Train Loss: 0.3900, Validation Loss: 0.3952\n",
      "Epoch [18/50], Train Loss: 0.3815, Validation Loss: 0.3917\n",
      "Epoch [19/50], Train Loss: 0.3780, Validation Loss: 0.3868\n",
      "Epoch [20/50], Train Loss: 0.3719, Validation Loss: 0.3822\n",
      "Epoch [21/50], Train Loss: 0.3659, Validation Loss: 0.3784\n",
      "Epoch [22/50], Train Loss: 0.3591, Validation Loss: 0.3745\n",
      "Epoch [23/50], Train Loss: 0.3527, Validation Loss: 0.3707\n",
      "Epoch [24/50], Train Loss: 0.3464, Validation Loss: 0.3684\n",
      "Epoch [25/50], Train Loss: 0.3428, Validation Loss: 0.3654\n",
      "Epoch [26/50], Train Loss: 0.3371, Validation Loss: 0.3649\n",
      "Epoch [27/50], Train Loss: 0.3358, Validation Loss: 0.3635\n",
      "Epoch [28/50], Train Loss: 0.3329, Validation Loss: 0.3623\n",
      "Epoch [29/50], Train Loss: 0.3274, Validation Loss: 0.3614\n",
      "Epoch [30/50], Train Loss: 0.3266, Validation Loss: 0.3617\n",
      "Epoch [31/50], Train Loss: 0.3217, Validation Loss: 0.3606\n",
      "Epoch [32/50], Train Loss: 0.3228, Validation Loss: 0.3603\n",
      "Epoch [33/50], Train Loss: 0.3178, Validation Loss: 0.3607\n",
      "Epoch [34/50], Train Loss: 0.3141, Validation Loss: 0.3600\n",
      "Epoch [35/50], Train Loss: 0.3144, Validation Loss: 0.3603\n",
      "Epoch [36/50], Train Loss: 0.3116, Validation Loss: 0.3593\n",
      "Epoch [37/50], Train Loss: 0.3087, Validation Loss: 0.3596\n",
      "Epoch [38/50], Train Loss: 0.3071, Validation Loss: 0.3593\n",
      "Epoch [39/50], Train Loss: 0.3038, Validation Loss: 0.3598\n",
      "Epoch [40/50], Train Loss: 0.3029, Validation Loss: 0.3602\n",
      "Epoch [41/50], Train Loss: 0.3029, Validation Loss: 0.3599\n",
      "Epoch [42/50], Train Loss: 0.2995, Validation Loss: 0.3603\n",
      "Epoch [43/50], Train Loss: 0.2985, Validation Loss: 0.3605\n",
      "Epoch [44/50], Train Loss: 0.2962, Validation Loss: 0.3599\n",
      "Epoch [45/50], Train Loss: 0.2913, Validation Loss: 0.3607\n",
      "Epoch [46/50], Train Loss: 0.2911, Validation Loss: 0.3604\n",
      "Epoch [47/50], Train Loss: 0.2884, Validation Loss: 0.3608\n",
      "Epoch [48/50], Train Loss: 0.2849, Validation Loss: 0.3615\n",
      "Epoch [49/50], Train Loss: 0.2879, Validation Loss: 0.3621\n",
      "Epoch [50/50], Train Loss: 0.2873, Validation Loss: 0.3629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-04-18 00:05:11,982]\u001b[0m Trial 11 finished with value: 0.3623266306727432 and parameters: {'hidden_size': 479, 'num_layers': 3, 'dropout_rate': 0.7079701185167607, 'learning_rate': 4.00050004590533e-05, 'batch_size': 57}. Best is trial 8 with value: 0.3616165483281726.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [51/50], Train Loss: 0.2810, Validation Loss: 0.3623\n",
      "Epoch [2/50], Train Loss: 0.5196, Validation Loss: 0.3932\n",
      "Epoch [3/50], Train Loss: 0.4414, Validation Loss: 0.3864\n",
      "Epoch [4/50], Train Loss: 0.4147, Validation Loss: 0.3944\n",
      "Epoch [5/50], Train Loss: 0.4080, Validation Loss: 0.3829\n",
      "Epoch [6/50], Train Loss: 0.3970, Validation Loss: 0.3989\n",
      "Epoch [7/50], Train Loss: 0.3960, Validation Loss: 0.4010\n",
      "Epoch [8/50], Train Loss: 0.3914, Validation Loss: 0.4085\n",
      "Epoch [9/50], Train Loss: 0.3960, Validation Loss: 0.4079\n",
      "Epoch [10/50], Train Loss: 0.3940, Validation Loss: 0.4038\n",
      "Epoch [11/50], Train Loss: 0.3965, Validation Loss: 0.4142\n",
      "Epoch [12/50], Train Loss: 0.3992, Validation Loss: 0.4141\n",
      "Epoch [13/50], Train Loss: 0.3934, Validation Loss: 0.4053\n",
      "Epoch [14/50], Train Loss: 0.3951, Validation Loss: 0.4089\n",
      "Epoch [15/50], Train Loss: 0.3884, Validation Loss: 0.4078\n",
      "Epoch [16/50], Train Loss: 0.3891, Validation Loss: 0.4052\n",
      "Epoch [17/50], Train Loss: 0.3979, Validation Loss: 0.4143\n",
      "Epoch [18/50], Train Loss: 0.3914, Validation Loss: 0.4051\n",
      "Epoch [19/50], Train Loss: 0.3938, Validation Loss: 0.4049\n",
      "Epoch [20/50], Train Loss: 0.3946, Validation Loss: 0.4002\n",
      "Epoch [21/50], Train Loss: 0.3936, Validation Loss: 0.4099\n",
      "Epoch [22/50], Train Loss: 0.3901, Validation Loss: 0.4097\n",
      "Epoch [23/50], Train Loss: 0.3925, Validation Loss: 0.4119\n",
      "Epoch [24/50], Train Loss: 0.3929, Validation Loss: 0.4092\n",
      "Epoch [25/50], Train Loss: 0.3876, Validation Loss: 0.4047\n",
      "Epoch [26/50], Train Loss: 0.3833, Validation Loss: 0.4093\n",
      "Epoch [27/50], Train Loss: 0.3872, Validation Loss: 0.4057\n",
      "Epoch [28/50], Train Loss: 0.3837, Validation Loss: 0.4044\n",
      "Epoch [29/50], Train Loss: 0.3824, Validation Loss: 0.4042\n",
      "Epoch [30/50], Train Loss: 0.3823, Validation Loss: 0.4036\n",
      "Epoch [31/50], Train Loss: 0.3777, Validation Loss: 0.4048\n",
      "Epoch [32/50], Train Loss: 0.3803, Validation Loss: 0.4018\n",
      "Epoch [33/50], Train Loss: 0.3753, Validation Loss: 0.4056\n",
      "Epoch [34/50], Train Loss: 0.3759, Validation Loss: 0.4057\n",
      "Epoch [35/50], Train Loss: 0.3821, Validation Loss: 0.4083\n",
      "Epoch [36/50], Train Loss: 0.3823, Validation Loss: 0.4085\n",
      "Epoch [37/50], Train Loss: 0.3789, Validation Loss: 0.3995\n",
      "Epoch [38/50], Train Loss: 0.3747, Validation Loss: 0.4066\n",
      "Epoch [39/50], Train Loss: 0.3849, Validation Loss: 0.3931\n",
      "Epoch [40/50], Train Loss: 0.3778, Validation Loss: 0.4008\n",
      "Epoch [41/50], Train Loss: 0.3757, Validation Loss: 0.3980\n",
      "Epoch [42/50], Train Loss: 0.3664, Validation Loss: 0.3985\n",
      "Epoch [43/50], Train Loss: 0.3779, Validation Loss: 0.3971\n",
      "Epoch [44/50], Train Loss: 0.3707, Validation Loss: 0.4025\n",
      "Epoch [45/50], Train Loss: 0.3749, Validation Loss: 0.3906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m[W 2023-04-18 00:05:38,020]\u001b[0m Trial 12 failed with parameters: {'hidden_size': 505, 'num_layers': 2, 'dropout_rate': 0.7999060329557599, 'learning_rate': 0.005926707695993871, 'batch_size': 64} because of the following error: KeyboardInterrupt().\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Miniconda\\envs\\ails\\lib\\site-packages\\optuna\\study\\_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"C:\\Users\\agneg\\AppData\\Local\\Temp\\ipykernel_9176\\2231237009.py\", line 37, in objective\n",
      "    outputs = model(inputs)\n",
      "  File \"d:\\Miniconda\\envs\\ails\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1130, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"C:\\Users\\agneg\\AppData\\Local\\Temp\\ipykernel_9176\\3529936515.py\", line 14, in forward\n",
      "    h0 = torch.zeros(self.num_layers, x.size(\n",
      "KeyboardInterrupt\n",
      "\u001b[33m[W 2023-04-18 00:05:38,021]\u001b[0m Trial 12 failed with value None.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [46/50], Train Loss: 0.3725, Validation Loss: 0.3942\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[69], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m study \u001b[39m=\u001b[39m optuna\u001b[39m.\u001b[39mcreate_study(direction\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mminimize\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      2\u001b[0m \u001b[39m# You can adjust the number of trials depending on your computational resources\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m study\u001b[39m.\u001b[39;49moptimize(objective, n_trials\u001b[39m=\u001b[39;49m\u001b[39m50\u001b[39;49m)\n",
      "File \u001b[1;32md:\\Miniconda\\envs\\ails\\lib\\site-packages\\optuna\\study\\study.py:425\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    321\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39moptimize\u001b[39m(\n\u001b[0;32m    322\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    323\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    330\u001b[0m     show_progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    331\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    332\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    333\u001b[0m \n\u001b[0;32m    334\u001b[0m \u001b[39m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    422\u001b[0m \u001b[39m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    423\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 425\u001b[0m     _optimize(\n\u001b[0;32m    426\u001b[0m         study\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[0;32m    427\u001b[0m         func\u001b[39m=\u001b[39;49mfunc,\n\u001b[0;32m    428\u001b[0m         n_trials\u001b[39m=\u001b[39;49mn_trials,\n\u001b[0;32m    429\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[0;32m    430\u001b[0m         n_jobs\u001b[39m=\u001b[39;49mn_jobs,\n\u001b[0;32m    431\u001b[0m         catch\u001b[39m=\u001b[39;49m\u001b[39mtuple\u001b[39;49m(catch) \u001b[39mif\u001b[39;49;00m \u001b[39misinstance\u001b[39;49m(catch, Iterable) \u001b[39melse\u001b[39;49;00m (catch,),\n\u001b[0;32m    432\u001b[0m         callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[0;32m    433\u001b[0m         gc_after_trial\u001b[39m=\u001b[39;49mgc_after_trial,\n\u001b[0;32m    434\u001b[0m         show_progress_bar\u001b[39m=\u001b[39;49mshow_progress_bar,\n\u001b[0;32m    435\u001b[0m     )\n",
      "File \u001b[1;32md:\\Miniconda\\envs\\ails\\lib\\site-packages\\optuna\\study\\_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     65\u001b[0m     \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m---> 66\u001b[0m         _optimize_sequential(\n\u001b[0;32m     67\u001b[0m             study,\n\u001b[0;32m     68\u001b[0m             func,\n\u001b[0;32m     69\u001b[0m             n_trials,\n\u001b[0;32m     70\u001b[0m             timeout,\n\u001b[0;32m     71\u001b[0m             catch,\n\u001b[0;32m     72\u001b[0m             callbacks,\n\u001b[0;32m     73\u001b[0m             gc_after_trial,\n\u001b[0;32m     74\u001b[0m             reseed_sampler_rng\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m     75\u001b[0m             time_start\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m     76\u001b[0m             progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[0;32m     77\u001b[0m         )\n\u001b[0;32m     78\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     79\u001b[0m         \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:\n",
      "File \u001b[1;32md:\\Miniconda\\envs\\ails\\lib\\site-packages\\optuna\\study\\_optimize.py:163\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    160\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 163\u001b[0m     frozen_trial \u001b[39m=\u001b[39m _run_trial(study, func, catch)\n\u001b[0;32m    164\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    165\u001b[0m     \u001b[39m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    166\u001b[0m     \u001b[39m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[0;32m    167\u001b[0m     \u001b[39m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    168\u001b[0m     \u001b[39m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    169\u001b[0m     \u001b[39mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[1;32md:\\Miniconda\\envs\\ails\\lib\\site-packages\\optuna\\study\\_optimize.py:251\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    244\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mFalse\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mShould not reach.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    246\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m    247\u001b[0m     frozen_trial\u001b[39m.\u001b[39mstate \u001b[39m==\u001b[39m TrialState\u001b[39m.\u001b[39mFAIL\n\u001b[0;32m    248\u001b[0m     \u001b[39mand\u001b[39;00m func_err \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    249\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    250\u001b[0m ):\n\u001b[1;32m--> 251\u001b[0m     \u001b[39mraise\u001b[39;00m func_err\n\u001b[0;32m    252\u001b[0m \u001b[39mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[1;32md:\\Miniconda\\envs\\ails\\lib\\site-packages\\optuna\\study\\_optimize.py:200\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[39mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[39m.\u001b[39m_trial_id, study\u001b[39m.\u001b[39m_storage):\n\u001b[0;32m    199\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 200\u001b[0m         value_or_values \u001b[39m=\u001b[39m func(trial)\n\u001b[0;32m    201\u001b[0m     \u001b[39mexcept\u001b[39;00m exceptions\u001b[39m.\u001b[39mTrialPruned \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    202\u001b[0m         \u001b[39m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    203\u001b[0m         state \u001b[39m=\u001b[39m TrialState\u001b[39m.\u001b[39mPRUNED\n",
      "Cell \u001b[1;32mIn[68], line 37\u001b[0m, in \u001b[0;36mobjective\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m     34\u001b[0m labels \u001b[39m=\u001b[39m labels\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     36\u001b[0m \u001b[39m# Forward pass\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m outputs \u001b[39m=\u001b[39m model(inputs)\n\u001b[0;32m     39\u001b[0m mask \u001b[39m=\u001b[39m labels \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m     40\u001b[0m masked_outputs \u001b[39m=\u001b[39m outputs[mask]\n",
      "File \u001b[1;32md:\\Miniconda\\envs\\ails\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[65], line 14\u001b[0m, in \u001b[0;36mSimpleGRU.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m---> 14\u001b[0m     h0 \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mzeros(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_layers, x\u001b[39m.\u001b[39;49msize(\n\u001b[0;32m     15\u001b[0m         \u001b[39m0\u001b[39;49m), \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhidden_size)\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m     16\u001b[0m     out, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgru(x, h0)\n\u001b[0;32m     17\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(out[:, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, :])\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction=\"minimize\")\n",
    "# You can adjust the number of trials depending on your computational resources\n",
    "study.optimize(objective, n_trials=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'hidden_size': 433, 'num_layers': 2, 'dropout_rate': 0.5442089330486015, 'learning_rate': 0.006091997080097445, 'weight_decay': 1.4045570090893667e-05}\n"
     ]
    }
   ],
   "source": [
    "best_params = study.best_params\n",
    "print(\"Best hyperparameters:\", best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hidden_size = 479  # 433\n",
    "best_num_layers = 3 #2\n",
    "best_dropout_rate = 0.7079701185167607  # 0.5442089330486015\n",
    "best_learning_rate = 4.00050004590533e-05  # 0.006091997080097445\n",
    "#best_weight_decay = #1.4045570090893667e-05\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleGRU(input_size, best_hidden_size,\n",
    "                  best_num_layers, best_dropout_rate, device).to(device)\n",
    "\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(), lr=best_learning_rate) #, weight_decay=best_weight_decay)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "patience = 30\n",
    "min_delta = 0.001\n",
    "num_epochs = 100\n",
    "\n",
    "best_validation_loss = float(\"inf\")\n",
    "counter = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/100], Train Loss: 0.9229, Validation Loss: 0.7500\n",
      "Epoch [3/100], Train Loss: 0.5646, Validation Loss: 0.4794\n",
      "Epoch [4/100], Train Loss: 0.4857, Validation Loss: 0.4577\n",
      "Epoch [5/100], Train Loss: 0.4697, Validation Loss: 0.4462\n",
      "Epoch [6/100], Train Loss: 0.4564, Validation Loss: 0.4379\n",
      "Epoch [7/100], Train Loss: 0.4501, Validation Loss: 0.4305\n",
      "Epoch [8/100], Train Loss: 0.4371, Validation Loss: 0.4245\n",
      "Epoch [9/100], Train Loss: 0.4323, Validation Loss: 0.4196\n",
      "Epoch [10/100], Train Loss: 0.4261, Validation Loss: 0.4153\n",
      "Epoch [11/100], Train Loss: 0.4179, Validation Loss: 0.4118\n",
      "Epoch [12/100], Train Loss: 0.4108, Validation Loss: 0.4083\n",
      "Epoch [13/100], Train Loss: 0.4085, Validation Loss: 0.4059\n",
      "Epoch [14/100], Train Loss: 0.4043, Validation Loss: 0.4025\n",
      "Epoch [15/100], Train Loss: 0.3979, Validation Loss: 0.3999\n",
      "Epoch [16/100], Train Loss: 0.3914, Validation Loss: 0.3959\n",
      "Epoch [17/100], Train Loss: 0.3845, Validation Loss: 0.3917\n",
      "Epoch [18/100], Train Loss: 0.3814, Validation Loss: 0.3879\n",
      "Epoch [19/100], Train Loss: 0.3745, Validation Loss: 0.3832\n",
      "Epoch [20/100], Train Loss: 0.3641, Validation Loss: 0.3783\n",
      "Epoch [21/100], Train Loss: 0.3600, Validation Loss: 0.3752\n",
      "Epoch [22/100], Train Loss: 0.3542, Validation Loss: 0.3717\n",
      "Epoch [23/100], Train Loss: 0.3495, Validation Loss: 0.3697\n",
      "Epoch [24/100], Train Loss: 0.3456, Validation Loss: 0.3675\n",
      "Epoch [25/100], Train Loss: 0.3426, Validation Loss: 0.3671\n",
      "Epoch [26/100], Train Loss: 0.3362, Validation Loss: 0.3655\n",
      "Epoch [27/100], Train Loss: 0.3321, Validation Loss: 0.3644\n",
      "Epoch [28/100], Train Loss: 0.3294, Validation Loss: 0.3637\n",
      "Epoch [29/100], Train Loss: 0.3286, Validation Loss: 0.3636\n",
      "Epoch [30/100], Train Loss: 0.3265, Validation Loss: 0.3624\n",
      "Epoch [31/100], Train Loss: 0.3231, Validation Loss: 0.3626\n",
      "Epoch [32/100], Train Loss: 0.3215, Validation Loss: 0.3624\n",
      "Epoch [33/100], Train Loss: 0.3198, Validation Loss: 0.3617\n",
      "Epoch [34/100], Train Loss: 0.3161, Validation Loss: 0.3611\n",
      "Epoch [35/100], Train Loss: 0.3140, Validation Loss: 0.3619\n",
      "Epoch [36/100], Train Loss: 0.3114, Validation Loss: 0.3622\n",
      "Epoch [37/100], Train Loss: 0.3103, Validation Loss: 0.3613\n",
      "Epoch [38/100], Train Loss: 0.3120, Validation Loss: 0.3608\n",
      "Epoch [39/100], Train Loss: 0.3078, Validation Loss: 0.3614\n",
      "Epoch [40/100], Train Loss: 0.3025, Validation Loss: 0.3610\n",
      "Epoch [41/100], Train Loss: 0.3017, Validation Loss: 0.3612\n",
      "Epoch [42/100], Train Loss: 0.3018, Validation Loss: 0.3618\n",
      "Epoch [43/100], Train Loss: 0.2978, Validation Loss: 0.3618\n",
      "Epoch [44/100], Train Loss: 0.2995, Validation Loss: 0.3610\n",
      "Epoch [45/100], Train Loss: 0.2983, Validation Loss: 0.3606\n",
      "Epoch [46/100], Train Loss: 0.2949, Validation Loss: 0.3611\n",
      "Epoch [47/100], Train Loss: 0.2908, Validation Loss: 0.3614\n",
      "Epoch [48/100], Train Loss: 0.2873, Validation Loss: 0.3610\n",
      "Epoch [49/100], Train Loss: 0.2874, Validation Loss: 0.3622\n",
      "Epoch [50/100], Train Loss: 0.2851, Validation Loss: 0.3620\n",
      "Epoch [51/100], Train Loss: 0.2836, Validation Loss: 0.3625\n",
      "Epoch [52/100], Train Loss: 0.2847, Validation Loss: 0.3629\n",
      "Epoch [53/100], Train Loss: 0.2816, Validation Loss: 0.3625\n",
      "Epoch [54/100], Train Loss: 0.2783, Validation Loss: 0.3631\n",
      "Epoch [55/100], Train Loss: 0.2751, Validation Loss: 0.3631\n",
      "Epoch [56/100], Train Loss: 0.2722, Validation Loss: 0.3636\n",
      "Epoch [57/100], Train Loss: 0.2732, Validation Loss: 0.3647\n",
      "Epoch [58/100], Train Loss: 0.2677, Validation Loss: 0.3644\n",
      "Epoch [59/100], Train Loss: 0.2676, Validation Loss: 0.3645\n",
      "Epoch [60/100], Train Loss: 0.2654, Validation Loss: 0.3659\n",
      "Epoch [61/100], Train Loss: 0.2660, Validation Loss: 0.3663\n",
      "Epoch [62/100], Train Loss: 0.2630, Validation Loss: 0.3670\n",
      "Epoch [63/100], Train Loss: 0.2600, Validation Loss: 0.3665\n",
      "Epoch [64/100], Train Loss: 0.2600, Validation Loss: 0.3677\n",
      "Early stopping at epoch 63\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, num_epochs+1):\n",
    "     # Training\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    for i, (fps, labels) in enumerate(train_loader):\n",
    "            inputs = fps.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            mask = labels != 0\n",
    "            masked_outputs = outputs[mask]\n",
    "            masked_labels = labels[mask]\n",
    "            # loss = criterion(outputs, labels)\n",
    "            loss = criterion(masked_outputs, masked_labels)\n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    train_loss = np.mean(train_losses)\n",
    "\n",
    "        # Validation\n",
    "    model.eval()\n",
    "    val_losses = []\n",
    "    with torch.no_grad():\n",
    "            for fps, labels in val_loader:\n",
    "                inputs = fps.to(device)\n",
    "                labels = labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                mask = labels != 0\n",
    "                masked_outputs = outputs[mask]\n",
    "                masked_labels = labels[mask]\n",
    "                loss = criterion(masked_outputs, masked_labels)\n",
    "                val_losses.append(loss.item())\n",
    "\n",
    "    val_loss = np.mean(val_losses)\n",
    "\n",
    "    print(\n",
    "            f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "    if val_loss < best_validation_loss - min_delta:\n",
    "        best_validation_loss = val_loss\n",
    "        counter = 0\n",
    "    else:\n",
    "        counter += 1\n",
    "        if counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch}\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Train Loss: 0.1514, Validation Loss: 0.1304\n",
      "Epoch [2/100], Train Loss: 0.1539, Validation Loss: 0.1314\n",
      "Epoch [3/100], Train Loss: 0.1583, Validation Loss: 0.1339\n",
      "Epoch [4/100], Train Loss: 0.1539, Validation Loss: 0.1319\n",
      "Epoch [5/100], Train Loss: 0.1574, Validation Loss: 0.1320\n",
      "Epoch [6/100], Train Loss: 0.1566, Validation Loss: 0.1322\n",
      "Epoch [7/100], Train Loss: 0.1587, Validation Loss: 0.1335\n",
      "Epoch [8/100], Train Loss: 0.1513, Validation Loss: 0.1340\n",
      "Epoch [9/100], Train Loss: 0.1565, Validation Loss: 0.1340\n",
      "Epoch [10/100], Train Loss: 0.1564, Validation Loss: 0.1351\n",
      "Epoch [11/100], Train Loss: 0.1583, Validation Loss: 0.1354\n",
      "Epoch [12/100], Train Loss: 0.1604, Validation Loss: 0.1362\n",
      "Epoch [13/100], Train Loss: 0.1623, Validation Loss: 0.1388\n",
      "Epoch [14/100], Train Loss: 0.1613, Validation Loss: 0.1372\n",
      "Epoch [15/100], Train Loss: 0.1657, Validation Loss: 0.1376\n",
      "Epoch [16/100], Train Loss: 0.1613, Validation Loss: 0.1366\n",
      "Epoch [17/100], Train Loss: 0.1600, Validation Loss: 0.1386\n",
      "Epoch [18/100], Train Loss: 0.1643, Validation Loss: 0.1393\n",
      "Epoch [19/100], Train Loss: 0.1621, Validation Loss: 0.1391\n",
      "Epoch [20/100], Train Loss: 0.1626, Validation Loss: 0.1378\n",
      "Epoch [21/100], Train Loss: 0.1643, Validation Loss: 0.1389\n",
      "Epoch [22/100], Train Loss: 0.1592, Validation Loss: 0.1376\n",
      "Epoch [23/100], Train Loss: 0.1646, Validation Loss: 0.1409\n",
      "Epoch [24/100], Train Loss: 0.1615, Validation Loss: 0.1413\n",
      "Epoch [25/100], Train Loss: 0.1630, Validation Loss: 0.1420\n",
      "Epoch [26/100], Train Loss: 0.1585, Validation Loss: 0.1377\n",
      "Epoch [27/100], Train Loss: 0.1650, Validation Loss: 0.1405\n",
      "Epoch [28/100], Train Loss: 0.1592, Validation Loss: 0.1388\n",
      "Epoch [29/100], Train Loss: 0.1609, Validation Loss: 0.1399\n",
      "Epoch [30/100], Train Loss: 0.1594, Validation Loss: 0.1403\n",
      "Epoch [31/100], Train Loss: 0.1663, Validation Loss: 0.1433\n",
      "Early stopping at epoch 30\n"
     ]
    }
   ],
   "source": [
    "patience = 30\n",
    "min_delta = 0.001\n",
    "num_epochs = 100 \n",
    "\n",
    "best_validation_loss = float(\"inf\")\n",
    "counter = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training loop\n",
    "    model.train()\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        valid_loss = 0\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            valid_loss += loss.item()\n",
    "\n",
    "        valid_loss /= len(val_loader)\n",
    "\n",
    "    print(\n",
    "        f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {loss.item():.4f}, Validation Loss: {valid_loss:.4f}\")\n",
    "    \n",
    "    if valid_loss < best_validation_loss - min_delta:\n",
    "        best_validation_loss = valid_loss\n",
    "        counter = 0\n",
    "    else:\n",
    "        counter += 1\n",
    "        if counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch}\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>smiles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>OC(COc1ccc(Cl)cc1)=N[C@H]1CC[C@H](N=C(O)COc2cc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CCCO/N=C(/C)c1cc(C(O)=NC(Cc2cc(F)cc(F)c2)[C@@H...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>COc1cc(Cl)ccc1Cl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>COc1cc(C(O)=NCc2ccc(OCCN(C)C)cc2)cc(OC)c1OC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CCC(=O)O[C@@]1(C(=O)CCl)[C@@H](C)C[C@H]2[C@@H]...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5891</th>\n",
       "      <td>N#Cc1cc(NC(=O)C(=O)O)c(Cl)c(NC(=O)C(=O)O)c1.NC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5892</th>\n",
       "      <td>O=c1cccc2n1C[C@@H]1CNC[C@H]2C1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5893</th>\n",
       "      <td>CSCC[C@H](N=C(O)[C@H](Cc1ccccc1)N=C(O)CN=C(O)C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5894</th>\n",
       "      <td>CCn1cc2c3c(cc(C(O)=NC(Cc4ccccc4)[C@H](O)C[NH2+...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5895</th>\n",
       "      <td>C[C@H]1CCCN(S(C)(=O)=O)[C@@H]1CO</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5896 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 smiles\n",
       "0     OC(COc1ccc(Cl)cc1)=N[C@H]1CC[C@H](N=C(O)COc2cc...\n",
       "1     CCCO/N=C(/C)c1cc(C(O)=NC(Cc2cc(F)cc(F)c2)[C@@H...\n",
       "2                                      COc1cc(Cl)ccc1Cl\n",
       "3           COc1cc(C(O)=NCc2ccc(OCCN(C)C)cc2)cc(OC)c1OC\n",
       "4     CCC(=O)O[C@@]1(C(=O)CCl)[C@@H](C)C[C@H]2[C@@H]...\n",
       "...                                                 ...\n",
       "5891  N#Cc1cc(NC(=O)C(=O)O)c(Cl)c(NC(=O)C(=O)O)c1.NC...\n",
       "5892                     O=c1cccc2n1C[C@@H]1CNC[C@H]2C1\n",
       "5893  CSCC[C@H](N=C(O)[C@H](Cc1ccccc1)N=C(O)CN=C(O)C...\n",
       "5894  CCn1cc2c3c(cc(C(O)=NC(Cc4ccccc4)[C@H](O)C[NH2+...\n",
       "5895                   C[C@H]1CCCN(S(C)(=O)=O)[C@@H]1CO\n",
       "\n",
       "[5896 rows x 1 columns]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = pd.read_csv(\"smiles_test.csv\", index_col=0).reset_index(drop=True)\n",
    "test_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[00:11:40] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:11:40] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:11:41] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:11:41] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    }
   ],
   "source": [
    "fp_length = 1024\n",
    "\n",
    "test_fps = np.zeros((len(test_data), fp_length))\n",
    "for i, smiles in enumerate(test_data['smiles']):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    fp_vec = AllChem.GetMorganFingerprintAsBitVect(\n",
    "        mol, radius=3, nBits=fp_length)\n",
    "    arr = np.zeros((1,))\n",
    "    Chem.DataStructs.ConvertToNumpyArray(fp_vec, arr)\n",
    "    test_fps[i] = arr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "x_test_tensor = torch.tensor(\n",
    "    test_fps, dtype=torch.float).unsqueeze(1).to(device)\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_preds = model(x_test_tensor)\n",
    "\n",
    "# Convert the predictions back to numpy\n",
    "test_preds_np = test_preds.cpu().numpy()\n",
    "# Save the predictions to a CSV file\n",
    "predictions_df = pd.DataFrame(test_preds_np, columns=[\n",
    "                              \"task1\", \"task2\", \"task3\", \"task4\", \"task5\", \"task6\", \"task7\", \"task8\", \"task9\", \"task10\", \"task11\"])\n",
    "\n",
    "predictions_df.to_csv(\"test_predictions_6.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>task1</th>\n",
       "      <th>task2</th>\n",
       "      <th>task3</th>\n",
       "      <th>task4</th>\n",
       "      <th>task5</th>\n",
       "      <th>task6</th>\n",
       "      <th>task7</th>\n",
       "      <th>task8</th>\n",
       "      <th>task9</th>\n",
       "      <th>task10</th>\n",
       "      <th>task11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.102386</td>\n",
       "      <td>-0.056833</td>\n",
       "      <td>-0.063949</td>\n",
       "      <td>-0.101871</td>\n",
       "      <td>0.017161</td>\n",
       "      <td>-0.100909</td>\n",
       "      <td>-0.248593</td>\n",
       "      <td>-0.153750</td>\n",
       "      <td>-0.438664</td>\n",
       "      <td>0.270830</td>\n",
       "      <td>-0.056658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.022583</td>\n",
       "      <td>-0.046084</td>\n",
       "      <td>-0.003668</td>\n",
       "      <td>0.000650</td>\n",
       "      <td>0.037067</td>\n",
       "      <td>0.039401</td>\n",
       "      <td>0.006188</td>\n",
       "      <td>-0.006683</td>\n",
       "      <td>0.057424</td>\n",
       "      <td>0.013478</td>\n",
       "      <td>-0.021712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.001237</td>\n",
       "      <td>0.083306</td>\n",
       "      <td>0.081864</td>\n",
       "      <td>-0.066646</td>\n",
       "      <td>-0.088987</td>\n",
       "      <td>-0.024911</td>\n",
       "      <td>-0.985238</td>\n",
       "      <td>0.184275</td>\n",
       "      <td>-0.454025</td>\n",
       "      <td>0.019616</td>\n",
       "      <td>-0.130998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.015700</td>\n",
       "      <td>0.071019</td>\n",
       "      <td>0.067633</td>\n",
       "      <td>-0.181052</td>\n",
       "      <td>-0.073512</td>\n",
       "      <td>-0.210930</td>\n",
       "      <td>-0.141984</td>\n",
       "      <td>-0.493230</td>\n",
       "      <td>-0.219230</td>\n",
       "      <td>0.077950</td>\n",
       "      <td>0.022317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.027253</td>\n",
       "      <td>-0.030939</td>\n",
       "      <td>0.005519</td>\n",
       "      <td>-0.004438</td>\n",
       "      <td>0.032031</td>\n",
       "      <td>-0.012964</td>\n",
       "      <td>-0.191509</td>\n",
       "      <td>0.013752</td>\n",
       "      <td>0.424474</td>\n",
       "      <td>1.003188</td>\n",
       "      <td>-0.276660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5891</th>\n",
       "      <td>0.006398</td>\n",
       "      <td>0.000837</td>\n",
       "      <td>-0.129234</td>\n",
       "      <td>-0.000288</td>\n",
       "      <td>0.027554</td>\n",
       "      <td>0.096568</td>\n",
       "      <td>-0.800623</td>\n",
       "      <td>-0.342583</td>\n",
       "      <td>-0.462321</td>\n",
       "      <td>-0.270477</td>\n",
       "      <td>-0.021117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5892</th>\n",
       "      <td>-0.209650</td>\n",
       "      <td>0.097711</td>\n",
       "      <td>0.058675</td>\n",
       "      <td>0.038899</td>\n",
       "      <td>-0.113364</td>\n",
       "      <td>-0.182386</td>\n",
       "      <td>-0.001555</td>\n",
       "      <td>-0.642442</td>\n",
       "      <td>0.122331</td>\n",
       "      <td>0.111838</td>\n",
       "      <td>0.225398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5893</th>\n",
       "      <td>-0.155359</td>\n",
       "      <td>0.033007</td>\n",
       "      <td>0.112743</td>\n",
       "      <td>0.028786</td>\n",
       "      <td>-0.065611</td>\n",
       "      <td>-0.092091</td>\n",
       "      <td>0.039802</td>\n",
       "      <td>-0.684746</td>\n",
       "      <td>-0.020961</td>\n",
       "      <td>0.181185</td>\n",
       "      <td>-0.021307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5894</th>\n",
       "      <td>0.972080</td>\n",
       "      <td>0.011116</td>\n",
       "      <td>-0.037574</td>\n",
       "      <td>0.014832</td>\n",
       "      <td>0.002973</td>\n",
       "      <td>0.051173</td>\n",
       "      <td>-0.027172</td>\n",
       "      <td>-0.084397</td>\n",
       "      <td>0.005172</td>\n",
       "      <td>-0.189104</td>\n",
       "      <td>0.021973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5895</th>\n",
       "      <td>-0.037996</td>\n",
       "      <td>0.005821</td>\n",
       "      <td>-0.022974</td>\n",
       "      <td>-0.907530</td>\n",
       "      <td>0.010004</td>\n",
       "      <td>0.050679</td>\n",
       "      <td>-0.054781</td>\n",
       "      <td>-0.175483</td>\n",
       "      <td>-0.058872</td>\n",
       "      <td>0.049929</td>\n",
       "      <td>-0.025315</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5896 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         task1     task2     task3     task4     task5     task6     task7  \\\n",
       "0    -0.102386 -0.056833 -0.063949 -0.101871  0.017161 -0.100909 -0.248593   \n",
       "1     1.022583 -0.046084 -0.003668  0.000650  0.037067  0.039401  0.006188   \n",
       "2    -0.001237  0.083306  0.081864 -0.066646 -0.088987 -0.024911 -0.985238   \n",
       "3     0.015700  0.071019  0.067633 -0.181052 -0.073512 -0.210930 -0.141984   \n",
       "4     0.027253 -0.030939  0.005519 -0.004438  0.032031 -0.012964 -0.191509   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "5891  0.006398  0.000837 -0.129234 -0.000288  0.027554  0.096568 -0.800623   \n",
       "5892 -0.209650  0.097711  0.058675  0.038899 -0.113364 -0.182386 -0.001555   \n",
       "5893 -0.155359  0.033007  0.112743  0.028786 -0.065611 -0.092091  0.039802   \n",
       "5894  0.972080  0.011116 -0.037574  0.014832  0.002973  0.051173 -0.027172   \n",
       "5895 -0.037996  0.005821 -0.022974 -0.907530  0.010004  0.050679 -0.054781   \n",
       "\n",
       "         task8     task9    task10    task11  \n",
       "0    -0.153750 -0.438664  0.270830 -0.056658  \n",
       "1    -0.006683  0.057424  0.013478 -0.021712  \n",
       "2     0.184275 -0.454025  0.019616 -0.130998  \n",
       "3    -0.493230 -0.219230  0.077950  0.022317  \n",
       "4     0.013752  0.424474  1.003188 -0.276660  \n",
       "...        ...       ...       ...       ...  \n",
       "5891 -0.342583 -0.462321 -0.270477 -0.021117  \n",
       "5892 -0.642442  0.122331  0.111838  0.225398  \n",
       "5893 -0.684746 -0.020961  0.181185 -0.021307  \n",
       "5894 -0.084397  0.005172 -0.189104  0.021973  \n",
       "5895 -0.175483 -0.058872  0.049929 -0.025315  \n",
       "\n",
       "[5896 rows x 11 columns]"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>task1</th>\n",
       "      <th>task2</th>\n",
       "      <th>task3</th>\n",
       "      <th>task4</th>\n",
       "      <th>task5</th>\n",
       "      <th>task6</th>\n",
       "      <th>task7</th>\n",
       "      <th>task8</th>\n",
       "      <th>task9</th>\n",
       "      <th>task10</th>\n",
       "      <th>task11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.965388</td>\n",
       "      <td>0.669021</td>\n",
       "      <td>0.364129</td>\n",
       "      <td>0.248534</td>\n",
       "      <td>0.082723</td>\n",
       "      <td>0.101662</td>\n",
       "      <td>0.894853</td>\n",
       "      <td>0.099291</td>\n",
       "      <td>0.931158</td>\n",
       "      <td>0.132221</td>\n",
       "      <td>0.617906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.972610</td>\n",
       "      <td>0.986971</td>\n",
       "      <td>0.060073</td>\n",
       "      <td>0.286885</td>\n",
       "      <td>0.865854</td>\n",
       "      <td>0.805776</td>\n",
       "      <td>0.481583</td>\n",
       "      <td>0.715330</td>\n",
       "      <td>0.388927</td>\n",
       "      <td>0.998184</td>\n",
       "      <td>0.378946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.366591</td>\n",
       "      <td>0.275695</td>\n",
       "      <td>0.063553</td>\n",
       "      <td>0.966171</td>\n",
       "      <td>0.442205</td>\n",
       "      <td>0.969089</td>\n",
       "      <td>0.509688</td>\n",
       "      <td>0.540241</td>\n",
       "      <td>0.441256</td>\n",
       "      <td>0.164225</td>\n",
       "      <td>0.070570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.475604</td>\n",
       "      <td>0.490168</td>\n",
       "      <td>0.755998</td>\n",
       "      <td>0.477857</td>\n",
       "      <td>0.371955</td>\n",
       "      <td>0.947405</td>\n",
       "      <td>0.280805</td>\n",
       "      <td>0.872361</td>\n",
       "      <td>0.513712</td>\n",
       "      <td>0.570384</td>\n",
       "      <td>0.990165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.034529</td>\n",
       "      <td>0.669413</td>\n",
       "      <td>0.480047</td>\n",
       "      <td>0.011377</td>\n",
       "      <td>0.747641</td>\n",
       "      <td>0.272674</td>\n",
       "      <td>0.322530</td>\n",
       "      <td>0.330088</td>\n",
       "      <td>0.929216</td>\n",
       "      <td>0.492997</td>\n",
       "      <td>0.496907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5891</th>\n",
       "      <td>0.841416</td>\n",
       "      <td>0.832933</td>\n",
       "      <td>0.144299</td>\n",
       "      <td>0.092632</td>\n",
       "      <td>0.860756</td>\n",
       "      <td>0.797975</td>\n",
       "      <td>0.407141</td>\n",
       "      <td>0.819184</td>\n",
       "      <td>0.808753</td>\n",
       "      <td>0.693338</td>\n",
       "      <td>0.253581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5892</th>\n",
       "      <td>0.634844</td>\n",
       "      <td>0.643848</td>\n",
       "      <td>0.698586</td>\n",
       "      <td>0.211566</td>\n",
       "      <td>0.791034</td>\n",
       "      <td>0.462967</td>\n",
       "      <td>0.498234</td>\n",
       "      <td>0.265715</td>\n",
       "      <td>0.171268</td>\n",
       "      <td>0.524664</td>\n",
       "      <td>0.046151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5893</th>\n",
       "      <td>0.161446</td>\n",
       "      <td>0.419693</td>\n",
       "      <td>0.310739</td>\n",
       "      <td>0.977375</td>\n",
       "      <td>0.632457</td>\n",
       "      <td>0.645635</td>\n",
       "      <td>0.952371</td>\n",
       "      <td>0.000913</td>\n",
       "      <td>0.391865</td>\n",
       "      <td>0.986964</td>\n",
       "      <td>0.953342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5894</th>\n",
       "      <td>0.630445</td>\n",
       "      <td>0.798230</td>\n",
       "      <td>0.842443</td>\n",
       "      <td>0.188696</td>\n",
       "      <td>0.407885</td>\n",
       "      <td>0.308575</td>\n",
       "      <td>0.523217</td>\n",
       "      <td>0.240382</td>\n",
       "      <td>0.564827</td>\n",
       "      <td>0.343042</td>\n",
       "      <td>0.005972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5895</th>\n",
       "      <td>0.479574</td>\n",
       "      <td>0.971824</td>\n",
       "      <td>0.321590</td>\n",
       "      <td>0.593165</td>\n",
       "      <td>0.526852</td>\n",
       "      <td>0.986518</td>\n",
       "      <td>0.237232</td>\n",
       "      <td>0.365996</td>\n",
       "      <td>0.190525</td>\n",
       "      <td>0.541898</td>\n",
       "      <td>0.030638</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5896 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         task1     task2     task3     task4     task5     task6     task7  \\\n",
       "0     0.965388  0.669021  0.364129  0.248534  0.082723  0.101662  0.894853   \n",
       "1     0.972610  0.986971  0.060073  0.286885  0.865854  0.805776  0.481583   \n",
       "2     0.366591  0.275695  0.063553  0.966171  0.442205  0.969089  0.509688   \n",
       "3     0.475604  0.490168  0.755998  0.477857  0.371955  0.947405  0.280805   \n",
       "4     0.034529  0.669413  0.480047  0.011377  0.747641  0.272674  0.322530   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "5891  0.841416  0.832933  0.144299  0.092632  0.860756  0.797975  0.407141   \n",
       "5892  0.634844  0.643848  0.698586  0.211566  0.791034  0.462967  0.498234   \n",
       "5893  0.161446  0.419693  0.310739  0.977375  0.632457  0.645635  0.952371   \n",
       "5894  0.630445  0.798230  0.842443  0.188696  0.407885  0.308575  0.523217   \n",
       "5895  0.479574  0.971824  0.321590  0.593165  0.526852  0.986518  0.237232   \n",
       "\n",
       "         task8     task9    task10    task11  \n",
       "0     0.099291  0.931158  0.132221  0.617906  \n",
       "1     0.715330  0.388927  0.998184  0.378946  \n",
       "2     0.540241  0.441256  0.164225  0.070570  \n",
       "3     0.872361  0.513712  0.570384  0.990165  \n",
       "4     0.330088  0.929216  0.492997  0.496907  \n",
       "...        ...       ...       ...       ...  \n",
       "5891  0.819184  0.808753  0.693338  0.253581  \n",
       "5892  0.265715  0.171268  0.524664  0.046151  \n",
       "5893  0.000913  0.391865  0.986964  0.953342  \n",
       "5894  0.240382  0.564827  0.343042  0.005972  \n",
       "5895  0.365996  0.190525  0.541898  0.030638  \n",
       "\n",
       "[5896 rows x 11 columns]"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_data = pd.read_csv(\"sample_submission.csv\",\n",
    "                         index_col=0).reset_index(drop=True)\n",
    "score_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>task1</th>\n",
       "      <th>task2</th>\n",
       "      <th>task3</th>\n",
       "      <th>task4</th>\n",
       "      <th>task5</th>\n",
       "      <th>task6</th>\n",
       "      <th>task7</th>\n",
       "      <th>task8</th>\n",
       "      <th>task9</th>\n",
       "      <th>task10</th>\n",
       "      <th>task11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5891</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5892</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5893</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5894</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5895</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5896 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      task1  task2  task3  task4  task5  task6  task7  task8  task9  task10  \\\n",
       "0      -1.0   -1.0   -1.0   -1.0   -1.0   -1.0   -1.0    0.0   -1.0    -1.0   \n",
       "1      -1.0   -1.0   -1.0   -1.0   -1.0   -1.0   -1.0    0.0   -1.0    -1.0   \n",
       "2      -1.0   -1.0   -1.0   -1.0   -1.0   -1.0   -1.0   -1.0   -1.0     1.0   \n",
       "3      -1.0   -1.0   -1.0   -1.0   -1.0   -1.0   -1.0    0.0   -1.0    -1.0   \n",
       "4      -1.0   -1.0   -1.0   -1.0   -1.0   -1.0   -1.0   -1.0   -1.0     1.0   \n",
       "...     ...    ...    ...    ...    ...    ...    ...    ...    ...     ...   \n",
       "5891   -1.0   -1.0   -1.0   -1.0   -1.0   -1.0    0.0   -1.0    0.0    -1.0   \n",
       "5892   -1.0   -1.0    0.0   -1.0   -1.0   -1.0   -1.0   -1.0   -1.0    -1.0   \n",
       "5893   -1.0   -1.0   -1.0   -1.0   -1.0   -1.0   -1.0    0.0   -1.0    -1.0   \n",
       "5894   -1.0   -1.0   -1.0    0.0   -1.0   -1.0   -1.0   -1.0   -1.0    -1.0   \n",
       "5895   -1.0   -1.0   -1.0   -1.0   -1.0   -1.0   -1.0   -1.0   -1.0    -1.0   \n",
       "\n",
       "      task11  \n",
       "0       -1.0  \n",
       "1       -1.0  \n",
       "2       -1.0  \n",
       "3        1.0  \n",
       "4       -1.0  \n",
       "...      ...  \n",
       "5891    -1.0  \n",
       "5892    -1.0  \n",
       "5893    -1.0  \n",
       "5894    -1.0  \n",
       "5895     0.0  \n",
       "\n",
       "[5896 rows x 11 columns]"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = pd.read_csv(\"data_train.csv\", index_col=0).reset_index(drop=True)\n",
    "target = target.iloc[:5896, 1:]\n",
    "target = (target + 1)/2\n",
    "target[target == 0.5] = -1\n",
    "\n",
    "target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4993757843211411\n"
     ]
    }
   ],
   "source": [
    "auc_per_task = []\n",
    "for j in range(target.shape[1]):\n",
    "    y_score = predictions_df.iloc[:, j]\n",
    "    y_true = target.iloc[:, j]\n",
    "    idx = (y_true != (-1))\n",
    "    y_true_filtered = y_true[idx]\n",
    "    y_score_filtered = y_score[idx]\n",
    "\n",
    "    if len(np.unique(y_true_filtered)) >= 2:\n",
    "        auc_per_task.append(roc_auc_score(y_true_filtered, y_score_filtered))\n",
    "    else:\n",
    "        auc_per_task.append(np.nan)\n",
    "\n",
    "avg_auc = np.nanmean(auc_per_task)\n",
    "print(avg_auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ails",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
