{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RDKit:  2022.09.5\n"
     ]
    }
   ],
   "source": [
    "#from fcd import get_fcd, load_ref_model, canonical_smiles, get_predictions, calculate_frechet_distance\n",
    "import warnings\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy\n",
    "import pickle\n",
    "import rdkit\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Ignore some warnings from RDKIT and keras\n",
    "from rdkit import RDLogger, Chem\n",
    "from rdkit.Chem import rdMolDescriptors\n",
    "RDLogger.DisableLog('rdApp.*')\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Load methods from the FCD library\n",
    "\n",
    "np.random.seed(1234)\n",
    "\n",
    "print(\"RDKit: \", rdkit.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"smiles_train.txt\", header=None)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_canonical(smiles):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is not None:\n",
    "        return Chem.MolToSmiles(mol, canonical=True)\n",
    "    else:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_canonical = data.apply(to_canonical).dropna().reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_canonical.to_csv(\"canonical_smiles_train.txt\", index=False, header=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_canonical = pd.read_csv(\n",
    "    \"canonical_smiles_train.txt\", header=None, squeeze=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                COc1ccc(N2CCN(C(=O)c3cc4ccccc4[nH]3)CC2)cc1\n",
       "1            c1ccc(CCCNC2CCCCN(CCOC(c3ccccc3)c3ccccc3)C2)cc1\n",
       "2                             Nc1nc(O)c(Br)c(-c2cccc(O)c2)n1\n",
       "3               CCc1nc2ccc(Br)cc2c(=O)n1-c1nc2c(C)cc(C)cc2s1\n",
       "4            O=c1cnn(-c2ccc(S(=O)(=O)N3CCCCC3)cc2)c(=O)[nH]1\n",
       "                                 ...                        \n",
       "1036638                  CCOc1ccc(-n2c(SC)nc3c(c2=O)SCC3)cc1\n",
       "1036639        Nc1ncnc2c1nc(I)n2C1SC(COC(=O)c2ccccc2)C(O)C1O\n",
       "1036640              O=C(O)CCc1sc(C=C2NC(=O)CS2)nc1-c1ccccn1\n",
       "1036641    CN(c1ncnc2[nH]ccc12)C1CC(CS(=O)(=O)N2CCC(C#N)C...\n",
       "1036642    CCc1ccc(S(=O)(=O)NC2c3cc(C(=O)NCCc4c[nH]cn4)cc...\n",
       "Name: 0, Length: 1036643, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_canonical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_data, val_data = train_test_split(data, test_size=1/6, random_state=42)\n",
    "train_data, val_data = train_test_split(\n",
    "    data, test_size=0.2, random_state=42)\n",
    "train_data = train_data.reset_index(drop=True)\n",
    "val_data = val_data.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "charset = sorted(list(set(''.join(data) + '^$')))\n",
    "pad_char = '_'\n",
    "charset.append(pad_char)\n",
    "charset = sorted(list(set(charset)))\n",
    "\n",
    "max_length = max([len(smile) for smile in data]) + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def vectorize_smiles(smiles, charset, max_length):\n",
    "#     indices = [charset.index(char) for char in smiles]\n",
    "#     padded_indices = indices + [0] * (max_length - len(indices))\n",
    "#     data = torch.tensor(padded_indices[:-1], dtype=torch.long)\n",
    "#     target = torch.tensor(padded_indices[1:], dtype=torch.long)\n",
    "#     return data, target\n",
    "\n",
    "def vectorize_smiles(smiles, charset, max_length):\n",
    "    smiles = '^' + smiles + '$'\n",
    "    indices = [charset.index(char) for char in smiles]\n",
    "    padded_indices = indices + [0] * (max_length - len(indices))\n",
    "    data = torch.tensor(padded_indices[:-1], dtype=torch.long)\n",
    "    target = torch.tensor(padded_indices[1:], dtype=torch.long)\n",
    "    return data, target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SMILESData(Dataset):\n",
    "    def __init__(self, smiles_list, charset, max_length, pad_char='_'):\n",
    "        self.smiles_list = smiles_list\n",
    "        self.max_length = max_length\n",
    "        self.pad_char = pad_char\n",
    "\n",
    "        # Use the provided charset\n",
    "        self.charset = charset\n",
    "        self.char_to_int = {char: i for i, char in enumerate(self.charset)}\n",
    "        self.int_to_char = {i: char for i, char in enumerate(self.charset)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.smiles_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        smiles = self.smiles_list[idx]\n",
    "        one_hot = self.smiles_to_one_hot(smiles)\n",
    "        target = torch.tensor([self.char_to_int[char] for char in smiles] + [\n",
    "                              self.char_to_int[self.pad_char]] * (self.max_length - len(smiles)), dtype=torch.long)\n",
    "\n",
    "\n",
    "        return one_hot, target\n",
    "\n",
    "    def smiles_to_one_hot(self, smiles):\n",
    "        one_hot = np.zeros((self.max_length, len(self.charset)), dtype=np.float32)\n",
    "        for i, char in enumerate(smiles):\n",
    "            one_hot[i, self.char_to_int[char]] = 1.0\n",
    "        return torch.tensor(one_hot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SMILESData(train_data, charset, max_length, pad_char=pad_char)\n",
    "val_dataset = SMILESData(val_data, charset, max_length, pad_char=pad_char)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "input_size = len(charset)\n",
    "hidden_size = 512\n",
    "output_size = len(charset)\n",
    "num_layers = 3\n",
    "num_epochs = 5\n",
    "learning_rate = 0.001\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimplifiedSMILESGRU(nn.Module):\n",
    "    def __init__(self, input_size=input_size, hidden_size=512, output_size=output_size, num_layers=3, dropout=0.2):\n",
    "        super(SimplifiedSMILESGRU, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers,\n",
    "                          batch_first=True, dropout=dropout)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0),\n",
    "                         self.hidden_size).to(device)\n",
    "        out, _ = self.gru(x, h0)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimplifiedSMILESGRU(input_size, hidden_size,\n",
    "                            output_size, num_layers).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "\n",
    "scheduler = ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.1, patience=2, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Train Loss: 0.0547, Validation Loss: 0.0014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/5], Train Loss: 0.0164, Validation Loss: 0.0012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/5], Train Loss: 0.0178, Validation Loss: 0.0012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[106], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m output \u001b[39m=\u001b[39m model(one_hot_data\u001b[39m.\u001b[39mfloat())\n\u001b[0;32m     12\u001b[0m loss \u001b[39m=\u001b[39m criterion(output\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m), target)\n\u001b[1;32m---> 13\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     14\u001b[0m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mclip_grad_norm_(model\u001b[39m.\u001b[39mparameters(), max_norm\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     15\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[1;32md:\\Miniconda\\envs\\dlnn\\lib\\site-packages\\torch\\_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    388\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    389\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    390\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    394\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[0;32m    395\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[1;32m--> 396\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[1;32md:\\Miniconda\\envs\\dlnn\\lib\\site-packages\\torch\\autograd\\__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    168\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    170\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 173\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    174\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    175\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_count = 0\n",
    "    for one_hot_data, target in tqdm(train_loader, desc=f\"Training Epoch {epoch + 1}/{num_epochs}\", leave=False):\n",
    "        one_hot_data, target = one_hot_data.to(device), target.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(one_hot_data.float())\n",
    "\n",
    "        loss = criterion(output.transpose(1, 2), target)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        train_count += 1\n",
    "\n",
    "    avg_train_loss = train_loss / train_count\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_count = 0\n",
    "    with torch.no_grad():\n",
    "        for one_hot_data, target in tqdm(val_loader, desc=f\"Validating Epoch {epoch + 1}/{num_epochs}\", leave=False):\n",
    "            one_hot_data, target = one_hot_data.to(device), target.to(device)\n",
    "            output = model(one_hot_data.float())\n",
    "            loss = criterion(output.transpose(1, 2), target)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            val_count += 1\n",
    "\n",
    "    avg_val_loss = val_loss / val_count\n",
    "\n",
    "    print(\n",
    "        f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}\")\n",
    "    \n",
    "    scheduler.step(avg_val_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"trained_model_lstm_8.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimplifiedSMILESGRU(\n",
       "  (gru): GRU(40, 512, num_layers=3, batch_first=True, dropout=0.2)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       "  (fc): Linear(in_features=512, out_features=40, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"trained_model_lstm_8.pth\"))\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_smiles(smiles, charset, max_length):\n",
    "    vector = [charset.index(char) for char in smiles]\n",
    "    padding = [len(charset) - 1] * (max_length - len(vector))\n",
    "    vector.extend(padding)\n",
    "    return torch.tensor(vector, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unvectorize_smiles(one_hot_array, charset):\n",
    "    smiles = ''\n",
    "    for i in one_hot_array:\n",
    "        if charset[i] == 's':\n",
    "            break  # Stop decoding when the first padding character is encountered\n",
    "        smiles += charset[i]\n",
    "    return smiles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_smiles(smiles):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    return mol is not None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, input_smiles, charset, max_length):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        data = vectorize_smiles(input_smiles, charset, max_length)\n",
    "        one_hot_data = torch.zeros(1, data.size(0), len(charset)).to(device)\n",
    "        one_hot_data.scatter_(2, data.to(device).unsqueeze(0).unsqueeze(2), 1)\n",
    "        output = model(one_hot_data.float())\n",
    "        output_smiles = unvectorize_smiles(\n",
    "            output[0].argmax(dim=1).tolist(), charset)\n",
    "        return output_smiles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_count = 0\n",
    "smiles_gen = set()\n",
    "with open(\"predictions_lstm_8.txt\", \"w\") as f:\n",
    "    for input_smiles in val_data:\n",
    "        predicted_smiles = predict(model, input_smiles, charset, max_length)\n",
    "        #print(f\"{predicted_smiles}\")\n",
    "        if predicted_smiles is not None and is_valid_smiles(predicted_smiles):\n",
    "            if predicted_smiles not in data_canonical:\n",
    "                smiles_gen.add(predicted_smiles)\n",
    "                f.write(predicted_smiles + \"\\n\")\n",
    "                valid_count += 1\n",
    "            if valid_count >= 10001:\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit import DataStructs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fingerprint(smiles):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    fp = AllChem.GetMorganFingerprint(mol, 2)\n",
    "    return fp\n",
    "\n",
    "\n",
    "threshold = 0.8\n",
    "\n",
    "valid_count = 0\n",
    "smiles_gen = set()\n",
    "with open(\"predictions_lstm_test.txt\", \"w\") as f:\n",
    "    for input_smiles in val_data:\n",
    "        predicted_smiles = predict(model, input_smiles, charset, max_length)\n",
    "        # print(f\"{predicted_smiles}\")\n",
    "        if predicted_smiles is not None and is_valid_smiles(predicted_smiles):\n",
    "            if predicted_smiles not in data_canonical:\n",
    "                # Compute the fingerprint of the predicted SMILES\n",
    "                fp_pred = get_fingerprint(predicted_smiles)\n",
    "                # Check if the predicted SMILES is similar to any of the previously generated SMILES\n",
    "                is_similar = False\n",
    "                for smiles in smiles_gen:\n",
    "                    fp = get_fingerprint(smiles)\n",
    "                    similarity = DataStructs.TanimotoSimilarity(fp, fp_pred)\n",
    "                    if similarity > threshold:\n",
    "                        is_similar = True\n",
    "                        break\n",
    "                if not is_similar:\n",
    "                    smiles_gen.add(predicted_smiles)\n",
    "                    f.write(predicted_smiles + \"\\n\")\n",
    "                    valid_count += 1\n",
    "                if valid_count >= 10001:\n",
    "                    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ails-fcd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
